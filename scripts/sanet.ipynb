{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"v13.ipynb","provenance":[{"file_id":"1R86MIUklfavvvKhYWZ3gVoCI-tx0hyJN","timestamp":1603943277862}],"collapsed_sections":["kFLA6KlvOHSe","3kanvU7-HW0e","zE6G7Xf372gy","ERq53azy8y29","Fdize3iB84bi","mW-jTytnNThj"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kFLA6KlvOHSe"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"9o_ElfycVtDD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606225649201,"user_tz":300,"elapsed":25091,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"d3ecabc0-1dbd-4607-888e-9a26aed9b04c"},"source":["### Mounting with google drive \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sgi6tCJ6e753"},"source":["### Importing required libraries\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","from sklearn.preprocessing import LabelEncoder\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","# Neural networks can be constructed using the torch.nn package.\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import Subset\n","from torch.utils.data.sampler import SequentialSampler\n","from torch.utils.data import Dataset\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gft5oR_hV-sO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606225661855,"user_tz":300,"elapsed":1803,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"62b5be96-90eb-4c08-a48b-68ee5bef5970"},"source":["## Switching to working directory\n","%cd '/content/drive/My Drive/Research/SAnet/yolov5/yolov5'\n","!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research/SAnet/yolov5/yolov5\n","/content/drive/My Drive/Research/SAnet/yolov5/yolov5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yhmk2LndG_6n"},"source":["# Yolo code"]},{"cell_type":"code","metadata":{"id":"YuXU6OVyguJa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604178237793,"user_tz":240,"elapsed":1688,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"bb1dcce5-e26f-4669-9e0a-e1e1bec30e1e"},"source":["import argparse\n","\n","import torch.backends.cudnn as cudnn\n","from google.colab.patches import cv2_imshow\n","from utils import google_utils\n","from utils.datasets import *\n","from utils.utils import *\n","\n","import glob\n","import math\n","import os\n","import random\n","import shutil\n","import time\n","from pathlib import Path\n","from threading import Thread\n","\n","import cv2\n","import numpy as np\n","import torch\n","from PIL import Image, ExifTags\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","\n","from utils.utils import xyxy2xywh, xywh2xyxy\n","\n","help_url = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n","img_formats = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.dng']\n","vid_formats = ['.mov', '.avi', '.mp4', '.mpg', '.mpeg', '.m4v', '.wmv', '.mkv']\n","\n","\n","\n","class LoadImages1:  # for inference\n","    def __init__(self, path, session, img_size=416):\n","        path = str(Path(path)) \n","        files = []\n","        if os.path.isdir(path):\n","            files = sorted(glob.glob(os.path.join(path, '*.*')))\n","        elif os.path.isfile(path):\n","            files = [path]\n","\n","        images = []\n","        images = [x for x in files if os.path.splitext(x)[-1].lower() in img_formats]          \n","\n","        videos = [x for x in files if os.path.splitext(x)[-1].lower() in vid_formats]\n","        nI, nV = len(images), len(videos)\n","\n","        self.img_size = img_size\n","        self.files = images + videos\n","        self.nF = nI + nV  # number of files\n","        self.video_flag = [False] * nI + [True] * nV\n","        self.mode = 'images'\n","        if any(videos):\n","            self.new_video(videos[0])  # new video\n","        else:\n","            self.cap = None\n","        assert self.nF > 0, 'No images or videos found in %s. Supported formats are:\\nimages: %s\\nvideos: %s' % \\\n","                            (path, img_formats, vid_formats)\n","\n","    def __iter__(self):\n","        self.count = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.count == self.nF:\n","            raise StopIteration\n","        path = self.files[self.count]\n","\n","        if self.video_flag[self.count]:\n","            # Read video\n","            self.mode = 'video'\n","            ret_val, img0 = self.cap.read()\n","            if not ret_val:\n","                self.count += 1\n","                self.cap.release()\n","                if self.count == self.nF:  # last video\n","                    raise StopIteration\n","                else:\n","                    path = self.files[self.count]\n","                    self.new_video(path)\n","                    ret_val, img0 = self.cap.read()\n","\n","            self.frame += 1\n","            print('video %g/%g (%g/%g) %s: ' % (self.count + 1, self.nF, self.frame, self.nframes, path), end='')\n","\n","        else:\n","            # Read image\n","            self.count += 1\n","            img0 = cv2.imread(path)  # BGR\n","            assert img0 is not None, 'Image Not Found ' + path\n","            print('image %g/%g %s: ' % (self.count, self.nF, path), end='')\n","\n","        # Padded resize\n","        img = letterbox(img0, new_shape=self.img_size)[0]\n","\n","        # Convert\n","        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        img = np.ascontiguousarray(img)\n","\n","        return path, img, img0, self.cap\n","\n","    def new_video(self, path):\n","        self.frame = 0\n","        self.cap = cv2.VideoCapture(path)\n","        self.nframes = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    def __len__(self):\n","        return self.nF  # number of files\n","\n","\n","\n","\n","\n","def detect(source_path,session,save_img=False):\n","    out, source, weights, view_img, save_txt, imgsz = \\\n","        opt.output, source_path, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n","    webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n","\n","\n","    # Initialize\n","    device = torch_utils.select_device(opt.device)\n","    if os.path.exists(out):\n","        shutil.rmtree(out)  # delete output folder\n","    os.makedirs(out)  # make new output folder\n","    half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","    # Load model\n","    google_utils.attempt_download(weights)\n","    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n","\n","    model.to(device).eval()\n","    if half:\n","        model.half()  # to FP16\n","\n","    # Second-stage classifier\n","    classify = False\n","    if classify:\n","        modelc = torch_utils.load_classifier(name='resnet101', n=2)  # initialize\n","        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n","        modelc.to(device).eval()\n","\n","    # Set Dataloader\n","    vid_path, vid_writer = None, None\n","    if webcam:\n","        view_img = True\n","        cudnn.benchmark = True  # set True to speed up constant image size inference\n","        dataset = LoadStreams(source, img_size=imgsz)\n","    else:\n","        save_img = True\n","        dataset = LoadImages1(source, session, img_size=imgsz)\n","\n","    # Get names and colors\n","    names = model.module.names if hasattr(model, 'module') else model.names\n","    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n","\n","    # Run inference\n","    t0 = time.time()\n","    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","    bounding_boxes_all_images = []\n","    for path, img, im0s, vid_cap in dataset:\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","            img = img.unsqueeze(0)\n","\n","        # Inference\n","        t1 = torch_utils.time_synchronized()\n","        pred = model(img, augment=opt.augment)[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n","        t2 = torch_utils.time_synchronized()\n","\n","        # Apply Classifier\n","        if classify:\n","            pred = apply_classifier(pred, modelc, img, im0s)\n","\n","        # Process detections\n","        \n","        for i, det in enumerate(pred):  # detections per image\n","            if webcam:  # batch_size >= 1\n","                p, s, im0 = path[i], '%g: ' % i, im0s[i].copy()\n","            else:\n","                p, s, im0 = path, '', im0s\n","\n","            save_path = str(Path(out) / Path(p).name)\n","            s += '%gx%g ' % img.shape[2:]  # print string\n","            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # Â normalization gain whwh\n","            if det is not None and len(det):\n","                # Rescale boxes from img_size to im0 size\n","                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                # Print results\n","                for c in det[:, -1].unique():\n","                    n = (det[:, -1] == c).sum()  # detections per class\n","                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                # Write results\n","                minx = float(\"inf\")\n","                miny = float(\"inf\")\n","                maxx = 0\n","                maxy = 0\n","\n","                box_num = 0\n","                bounding_boxes = {}\n","                for *xyxy, conf, cls in det:\n","                    box_num+=1\n","                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","                    ### Saving bounding box\n","                    if (bounding_boxes.get(names[int(cls)], None) == None):\n","                        bounding_boxes[names[int(cls)]] = [[int(xyxy[0]), int(xyxy[1]), abs(int(xyxy[2])-int(xyxy[0])), abs(int(xyxy[3])-int(xyxy[1]))]]\n","                    else:\n","                        bounding_boxes[names[int(cls)]].append([int(xyxy[0]), int(xyxy[1]), abs(int(xyxy[2])-int(xyxy[0])), abs(int(xyxy[3])-int(xyxy[1]))])\n","\n","\n","                    tlx,tly, brx, bry = int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])\n","                    if tlx < minx: \n","                        minx = tlx \n","                    if tly < miny:\n","                        miny = tly\n","                    if bry > maxy:\n","                        maxy = bry\n","                    if brx > maxx:\n","                        maxx = brx # For cropped image use: crop_img = img[y:y+h, x:x+w]\n","\n","\n","                    if save_txt:  # Write to file\n","                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","                        with open(save_path[:save_path.rfind('.')] + '.txt', 'a') as file:\n","                            file.write(('%g ' * 5 + '\\n') % (cls, *xywh))  # label format\n","\n","                    if save_img or view_img:  # Add bbox to image\n","                        label = '%s %.2f' % (names[int(cls)], conf)\n","                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n","            bounding_boxes_all_images.append(bounding_boxes)    \n","            # Print time (inference + NMS)\n","            print('%sDone. (%.3fs)' % (s, t2 - t1))\n","            im0 = im0[miny:maxy, minx:maxx]\n","\n","\n","            # Stream results\n","            if view_img:\n","                cv2_imshow( im0)\n","                not_showing = True\n","                #cv2.imshow(p, im0)\n","                \n","                if cv2.waitKey(1) == ord('q'):  # q to quit\n","                    raise StopIteration\n","\n","            # Save results (image with detections)\n","            if save_img:\n","                if dataset.mode == 'images':\n","                    print(save_path)\n","                    cv2.imwrite(save_path, im0)\n","                    not_saving = True\n","                else:\n","                    if vid_path != save_path:  # new video\n","                        vid_path = save_path\n","                        if isinstance(vid_writer, cv2.VideoWriter):\n","                            vid_writer.release()  # release previous video writer\n","\n","                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n","                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*opt.fourcc), fps, (w, h))\n","                    vid_writer.write(im0)\n","\n","    if save_txt or save_img:\n","        print('Results saved to %s' % os.getcwd() + os.sep + out)\n","        if platform == 'darwin':  # MacOS\n","            os.system('open ' + save_path)\n","\n","    print('Done. (%.3fs)' % (time.time() - t0))\n","    return bounding_boxes_all_images\n","\n","######################################################################################\n","\n","class Options:\n","    def __init__(self):\n","      self.weights = 'weights/best.pt'\n","      self.source = 'inference/images'\n","      self.output = '/content/drive/My Drive/cropped_images' #'inference/output'\n","      self.img_size = 640\n","      self.conf_thres = 0.4\n","      self.iou_thres = 0.5\n","      self.fourcc = 'mp4v'\n","      self.device = ''\n","      self.view_img = False\n","      self.agnostic_nms = False\n","      self.augment = False      \n","      self.save_txt = False\n","      self.classes = None\n","\n","opt = Options()\n","# # if __name__ == '__main__':\n","# parser = argparse.ArgumentParser()\n","# parser.add_argument('--weights', type=str, default='weights/yolov5s.pt', help='model.pt path')\n","# parser.add_argument('--source', type=str, default='inference/images', help='source')  # file/folder, 0 for webcam\n","# parser.add_argument('--output', type=str, default='inference/output', help='output folder')  # output folder\n","# parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n","# parser.add_argument('--conf-thres', type=float, default=0.4, help='object confidence threshold')\n","# parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')\n","# parser.add_argument('--fourcc', type=str, default='mp4v', help='output video codec (verify ffmpeg support)')\n","# parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","# parser.add_argument('--view-img', action='store_true', help='display results')\n","# parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n","# parser.add_argument('--classes', nargs='+', type=int, help='filter by class')\n","# parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n","# parser.add_argument('--augment', action='store_true', help='augmented inference')\n","# opt = parser.parse_args()\n","opt.img_size = check_img_size(opt.img_size)\n","print(opt)\n","\n","opt.weights='weights/best.pt'\n","opt.view_img=False\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<__main__.Options object at 0x7f1ee74ee320>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iwX30EokguZY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604188980883,"user_tz":240,"elapsed":123624,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"c73f2a75-7e07-4326-aae2-e431d354de2a"},"source":["# # BASE_PATH = './'\n","DATA_PATH = '../../data_files/session7'\n","\n","with torch.no_grad():\n","    source_path = DATA_PATH #os.path.join(DATA_PATH,'session1 ('+str(i+1)+').jpg')\n","    det_boxes_rgb_images5 = detect(source_path, 'rgb_images2')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.upsampling.Upsample' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["image 1/283 ../../data_files/session7/session7-1.jpg: 576x640 5 blemisheds, 6 unblemisheds, 3 gloves, 1 belts, 4 bins, 1 heads, Done. (0.052s)\n","../../data_files/cc7/session7-1.jpg\n","image 2/283 ../../data_files/session7/session7-10.jpg: 576x640 5 blemisheds, 6 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-10.jpg\n","image 3/283 ../../data_files/session7/session7-100.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-100.jpg\n","image 4/283 ../../data_files/session7/session7-101.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-101.jpg\n","image 5/283 ../../data_files/session7/session7-102.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-102.jpg\n","image 6/283 ../../data_files/session7/session7-103.jpg: 576x640 5 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-103.jpg\n","image 7/283 ../../data_files/session7/session7-104.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-104.jpg\n","image 8/283 ../../data_files/session7/session7-105.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-105.jpg\n","image 9/283 ../../data_files/session7/session7-106.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-106.jpg\n","image 10/283 ../../data_files/session7/session7-107.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-107.jpg\n","image 11/283 ../../data_files/session7/session7-108.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-108.jpg\n","image 12/283 ../../data_files/session7/session7-109.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-109.jpg\n","image 13/283 ../../data_files/session7/session7-11.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-11.jpg\n","image 14/283 ../../data_files/session7/session7-110.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-110.jpg\n","image 15/283 ../../data_files/session7/session7-111.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-111.jpg\n","image 16/283 ../../data_files/session7/session7-112.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-112.jpg\n","image 17/283 ../../data_files/session7/session7-113.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-113.jpg\n","image 18/283 ../../data_files/session7/session7-114.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-114.jpg\n","image 19/283 ../../data_files/session7/session7-115.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-115.jpg\n","image 20/283 ../../data_files/session7/session7-116.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-116.jpg\n","image 21/283 ../../data_files/session7/session7-117.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.053s)\n","../../data_files/cc7/session7-117.jpg\n","image 22/283 ../../data_files/session7/session7-118.jpg: 576x640 4 blemisheds, 4 unblemisheds, 3 gloves, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-118.jpg\n","image 23/283 ../../data_files/session7/session7-119.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-119.jpg\n","image 24/283 ../../data_files/session7/session7-12.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-12.jpg\n","image 25/283 ../../data_files/session7/session7-120.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-120.jpg\n","image 26/283 ../../data_files/session7/session7-121.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-121.jpg\n","image 27/283 ../../data_files/session7/session7-122.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-122.jpg\n","image 28/283 ../../data_files/session7/session7-123.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-123.jpg\n","image 29/283 ../../data_files/session7/session7-124.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-124.jpg\n","image 30/283 ../../data_files/session7/session7-125.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-125.jpg\n","image 31/283 ../../data_files/session7/session7-126.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-126.jpg\n","image 32/283 ../../data_files/session7/session7-127.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-127.jpg\n","image 33/283 ../../data_files/session7/session7-128.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-128.jpg\n","image 34/283 ../../data_files/session7/session7-129.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-129.jpg\n","image 35/283 ../../data_files/session7/session7-13.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-13.jpg\n","image 36/283 ../../data_files/session7/session7-130.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-130.jpg\n","image 37/283 ../../data_files/session7/session7-131.jpg: 576x640 5 blemisheds, 3 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-131.jpg\n","image 38/283 ../../data_files/session7/session7-132.jpg: 576x640 4 blemisheds, 3 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-132.jpg\n","image 39/283 ../../data_files/session7/session7-133.jpg: 576x640 4 blemisheds, 3 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-133.jpg\n","image 40/283 ../../data_files/session7/session7-134.jpg: 576x640 5 blemisheds, 3 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-134.jpg\n","image 41/283 ../../data_files/session7/session7-135.jpg: 576x640 4 blemisheds, 3 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-135.jpg\n","image 42/283 ../../data_files/session7/session7-136.jpg: 576x640 4 blemisheds, 3 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-136.jpg\n","image 43/283 ../../data_files/session7/session7-137.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-137.jpg\n","image 44/283 ../../data_files/session7/session7-138.jpg: 576x640 4 blemisheds, 3 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-138.jpg\n","image 45/283 ../../data_files/session7/session7-139.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-139.jpg\n","image 46/283 ../../data_files/session7/session7-14.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-14.jpg\n","image 47/283 ../../data_files/session7/session7-140.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-140.jpg\n","image 48/283 ../../data_files/session7/session7-141.jpg: 576x640 4 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-141.jpg\n","image 49/283 ../../data_files/session7/session7-142.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-142.jpg\n","image 50/283 ../../data_files/session7/session7-143.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-143.jpg\n","image 51/283 ../../data_files/session7/session7-144.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-144.jpg\n","image 52/283 ../../data_files/session7/session7-145.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-145.jpg\n","image 53/283 ../../data_files/session7/session7-146.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-146.jpg\n","image 54/283 ../../data_files/session7/session7-147.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-147.jpg\n","image 55/283 ../../data_files/session7/session7-148.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.054s)\n","../../data_files/cc7/session7-148.jpg\n","image 56/283 ../../data_files/session7/session7-149.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-149.jpg\n","image 57/283 ../../data_files/session7/session7-15.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-15.jpg\n","image 58/283 ../../data_files/session7/session7-150.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 2 bins, 3 heads, Done. (0.050s)\n","../../data_files/cc7/session7-150.jpg\n","image 59/283 ../../data_files/session7/session7-151.jpg: 576x640 3 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-151.jpg\n","image 60/283 ../../data_files/session7/session7-152.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-152.jpg\n","image 61/283 ../../data_files/session7/session7-153.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-153.jpg\n","image 62/283 ../../data_files/session7/session7-154.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-154.jpg\n","image 63/283 ../../data_files/session7/session7-155.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-155.jpg\n","image 64/283 ../../data_files/session7/session7-156.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-156.jpg\n","image 65/283 ../../data_files/session7/session7-157.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-157.jpg\n","image 66/283 ../../data_files/session7/session7-158.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-158.jpg\n","image 67/283 ../../data_files/session7/session7-159.jpg: 576x640 3 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-159.jpg\n","image 68/283 ../../data_files/session7/session7-16.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-16.jpg\n","image 69/283 ../../data_files/session7/session7-160.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-160.jpg\n","image 70/283 ../../data_files/session7/session7-161.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-161.jpg\n","image 71/283 ../../data_files/session7/session7-162.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-162.jpg\n","image 72/283 ../../data_files/session7/session7-163.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-163.jpg\n","image 73/283 ../../data_files/session7/session7-164.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-164.jpg\n","image 74/283 ../../data_files/session7/session7-165.jpg: 576x640 3 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-165.jpg\n","image 75/283 ../../data_files/session7/session7-166.jpg: 576x640 3 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-166.jpg\n","image 76/283 ../../data_files/session7/session7-167.jpg: 576x640 3 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-167.jpg\n","image 77/283 ../../data_files/session7/session7-168.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-168.jpg\n","image 78/283 ../../data_files/session7/session7-169.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-169.jpg\n","image 79/283 ../../data_files/session7/session7-17.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-17.jpg\n","image 80/283 ../../data_files/session7/session7-170.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-170.jpg\n","image 81/283 ../../data_files/session7/session7-171.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-171.jpg\n","image 82/283 ../../data_files/session7/session7-172.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-172.jpg\n","image 83/283 ../../data_files/session7/session7-173.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-173.jpg\n","image 84/283 ../../data_files/session7/session7-174.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-174.jpg\n","image 85/283 ../../data_files/session7/session7-175.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-175.jpg\n","image 86/283 ../../data_files/session7/session7-176.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.052s)\n","../../data_files/cc7/session7-176.jpg\n","image 87/283 ../../data_files/session7/session7-177.jpg: 576x640 2 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-177.jpg\n","image 88/283 ../../data_files/session7/session7-178.jpg: 576x640 2 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-178.jpg\n","image 89/283 ../../data_files/session7/session7-179.jpg: 576x640 2 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-179.jpg\n","image 90/283 ../../data_files/session7/session7-18.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-18.jpg\n","image 91/283 ../../data_files/session7/session7-180.jpg: 576x640 2 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-180.jpg\n","image 92/283 ../../data_files/session7/session7-181.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-181.jpg\n","image 93/283 ../../data_files/session7/session7-182.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-182.jpg\n","image 94/283 ../../data_files/session7/session7-183.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-183.jpg\n","image 95/283 ../../data_files/session7/session7-184.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-184.jpg\n","image 96/283 ../../data_files/session7/session7-185.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-185.jpg\n","image 97/283 ../../data_files/session7/session7-186.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-186.jpg\n","image 98/283 ../../data_files/session7/session7-187.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-187.jpg\n","image 99/283 ../../data_files/session7/session7-188.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-188.jpg\n","image 100/283 ../../data_files/session7/session7-189.jpg: 576x640 3 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-189.jpg\n","image 101/283 ../../data_files/session7/session7-19.jpg: 576x640 5 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-19.jpg\n","image 102/283 ../../data_files/session7/session7-190.jpg: 576x640 3 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-190.jpg\n","image 103/283 ../../data_files/session7/session7-191.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 3 heads, Done. (0.050s)\n","../../data_files/cc7/session7-191.jpg\n","image 104/283 ../../data_files/session7/session7-192.jpg: 576x640 3 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-192.jpg\n","image 105/283 ../../data_files/session7/session7-193.jpg: 576x640 2 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-193.jpg\n","image 106/283 ../../data_files/session7/session7-194.jpg: 576x640 2 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-194.jpg\n","image 107/283 ../../data_files/session7/session7-195.jpg: 576x640 2 blemisheds, 5 unblemisheds, 3 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-195.jpg\n","image 108/283 ../../data_files/session7/session7-196.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-196.jpg\n","image 109/283 ../../data_files/session7/session7-197.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-197.jpg\n","image 110/283 ../../data_files/session7/session7-198.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-198.jpg\n","image 111/283 ../../data_files/session7/session7-199.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-199.jpg\n","image 112/283 ../../data_files/session7/session7-2.jpg: 576x640 4 blemisheds, 7 unblemisheds, 3 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-2.jpg\n","image 113/283 ../../data_files/session7/session7-20.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-20.jpg\n","image 114/283 ../../data_files/session7/session7-200.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-200.jpg\n","image 115/283 ../../data_files/session7/session7-201.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-201.jpg\n","image 116/283 ../../data_files/session7/session7-202.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-202.jpg\n","image 117/283 ../../data_files/session7/session7-203.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-203.jpg\n","image 118/283 ../../data_files/session7/session7-204.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-204.jpg\n","image 119/283 ../../data_files/session7/session7-205.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-205.jpg\n","image 120/283 ../../data_files/session7/session7-206.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-206.jpg\n","image 121/283 ../../data_files/session7/session7-207.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-207.jpg\n","image 122/283 ../../data_files/session7/session7-208.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-208.jpg\n","image 123/283 ../../data_files/session7/session7-209.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-209.jpg\n","image 124/283 ../../data_files/session7/session7-21.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-21.jpg\n","image 125/283 ../../data_files/session7/session7-210.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-210.jpg\n","image 126/283 ../../data_files/session7/session7-211.jpg: 576x640 2 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-211.jpg\n","image 127/283 ../../data_files/session7/session7-212.jpg: 576x640 2 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-212.jpg\n","image 128/283 ../../data_files/session7/session7-213.jpg: 576x640 2 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-213.jpg\n","image 129/283 ../../data_files/session7/session7-214.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-214.jpg\n","image 130/283 ../../data_files/session7/session7-215.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-215.jpg\n","image 131/283 ../../data_files/session7/session7-216.jpg: 576x640 2 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-216.jpg\n","image 132/283 ../../data_files/session7/session7-217.jpg: 576x640 2 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-217.jpg\n","image 133/283 ../../data_files/session7/session7-218.jpg: 576x640 2 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-218.jpg\n","image 134/283 ../../data_files/session7/session7-219.jpg: 576x640 2 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-219.jpg\n","image 135/283 ../../data_files/session7/session7-22.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-22.jpg\n","image 136/283 ../../data_files/session7/session7-220.jpg: 576x640 2 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-220.jpg\n","image 137/283 ../../data_files/session7/session7-221.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-221.jpg\n","image 138/283 ../../data_files/session7/session7-222.jpg: 576x640 2 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-222.jpg\n","image 139/283 ../../data_files/session7/session7-223.jpg: 576x640 2 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-223.jpg\n","image 140/283 ../../data_files/session7/session7-224.jpg: 576x640 2 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-224.jpg\n","image 141/283 ../../data_files/session7/session7-225.jpg: 576x640 1 blemisheds, 5 unblemisheds, 3 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-225.jpg\n","image 142/283 ../../data_files/session7/session7-226.jpg: 576x640 1 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-226.jpg\n","image 143/283 ../../data_files/session7/session7-227.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-227.jpg\n","image 144/283 ../../data_files/session7/session7-228.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-228.jpg\n","image 145/283 ../../data_files/session7/session7-229.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-229.jpg\n","image 146/283 ../../data_files/session7/session7-23.jpg: 576x640 4 blemisheds, 7 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.051s)\n","../../data_files/cc7/session7-23.jpg\n","image 147/283 ../../data_files/session7/session7-230.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-230.jpg\n","image 148/283 ../../data_files/session7/session7-231.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-231.jpg\n","image 149/283 ../../data_files/session7/session7-232.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-232.jpg\n","image 150/283 ../../data_files/session7/session7-233.jpg: 576x640 1 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-233.jpg\n","image 151/283 ../../data_files/session7/session7-234.jpg: 576x640 1 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-234.jpg\n","image 152/283 ../../data_files/session7/session7-235.jpg: 576x640 1 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-235.jpg\n","image 153/283 ../../data_files/session7/session7-236.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-236.jpg\n","image 154/283 ../../data_files/session7/session7-237.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-237.jpg\n","image 155/283 ../../data_files/session7/session7-238.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-238.jpg\n","image 156/283 ../../data_files/session7/session7-239.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-239.jpg\n","image 157/283 ../../data_files/session7/session7-24.jpg: 576x640 4 blemisheds, 7 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-24.jpg\n","image 158/283 ../../data_files/session7/session7-240.jpg: 576x640 1 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-240.jpg\n","image 159/283 ../../data_files/session7/session7-241.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-241.jpg\n","image 160/283 ../../data_files/session7/session7-242.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-242.jpg\n","image 161/283 ../../data_files/session7/session7-243.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-243.jpg\n","image 162/283 ../../data_files/session7/session7-244.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-244.jpg\n","image 163/283 ../../data_files/session7/session7-245.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-245.jpg\n","image 164/283 ../../data_files/session7/session7-246.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-246.jpg\n","image 165/283 ../../data_files/session7/session7-247.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-247.jpg\n","image 166/283 ../../data_files/session7/session7-248.jpg: 576x640 1 blemisheds, 4 unblemisheds, 1 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-248.jpg\n","image 167/283 ../../data_files/session7/session7-249.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-249.jpg\n","image 168/283 ../../data_files/session7/session7-25.jpg: 512x640 3 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-25.jpg\n","image 169/283 ../../data_files/session7/session7-250.jpg: 576x640 5 unblemisheds, 3 gloves, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-250.jpg\n","image 170/283 ../../data_files/session7/session7-251.jpg: 576x640 5 unblemisheds, 3 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-251.jpg\n","image 171/283 ../../data_files/session7/session7-252.jpg: 576x640 5 unblemisheds, 3 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-252.jpg\n","image 172/283 ../../data_files/session7/session7-253.jpg: 576x640 5 unblemisheds, 3 gloves, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-253.jpg\n","image 173/283 ../../data_files/session7/session7-254.jpg: 576x640 5 unblemisheds, 3 gloves, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-254.jpg\n","image 174/283 ../../data_files/session7/session7-255.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-255.jpg\n","image 175/283 ../../data_files/session7/session7-256.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-256.jpg\n","image 176/283 ../../data_files/session7/session7-257.jpg: 576x640 5 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-257.jpg\n","image 177/283 ../../data_files/session7/session7-258.jpg: 576x640 1 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-258.jpg\n","image 178/283 ../../data_files/session7/session7-259.jpg: 576x640 1 blemisheds, 5 unblemisheds, 3 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-259.jpg\n","image 179/283 ../../data_files/session7/session7-26.jpg: 512x640 3 blemisheds, 5 unblemisheds, 2 gloves, 3 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-26.jpg\n","image 180/283 ../../data_files/session7/session7-260.jpg: 576x640 1 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.051s)\n","../../data_files/cc7/session7-260.jpg\n","image 181/283 ../../data_files/session7/session7-261.jpg: 576x640 5 unblemisheds, 3 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-261.jpg\n","image 182/283 ../../data_files/session7/session7-262.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-262.jpg\n","image 183/283 ../../data_files/session7/session7-263.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-263.jpg\n","image 184/283 ../../data_files/session7/session7-264.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 1 heads, Done. (0.054s)\n","../../data_files/cc7/session7-264.jpg\n","image 185/283 ../../data_files/session7/session7-265.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-265.jpg\n","image 186/283 ../../data_files/session7/session7-266.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-266.jpg\n","image 187/283 ../../data_files/session7/session7-267.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-267.jpg\n","image 188/283 ../../data_files/session7/session7-268.jpg: 576x640 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-268.jpg\n","image 189/283 ../../data_files/session7/session7-269.jpg: 576x640 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-269.jpg\n","image 190/283 ../../data_files/session7/session7-27.jpg: 512x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-27.jpg\n","image 191/283 ../../data_files/session7/session7-270.jpg: 576x640 4 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-270.jpg\n","image 192/283 ../../data_files/session7/session7-271.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-271.jpg\n","image 193/283 ../../data_files/session7/session7-272.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-272.jpg\n","image 194/283 ../../data_files/session7/session7-273.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-273.jpg\n","image 195/283 ../../data_files/session7/session7-274.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-274.jpg\n","image 196/283 ../../data_files/session7/session7-275.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-275.jpg\n","image 197/283 ../../data_files/session7/session7-276.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-276.jpg\n","image 198/283 ../../data_files/session7/session7-277.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-277.jpg\n","image 199/283 ../../data_files/session7/session7-278.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-278.jpg\n","image 200/283 ../../data_files/session7/session7-279.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-279.jpg\n","image 201/283 ../../data_files/session7/session7-28.jpg: 512x640 3 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-28.jpg\n","image 202/283 ../../data_files/session7/session7-280.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.051s)\n","../../data_files/cc7/session7-280.jpg\n","image 203/283 ../../data_files/session7/session7-281.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-281.jpg\n","image 204/283 ../../data_files/session7/session7-282.jpg: 576x640 4 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-282.jpg\n","image 205/283 ../../data_files/session7/session7-283.jpg: 576x640 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-283.jpg\n","image 206/283 ../../data_files/session7/session7-29.jpg: 512x640 3 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-29.jpg\n","image 207/283 ../../data_files/session7/session7-3.jpg: 576x640 3 blemisheds, 7 unblemisheds, 3 gloves, 2 belts, 2 bins, 1 heads, Done. (0.051s)\n","../../data_files/cc7/session7-3.jpg\n","image 208/283 ../../data_files/session7/session7-30.jpg: 512x640 4 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-30.jpg\n","image 209/283 ../../data_files/session7/session7-31.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 1 bins, 1 heads, Done. (0.051s)\n","../../data_files/cc7/session7-31.jpg\n","image 210/283 ../../data_files/session7/session7-32.jpg: 576x640 3 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-32.jpg\n","image 211/283 ../../data_files/session7/session7-33.jpg: 512x640 3 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 1 bins, 1 heads, Done. (0.046s)\n","../../data_files/cc7/session7-33.jpg\n","image 212/283 ../../data_files/session7/session7-34.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.052s)\n","../../data_files/cc7/session7-34.jpg\n","image 213/283 ../../data_files/session7/session7-35.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-35.jpg\n","image 214/283 ../../data_files/session7/session7-36.jpg: 576x640 4 blemisheds, 5 unblemisheds, 3 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-36.jpg\n","image 215/283 ../../data_files/session7/session7-37.jpg: 576x640 4 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-37.jpg\n","image 216/283 ../../data_files/session7/session7-38.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-38.jpg\n","image 217/283 ../../data_files/session7/session7-39.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 3 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-39.jpg\n","image 218/283 ../../data_files/session7/session7-4.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-4.jpg\n","image 219/283 ../../data_files/session7/session7-40.jpg: 576x640 5 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-40.jpg\n","image 220/283 ../../data_files/session7/session7-41.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-41.jpg\n","image 221/283 ../../data_files/session7/session7-42.jpg: 576x640 5 blemisheds, 3 unblemisheds, 3 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-42.jpg\n","image 222/283 ../../data_files/session7/session7-43.jpg: 576x640 4 blemisheds, 4 unblemisheds, 3 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-43.jpg\n","image 223/283 ../../data_files/session7/session7-44.jpg: 576x640 4 blemisheds, 4 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-44.jpg\n","image 224/283 ../../data_files/session7/session7-45.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-45.jpg\n","image 225/283 ../../data_files/session7/session7-46.jpg: 576x640 6 blemisheds, 5 unblemisheds, 2 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-46.jpg\n","image 226/283 ../../data_files/session7/session7-47.jpg: 576x640 5 blemisheds, 5 unblemisheds, 3 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-47.jpg\n","image 227/283 ../../data_files/session7/session7-48.jpg: 576x640 4 blemisheds, 6 unblemisheds, 3 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-48.jpg\n","image 228/283 ../../data_files/session7/session7-49.jpg: 576x640 4 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-49.jpg\n","image 229/283 ../../data_files/session7/session7-5.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-5.jpg\n","image 230/283 ../../data_files/session7/session7-50.jpg: 576x640 5 blemisheds, 4 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-50.jpg\n","image 231/283 ../../data_files/session7/session7-51.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-51.jpg\n","image 232/283 ../../data_files/session7/session7-52.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-52.jpg\n","image 233/283 ../../data_files/session7/session7-53.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-53.jpg\n","image 234/283 ../../data_files/session7/session7-54.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 1 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-54.jpg\n","image 235/283 ../../data_files/session7/session7-55.jpg: 576x640 5 blemisheds, 4 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-55.jpg\n","image 236/283 ../../data_files/session7/session7-56.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-56.jpg\n","image 237/283 ../../data_files/session7/session7-57.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-57.jpg\n","image 238/283 ../../data_files/session7/session7-58.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-58.jpg\n","image 239/283 ../../data_files/session7/session7-59.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-59.jpg\n","image 240/283 ../../data_files/session7/session7-6.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-6.jpg\n","image 241/283 ../../data_files/session7/session7-60.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-60.jpg\n","image 242/283 ../../data_files/session7/session7-61.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-61.jpg\n","image 243/283 ../../data_files/session7/session7-62.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-62.jpg\n","image 244/283 ../../data_files/session7/session7-63.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-63.jpg\n","image 245/283 ../../data_files/session7/session7-64.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-64.jpg\n","image 246/283 ../../data_files/session7/session7-65.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-65.jpg\n","image 247/283 ../../data_files/session7/session7-66.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-66.jpg\n","image 248/283 ../../data_files/session7/session7-67.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-67.jpg\n","image 249/283 ../../data_files/session7/session7-68.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-68.jpg\n","image 250/283 ../../data_files/session7/session7-69.jpg: 576x640 5 blemisheds, 6 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-69.jpg\n","image 251/283 ../../data_files/session7/session7-7.jpg: 576x640 3 blemisheds, 6 unblemisheds, 3 gloves, 1 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-7.jpg\n","image 252/283 ../../data_files/session7/session7-70.jpg: 576x640 4 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-70.jpg\n","image 253/283 ../../data_files/session7/session7-71.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-71.jpg\n","image 254/283 ../../data_files/session7/session7-72.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-72.jpg\n","image 255/283 ../../data_files/session7/session7-73.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-73.jpg\n","image 256/283 ../../data_files/session7/session7-74.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-74.jpg\n","image 257/283 ../../data_files/session7/session7-75.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-75.jpg\n","image 258/283 ../../data_files/session7/session7-76.jpg: 576x640 6 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.053s)\n","../../data_files/cc7/session7-76.jpg\n","image 259/283 ../../data_files/session7/session7-77.jpg: 576x640 6 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-77.jpg\n","image 260/283 ../../data_files/session7/session7-78.jpg: 576x640 5 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-78.jpg\n","image 261/283 ../../data_files/session7/session7-79.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-79.jpg\n","image 262/283 ../../data_files/session7/session7-8.jpg: 576x640 4 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 2 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-8.jpg\n","image 263/283 ../../data_files/session7/session7-80.jpg: 576x640 5 blemisheds, 4 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-80.jpg\n","image 264/283 ../../data_files/session7/session7-81.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-81.jpg\n","image 265/283 ../../data_files/session7/session7-82.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-82.jpg\n","image 266/283 ../../data_files/session7/session7-83.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-83.jpg\n","image 267/283 ../../data_files/session7/session7-84.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-84.jpg\n","image 268/283 ../../data_files/session7/session7-85.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-85.jpg\n","image 269/283 ../../data_files/session7/session7-86.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-86.jpg\n","image 270/283 ../../data_files/session7/session7-87.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-87.jpg\n","image 271/283 ../../data_files/session7/session7-88.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-88.jpg\n","image 272/283 ../../data_files/session7/session7-89.jpg: 576x640 5 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-89.jpg\n","image 273/283 ../../data_files/session7/session7-9.jpg: 576x640 3 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 1 heads, Done. (0.050s)\n","../../data_files/cc7/session7-9.jpg\n","image 274/283 ../../data_files/session7/session7-90.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-90.jpg\n","image 275/283 ../../data_files/session7/session7-91.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-91.jpg\n","image 276/283 ../../data_files/session7/session7-92.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-92.jpg\n","image 277/283 ../../data_files/session7/session7-93.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-93.jpg\n","image 278/283 ../../data_files/session7/session7-94.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-94.jpg\n","image 279/283 ../../data_files/session7/session7-95.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-95.jpg\n","image 280/283 ../../data_files/session7/session7-96.jpg: 576x640 5 blemisheds, 6 unblemisheds, 2 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-96.jpg\n","image 281/283 ../../data_files/session7/session7-97.jpg: 576x640 5 blemisheds, 5 unblemisheds, 2 gloves, 2 belts, 2 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-97.jpg\n","image 282/283 ../../data_files/session7/session7-98.jpg: 576x640 5 blemisheds, 5 unblemisheds, 3 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-98.jpg\n","image 283/283 ../../data_files/session7/session7-99.jpg: 576x640 5 blemisheds, 5 unblemisheds, 1 gloves, 2 belts, 1 bins, 2 heads, Done. (0.050s)\n","../../data_files/cc7/session7-99.jpg\n","Results saved to /content/drive/My Drive/Research/SAnet/yolov5/yolov5/../../data_files/cc7\n","Done. (118.703s)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3kanvU7-HW0e"},"source":["# Data loader"]},{"cell_type":"code","metadata":{"id":"PmtP3o2oZu8_"},"source":["## Some paths used for loading annotation and data\n","ANNOS_PATH = '/content/drive/My Drive/Research/SAnet/data_files/annos/'\n","DATA_PATH = '/content/drive/My Drive/Research/SAnet/data_files/'\n","\n","\n","## Images will be normalised using this\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4e33cKyno2U"},"source":["\n","## Read annotation file and return dataframe of annotations with 'session number', 'Image','O_loc','EE_loc'\n","def read_anno_file(file_path):\n","    session_annos = pd.read_csv(file_path)[['Image','O_loc','EE_loc']]\n","    session_annos['session'] = int( (file_path).split('_annos.csv')[0][-1] )\n","    return session_annos\n","\n","## Read annotation files\n","\n","# session1_annos = read_anno_file(ANNOS_PATH+'session1_annos.csv')\n","# session2_annos = read_anno_file(ANNOS_PATH+'session2_annos.csv')\n","session3_annos = read_anno_file(ANNOS_PATH+'session3_annos.csv')\n","session4_annos = read_anno_file(ANNOS_PATH+'session4_annos.csv')\n","session5_annos = read_anno_file(ANNOS_PATH+'session5_annos.csv')\n","session6_annos = read_anno_file(ANNOS_PATH+'session6_annos.csv')\n","session7_annos = read_anno_file(ANNOS_PATH+'session7_annos.csv')\n","\n","## Concatenate annotaion frames\n","data = pd.concat([\n","                  # session1_annos, \n","                  # session2_annos, \n","                  session3_annos, \n","                  session4_annos, \n","                  session5_annos, \n","                  session6_annos, \n","                  session7_annos]).reset_index(drop=True)\n","\n","data.loc[(data.O_loc == 0) & (data.EE_loc == 2), 'O_loc'] = 2\n","#data.groupby(['O_loc','EE_loc']).size().reset_index(name='counts')\n","\n","print('annotations',data.head())\n","\n","\n","\n","## loading all images to images_lib\n","images_lib = {}\n","for index in range(data.shape[0]):\n","    print(index)\n","    session_name = 'session'+str(data.loc[index, 'session'])\n","    img_name = session_name+'-'+str(data.loc[index, 'Image'])+'.jpg'\n","    img_name = os.path.join(DATA_PATH+session_name, img_name)\n","    image = Image.open(img_name)\n","    \n","    #image = image.convert('RGB')\n","    image = image.resize((640,480))\n","    label = torch.tensor(data.loc[index, ['EE_loc', 'O_loc']])\n","    if transform is not None:\n","        image = transform(image)\n","        images_lib[img_name] = image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj95B1RVZqYl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226521763,"user_tz":300,"elapsed":94,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"789a9d73-12f3-4637-d43b-9fb54a5756ce"},"source":["\n","\n","## Shuffling and splitting dataset into train val test dataset. Split = 0.7:0.15:0.15\n","batch_size = 32\n","shuffle_dataset = False\n","random_seed= 42\n","dataset_size = len(data) \n","indices = list(range(dataset_size))\n","split1 = int(np.floor(0.7 * dataset_size))\n","split2 = int(np.floor(0.15 * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","test_indices = indices[576:700]+indices[2065:] \n","test1_indices = indices[2065:] # test1_indices is not required\n","train_val_indices = indices[:576]+indices[700:2065] \n","\n","np.random.seed(random_seed)\n","np.random.shuffle(train_val_indices)\n","train_indices = train_val_indices[:1358]\n","val_indices = train_val_indices[1358:]\n","\n","# shuffling only train and val indices\n","np.random.seed(random_seed)\n","np.random.seed(random_seed)\n","\n","print('train_indices', train_indices)\n","print('val_indices', val_indices)\n","print('test_indices', test_indices)\n","print('test1_indices', test1_indices)\n","\n","print('len train_indices', len(train_indices))\n","print('len val_indices', len(val_indices))\n","print('len test_indices', len(test_indices))\n","print('len test1_indices', len(test1_indices))\n","\n","\n","## Dataset Class\n","class Arthopod_Dataset(Dataset):\n","    def __init__(self, data, transform, indexes):\n","        self.data = data\n","        self.transform = transform\n","        self.indices = indexes\n","        \n","    def __len__(self):\n","        return len(self.indices)\n","    \n","    def __getitem__(self, index):\n","        index = self.indices[index]\n","        session_name = 'session'+str(self.data.loc[index, 'session'])\n","        img_name = session_name+'-'+str(self.data.loc[index, 'Image'])+'.jpg'\n","        img_name = os.path.join(DATA_PATH+session_name, img_name)\n","        image = images_lib[img_name]\n","        label = torch.tensor(self.data.loc[index, ['EE_loc', 'O_loc']])\n","        return image, label\n","\n","  \n","\n","train_dataset = Arthopod_Dataset(data,transform, train_indices)\n","val_dataset = Arthopod_Dataset(data,transform, val_indices)\n","test_dataset = Arthopod_Dataset(data,transform, test_indices)\n","test1_dataset = Arthopod_Dataset(data,transform, test1_indices)\n","\n","## Creating PT data samplers and loaders:\n","train_sampler = SequentialSampler( train_indices)\n","valid_sampler = SequentialSampler( val_indices)\n","test_sampler = SequentialSampler( test_indices)\n","test1_sampler = SequentialSampler( test1_indices)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,sampler=valid_sampler)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,sampler=test_sampler)\n","test1_loader = torch.utils.data.DataLoader(test1_dataset, batch_size=batch_size,sampler=test1_sampler)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_indices [1729, 1626, 70, 1100, 1176, 1326, 1985, 867, 543, 1979, 1417, 429, 1523, 818, 708, 1098, 1245, 2007, 548, 1133, 2044, 212, 307, 1262, 69, 432, 1957, 1932, 128, 1818, 1475, 731, 1027, 1359, 1490, 1208, 247, 1865, 1171, 1270, 1426, 56, 1726, 196, 494, 962, 1612, 2024, 798, 239, 1798, 1616, 1513, 275, 1054, 1837, 717, 383, 1121, 529, 1012, 1750, 898, 538, 905, 1371, 744, 351, 1731, 868, 1311, 342, 1618, 1917, 1551, 303, 993, 1890, 2000, 483, 722, 1131, 1881, 324, 1648, 1283, 439, 111, 1839, 65, 188, 802, 1853, 331, 29, 297, 986, 99, 1743, 931, 1826, 1914, 256, 1124, 1357, 2031, 734, 353, 1861, 251, 879, 344, 1429, 1996, 120, 233, 1600, 479, 1594, 414, 1382, 519, 1181, 1109, 1444, 1103, 231, 1998, 1789, 109, 411, 135, 754, 943, 1642, 289, 983, 761, 1871, 1952, 1820, 1073, 1804, 1690, 237, 426, 1428, 374, 2055, 1752, 1801, 1332, 350, 1281, 801, 210, 775, 1538, 1198, 1634, 480, 1564, 1373, 741, 2060, 298, 23, 1892, 63, 270, 901, 1177, 1226, 316, 1889, 1325, 1841, 382, 1074, 1356, 100, 352, 2004, 1358, 2001, 1746, 857, 572, 936, 1158, 254, 1993, 1828, 115, 707, 1536, 1687, 738, 1375, 883, 1573, 367, 1153, 1191, 173, 1627, 1775, 1340, 1408, 518, 507, 1125, 1986, 32, 1033, 464, 1320, 714, 124, 834, 163, 203, 1529, 794, 1942, 1780, 49, 438, 1955, 1030, 1622, 1479, 922, 1376, 1240, 1593, 495, 433, 1453, 218, 720, 1257, 1483, 1964, 1174, 2030, 220, 865, 1970, 78, 59, 825, 366, 1672, 2052, 1118, 73, 162, 1915, 926, 1706, 1130, 555, 365, 45, 1541, 1519, 742, 76, 1982, 1494, 544, 1542, 398, 1717, 895, 30, 1869, 1813, 1028, 880, 1923, 1413, 1675, 1645, 1677, 1702, 44, 1856, 168, 1685, 471, 1990, 570, 1334, 1989, 1510, 997, 1117, 305, 306, 1167, 185, 723, 1414, 453, 1016, 1199, 1087, 1678, 1592, 1751, 1039, 1503, 427, 1688, 123, 1407, 354, 1477, 1988, 1088, 1425, 361, 194, 1754, 416, 1788, 322, 1284, 266, 1666, 67, 1013, 514, 2003, 854, 420, 1936, 462, 1737, 956, 1474, 1711, 778, 1465, 482, 1610, 1816, 1735, 1766, 1072, 1393, 1654, 735, 981, 261, 1029, 240, 1242, 774, 1607, 1178, 1469, 1691, 552, 1112, 1724, 1185, 71, 712, 259, 51, 1063, 394, 733, 332, 1439, 1636, 1306, 839, 1901, 967, 535, 1671, 530, 737, 554, 1396, 2041, 932, 804, 534, 192, 575, 450, 706, 881, 726, 1584, 141, 2, 830, 1938, 1507, 339, 1360, 486, 1102, 2005, 803, 271, 184, 1003, 244, 1441, 1613, 2042, 1090, 979, 1515, 2057, 415, 273, 1695, 1565, 1814, 250, 413, 1987, 1880, 1643, 310, 381, 1402, 425, 485, 831, 844, 1686, 408, 1366, 998, 1712, 1656, 527, 1422, 182, 1621, 170, 292, 526, 1034, 300, 1944, 1187, 1728, 985, 1997, 1458, 1236, 1249, 1368, 422, 371, 1214, 743, 1355, 1106, 1833, 175, 561, 1511, 1725, 1437, 334, 1761, 1038, 1255, 1142, 752, 1111, 869, 567, 198, 15, 265, 1794, 912, 1056, 1870, 1812, 1598, 1046, 958, 1591, 1364, 465, 2018, 199, 1463, 1771, 1294, 1978, 2015, 1395, 1544, 478, 1831, 2022, 1857, 1699, 1047, 1480, 1557, 906, 532, 43, 274, 101, 937, 1633, 1314, 107, 948, 497, 2045, 1287, 896, 1005, 1667, 1847, 700, 1041, 72, 1935, 1302, 1256, 715, 481, 1161, 506, 1912, 870, 2054, 1931, 1151, 311, 989, 405, 2039, 1324, 309, 226, 1773, 1505, 1885, 1011, 973, 148, 709, 1197, 904, 1601, 909, 557, 1887, 1244, 1427, 1577, 1492, 286, 1682, 817, 876, 2012, 1665, 1123, 1317, 1676, 2046, 2027, 1128, 846, 1129, 1398, 1502, 1387, 493, 1852, 277, 1916, 358, 1580, 236, 551, 1937, 965, 208, 1466, 1638, 58, 1662, 1929, 1655, 363, 505, 755, 1211, 1681, 1940, 1908, 1617, 891, 118, 1335, 845, 522, 513, 81, 1192, 1394, 380, 1763, 84, 1753, 329, 6, 824, 1147, 1230, 942, 451, 490, 1560, 287, 1953, 2050, 1448, 1553, 836, 1951, 1776, 1309, 1300, 988, 1696, 1727, 327, 1108, 1795, 1961, 1031, 1629, 177, 816, 1447, 86, 1488, 2006, 457, 1898, 54, 1581, 767, 221, 113, 1468, 1389, 1060, 174, 860, 713, 1922, 18, 423, 1423, 285, 1661, 428, 1589, 829, 793, 31, 1065, 1055, 214, 889, 326, 560, 1202, 828, 1797, 341, 753, 147, 1285, 360, 83, 48, 468, 155, 851, 2019, 1288, 1482, 1446, 1707, 500, 1097, 179, 1134, 1086, 1316, 1710, 1697, 528, 346, 1467, 2025, 1235, 2037, 1070, 725, 1736, 243, 1641, 1160, 1555, 312, 435, 1965, 1745, 308, 430, 784, 370, 566, 1040, 181, 2056, 745, 1971, 1404, 1416, 1910, 1434, 296, 969, 461, 1383, 1545, 377, 1660, 105, 126, 1220, 911, 355, 1903, 806, 888, 1157, 209, 1361, 140, 1517, 1552, 1525, 2009, 1412, 960, 835, 1349, 933, 1518, 10, 1769, 1213, 282, 291, 878, 750, 1572, 930, 129, 1350, 323, 1182, 916, 1024, 1934, 934, 941, 819, 215, 1585, 158, 1049, 758, 516, 1508, 721, 195, 1720, 1344, 410, 96, 1061, 1010, 156, 419, 1224, 477, 1450, 792, 1122, 1939, 1683, 447, 1582, 178, 390, 702, 1067, 1781, 1146, 1860, 1330, 944, 1328, 1045, 1137, 788, 142, 1596, 1509, 421, 760, 1486, 923, 1409, 739, 294, 1442, 1756, 1972, 88, 41, 1873, 1999, 131, 1062, 1050, 1888, 1924, 1459, 1578, 164, 848, 290, 1231, 1614, 1963, 1693, 902, 963, 348, 2010, 571, 1803, 1954, 1548, 1669, 1397, 1902, 458, 375, 333, 1248, 1832, 1976, 940, 1204, 1064, 1926, 1392, 1995, 1215, 328, 1114, 318, 1899, 1438, 1805, 167, 12, 1992, 171, 773, 260, 1301, 1590, 1025, 796, 258, 968, 991, 1700, 138, 1017, 1907, 549, 1110, 2051, 1322, 1464, 558, 139, 1807, 531, 224, 847, 910, 1792, 1066, 409, 325, 1410, 299, 1345, 1229, 25, 1928, 838, 1068, 193, 440, 573, 503, 729, 1261, 1252, 1858, 970, 3, 1238, 1640, 467, 321, 5, 222, 1454, 547, 228, 39, 542, 242, 1022, 1462, 136, 2011, 1827, 1650, 1092, 953, 445, 966, 1179, 1498, 1546, 1895, 705, 1836, 2034, 785, 436, 1925, 376, 1556, 1089, 1485, 314, 1810, 442, 1234, 319, 281, 815, 1730, 2063, 424, 553, 718, 2028, 368, 1723, 1471, 746, 1718, 1692, 1579, 227, 1207, 1313, 1411, 1941, 66, 393, 703, 24, 211, 1615, 144, 536, 935, 1209, 1164, 85, 1958, 272, 1076, 1369, 1774, 1571, 1876, 1452, 1530, 362, 1689, 1241, 1150, 1154, 1305, 1631, 1165, 789, 364, 77, 27, 1516, 525, 1904, 55, 762, 302, 842, 748, 7, 927, 1173, 336, 47, 1559, 106, 349, 238, 213, 1354, 1227, 917, 1293, 1748, 523, 232, 768, 1587, 533, 1588, 727, 1297, 1385, 60, 1346, 92, 790, 280, 1203, 359, 1218, 1258, 971, 1457, 110, 42, 125, 204, 52, 545, 68, 1268, 1625, 183, 137, 1023, 920, 132, 80, 1823, 541, 1673, 1107, 165, 248, 446, 1237, 982, 1299, 97, 2038, 861, 1668, 102, 434, 74, 1863, 1289, 1042, 1786, 1747, 1698, 886, 1113, 388, 235, 1791, 267, 1275, 117, 1684, 797, 249, 1844, 2059, 94, 33, 1543, 1744, 448, 840, 1347, 1597, 0, 1455, 1225, 1232, 82, 1884, 800, 1531, 1170, 894, 1303, 1363, 1764, 1568, 404, 1800, 1379, 1440, 104, 812, 1386, 950, 62, 976, 1433, 223, 145, 1212, 373, 1500, 1266, 1897, 1574, 1679, 1353, 9, 449, 832, 255, 1006, 389, 1032, 1048, 1778, 1664, 460, 1352, 1292, 1082, 501, 863, 1973, 992, 855, 539, 821, 1499, 2043, 1269, 1155, 372, 791, 1605, 357, 1091, 907, 859, 1159, 1094, 1604, 1566, 1835, 1570, 1768, 1790, 1009, 79, 1296, 938, 1527, 795, 1611, 57, 133, 1264, 1057, 1759, 947, 1338, 504, 1866, 347, 386, 1148, 1018, 952, 1960, 475, 1799, 1476, 776, 172, 1273, 38, 837, 90, 2002, 1329, 999, 1431, 1217, 1279, 1443, 2023, 499, 338, 1312, 28, 1341, 1405, 1891, 1069, 169, 1484, 1321, 1377, 1036, 491, 1077, 1221, 1156, 711, 874, 781, 234, 856, 2053, 1980, 1280, 1549, 1504, 1705, 1058, 1333, 1189, 108, 1716, 568, 1291, 1298, 1014, 1071, 1172, 1528, 1741, 1526, 1008, 1749, 974, 858, 841, 454, 264, 75, 1734, 444, 395, 176, 1166, 1808, 1481, 1460, 719, 61, 1420, 996, 1883, 1946, 278, 36, 1595, 786, 1877, 1843, 1101, 1649, 862, 191, 1043, 456, 799, 884, 1878, 1680, 852, 412, 1222, 949, 1576, 740, 114, 417, 1977, 1093, 728, 1984, 89, 972, 1599, 1983, 877, 498, 1400, 1449, 11, 396]\n","val_indices [284, 1190, 1315, 1817, 814, 2026, 1943, 813, 975, 159, 1019, 1850, 257, 335, 1947, 1163, 515, 1472, 1139, 892, 1825, 885, 1370, 1327, 1547, 2048, 521, 22, 356, 1401, 1436, 1758, 340, 431, 473, 217, 1035, 1351, 1674, 1135, 2014, 93, 1132, 1194, 1216, 1670, 1432, 820, 1864, 1120, 153, 1223, 1793, 2062, 913, 704, 827, 1336, 1419, 1742, 1384, 487, 559, 1265, 1243, 925, 512, 757, 1834, 1859, 2021, 1274, 914, 116, 864, 1765, 1205, 954, 119, 780, 1974, 759, 369, 268, 779, 1059, 946, 990, 46, 1497, 1783, 1115, 1000, 897, 1343, 2064, 4, 1127, 1796, 263, 1647, 1900, 443, 1126, 1201, 961, 1862, 304, 1489, 1635, 313, 149, 1272, 1337, 574, 50, 1630, 850, 957, 1501, 1239, 1290, 470, 1095, 777, 399, 511, 320, 19, 808, 35, 1026, 951, 1868, 407, 1044, 537, 882, 1186, 1323, 245, 1540, 1251, 908, 1815, 489, 154, 1085, 977, 1435, 749, 809, 2040, 569, 1975, 17, 127, 1470, 1403, 2033, 1051, 1104, 2029, 190, 1263, 1116, 2020, 1733, 730, 180, 301, 496, 1770, 1784, 1738, 1715, 517, 2049, 1193, 476, 157, 1367, 1319, 16, 1659, 1196, 2036, 546, 1663, 782, 1854, 1143, 1739, 1628, 1083, 1949, 283, 921, 1704, 225, 26, 1842, 437, 2013, 1512, 229, 37, 873, 1169, 1522, 469, 1855, 2032, 811, 1913, 763, 1851, 1342, 1282, 1539, 1651, 1732, 1424, 1893, 924, 160, 1080, 1310, 945, 1782, 1001, 1037, 152, 1906, 2058, 509, 1259, 1374, 1740, 103, 1569, 1567, 710, 1840, 1267, 53, 151, 403, 2008, 207, 1962, 1246, 1927, 8, 1138, 1867, 1184, 452, 253, 1451, 1020, 1806, 1004, 1719, 1956, 747, 345, 1777, 262, 1445, 1562, 150, 472, 764, 2047, 1950, 550, 1052, 488, 1882, 1295, 146, 1779, 1994, 402, 1078, 1945, 783, 1496, 463, 1473, 186, 1271, 1348, 732, 143, 875, 1658, 1105, 197, 1757, 1007, 279, 293, 1894, 400, 122, 1331, 202, 1586, 959, 246, 1563, 1968, 1948, 1277, 1785, 1550, 384, 1830, 978, 1713, 219, 765, 1, 112, 822, 1981, 1075, 1772, 1760, 1821, 1534, 1533, 1657, 1415, 1879, 441, 1644, 1260, 1380, 1933, 1755, 1819, 1521, 787, 1905, 1381, 1845, 1824, 317, 772, 1558, 1603, 833, 1506, 1406, 1096, 751, 756, 1430, 1524, 1532, 1372, 2017, 1378, 1620, 1495, 1653, 1787, 1478, 1991, 1967, 1767, 1918, 919, 769, 1136, 1909, 556, 805, 701, 1233, 1390, 1846, 1307, 524, 1183, 540, 1318, 1802, 872, 1921, 1554, 484, 95, 1144, 563, 1388, 866, 987, 1722, 1838, 1608, 1015, 1701, 1162, 206, 392, 918, 994, 397, 890, 1911, 1365, 1152, 766, 1959, 736, 1886, 1084, 1694, 849, 807, 98, 928, 1875, 1721, 406, 502, 1195, 1180, 1053, 903, 200, 134, 1175, 40, 1141, 230, 1637, 1920, 378, 288, 418, 1822, 391, 716, 1646, 1286, 1210, 1619, 771, 1276, 1491, 520, 64, 14, 1304, 1188, 492, 379, 187, 887, 216, 915, 1200, 1002, 337, 1896, 843, 1872, 295, 1849, 1140, 1399, 1874, 1609, 455, 2061, 939, 269, 1119, 201, 161, 1703, 853, 401, 826, 1624, 1969, 1919, 1253, 565, 1537, 1145, 1149, 1535, 1709, 1228, 205, 34, 899, 508, 1575, 1602, 2035, 1514, 91, 1487, 1021, 564, 1493, 900, 241, 13, 315, 724, 387, 1421, 166, 964, 20, 770, 1966, 1632, 1278, 1652, 955, 1391, 1714, 562, 2016, 810, 1081, 189, 1930, 1099, 823, 1623, 1829, 510, 1206, 474, 1708, 980, 871, 252, 21, 1461, 459, 1308, 276, 1079, 1339, 385, 929, 1561, 1639, 343, 893, 1456, 1809, 130, 1811, 995, 1247, 1520, 87, 1606, 330, 1362, 466, 121, 1762, 1168, 1848, 1219, 1254, 1418, 984, 1583, 1250]\n","test_indices [576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347]\n","test1_indices [2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347]\n","train_indices 1358\n","val_indices 583\n","test_indices 407\n","test1_indices 283\n","train_indices 1358\n","val_indices 583\n","test_indices 407\n","test1_indices 283\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zE6G7Xf372gy"},"source":["# Neural Net\n"]},{"cell_type":"code","metadata":{"id":"9Hs9WCKP74z0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226521764,"user_tz":300,"elapsed":75,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"f972bddc-2904-442f-ea9e-05e5ee4dcc8a"},"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,9), stride=1)\n","        self.pool1 = nn.MaxPool2d(4, 3)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,9), stride=1)\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,9), stride=1)\n","        self.pool2 = nn.MaxPool2d(2, 3)\n","        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,9), stride=1)\n","        self.conv5 = nn.Conv2d(32, 32, kernel_size=(3,9), stride=1)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        \n","        self.fc1_1 = nn.Linear((32*24*24), 128) ### for input shape: [1, 3, 480, 640], the output shape is: [1, 32, 24, 24]\n","        self.fc1_2 = nn.Linear(128, 64)\n","        self.fc1_3 = nn.Linear(64, 32)\n","        self.fc1_4 = nn.Linear(32, 4) # for ee_loc\n","        self.fc1_5 = nn.Linear(32, 4) # for o_loc\n","\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.pool1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.pool2(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.pool3(x) \n","        \n","        x = x.view(-1, 32*24*24) # Flatten layer\n","        x = self.fc1_1(x)\n","        x = self.fc1_2(x)\n","        x = self.fc1_3(x)\n","        x1 = self.fc1_4(x) ## for ee_loc\n","        x2 = self.fc1_5(x) ## for o_loc\n","        x1 = F.softmax(x1,dim = 1)\n","        x2 = F.softmax(x2,dim = 1)\n","        return x1, x2 \n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)\n","\n","# model = Net() # On CPU\n","model = Net().to(device)  # On GPU\n","print(model)\n","\n","\n","## Weights based on data imbalance for ee_loc\n","w1 = torch.tensor([443, 525, 211, 762], dtype=torch.float32)\n","w1 = 1.0 / w1\n","w1 = w1 / w1.sum()\n","w1 = torch.FloatTensor(w1).cuda()\n","criterion1 = nn.CrossEntropyLoss(weight=w1)\n","# criterion1 = nn.CrossEntropyLoss()\n","\n","\n","## Weights based on data imbalance for o_loc\n","w2 = torch.tensor([719, 525, 211, 486], dtype=torch.float32)\n","w2 = 1.0 / w2\n","w2 = w2 / w2.sum()\n","w2 = torch.FloatTensor(w2).cuda()\n","criterion2= nn.CrossEntropyLoss(weight=w2)\n","# criterion2= nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","def accuracy(out, labels):\n","    _,pred = torch.max(out, dim=1)\n","    return torch.sum(pred==labels).item()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n","Net(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 9), stride=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 1))\n","  (conv3): Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv4): Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 1))\n","  (conv5): Conv2d(32, 32, kernel_size=(3, 9), stride=(1, 1))\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1_1): Linear(in_features=18432, out_features=128, bias=True)\n","  (fc1_2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc1_3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc1_4): Linear(in_features=32, out_features=4, bias=True)\n","  (fc1_5): Linear(in_features=32, out_features=4, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fEUMXZcvdGSY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1603943035983,"user_tz":240,"elapsed":313,"user":{"displayName":"Ehsan Asali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9TPBD9D7cECX0PHZtuuxSB8zI0mb6m3G46NH6Rw=s64","userId":"01624248885938823239"}},"outputId":"b74c7516-c465-4681-dd65-68062fb63f85"},"source":["w1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.17094, 0.37821, 0.34579, 0.10506], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"0wPY5OsUdGE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1603943039039,"user_tz":240,"elapsed":333,"user":{"displayName":"Ehsan Asali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9TPBD9D7cECX0PHZtuuxSB8zI0mb6m3G46NH6Rw=s64","userId":"01624248885938823239"}},"outputId":"8acaa1e0-95b1-43d1-ce86-b23a08891b17"},"source":["w2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.10678, 0.37908, 0.34659, 0.16755], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"ERq53azy8y29"},"source":["# Training\n"]},{"cell_type":"code","metadata":{"id":"6mEO7STb8puj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606244297428,"user_tz":300,"elapsed":53067,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"66672eab-1c8d-4103-d182-a8b39db0ec42"},"source":["n_epochs = 1000\n","print_every = 10\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc1 = []\n","val_acc2 = []\n","train_loss = []\n","train_acc1 = []\n","train_acc2 = []\n","total_step = len(train_loader)\n","\n","\n","### In case we want to use pretrained model for further training\n","# ## load model state\n","# checkpoint = torch.load(\"model_classification_tutorial10.pt\")\n","# model.load_state_dict(checkpoint['state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer'])\n","# # epoch = checkpoint['epoch']\n","# valid_loss_min = checkpoint['loss']\n","\n","\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","\n","    # For accuracy estimation\n","    correct1 = 0\n","    correct2 = 0\n","    total1=0\n","    total2=0\n","\n","    print(f'Epoch {epoch}\\n')\n","    \n","    for batch_idx, (data_,  target_) in enumerate(train_loader):\n","        data_, target_ = data_.to(device),  target_.to(device)# on GPU\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        outputs1, outputs2 = model(data_)\n","        loss1 = criterion1(outputs1, target_[:,0]) # loss for ee_loc\n","        loss2 = criterion2(outputs2, target_[:,1]) # loss for o_loc\n","        loss = loss1 + loss2 # total loss\n","        loss.backward()\n","        optimizer.step()\n","        # print statistics\n","        running_loss += loss.item()\n","        _,pred1 = torch.max(outputs1, dim=1) # Predictions for ee_loc\n","        _,pred2 = torch.max(outputs2, dim=1) # Predictions for o_loc\n","        correct1 += torch.sum(pred1==target_[:,0]).item() \n","        correct2 += torch.sum(pred2==target_[:,1]).item()\n","        total1 += target_[:,0].size(0)\n","        total2 += target_[:,1].size(0)\n","        if (batch_idx) % 20 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n","    train_acc1.append(100 * correct1 / total1) # train accuracy for ee_loc\n","    train_acc2.append(100 * correct2 / total2) # train accuracy for o_loc\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc1: {(100 * correct1 / total1):.4f}, train acc2: {(100 * correct2 / total2):.4f}')\n","    batch_loss = 0\n","    total_t1=0\n","    total_t2=0\n","    correct_t1=0\n","    correct_t2=0\n","\n","    ## Evaluation\n","    with torch.no_grad():\n","        model.eval()\n","        for data_t,  target_t in (validation_loader):\n","            data_t,  target_t = data_t.to(device), target_t.to(device)# on GPU\n","            outputs_t1, outputs_t2 = model(data_t)#, bb_t)\n","            loss_t1 = criterion1(outputs_t1, target_t[:,0])\n","            loss_t2 = criterion2(outputs_t2, target_t[:,1])\n","            loss_t = loss_t1 + loss_t2 \n","            batch_loss += loss_t.item()\n","            _,pred_t1 = torch.max(outputs_t1, dim=1)\n","            _,pred_t2 = torch.max(outputs_t2, dim=1)\n","            correct_t1 += torch.sum(pred_t1==target_t[:,0]).item()\n","            correct_t2 += torch.sum(pred_t2==target_t[:,1]).item()\n","            total_t1 += target_t[:,0].size(0)\n","            total_t2 += target_t[:,1].size(0)\n","        val_acc1.append(100 * correct_t1 / total_t1)\n","        val_acc2.append(100 * correct_t2 / total_t2)\n","        val_loss.append(batch_loss/len(validation_loader))\n","        network_learned = batch_loss < valid_loss_min\n","        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc1: {(100 * correct_t1 / total_t1):.4f}, validation acc2: {(100 * correct_t2 / total_t2):.4f}\\n')#, validation acc3: {(100 * correct_t3 / total_t3):.4f}\\n')\n","        # Saving the best weight \n","        if network_learned:\n","            valid_loss_min = batch_loss\n","            ## Save model\n","            state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'loss': valid_loss_min,  }\n","            torch.save(state, 'model_classification_tutorial23.pt') \n","\n","            print('Detected network improvement, saving current model')\n","    model.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","\n","train loss: 1.6166, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6706, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 446\n","\n","Epoch [446/1000], Step [0/43], Loss: 1.4874\n","Epoch [446/1000], Step [20/43], Loss: 1.4873\n","Epoch [446/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6164, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6704, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 447\n","\n","Epoch [447/1000], Step [0/43], Loss: 1.4874\n","Epoch [447/1000], Step [20/43], Loss: 1.4873\n","Epoch [447/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.6162, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6703, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 448\n","\n","Epoch [448/1000], Step [0/43], Loss: 1.4874\n","Epoch [448/1000], Step [20/43], Loss: 1.4873\n","Epoch [448/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.6160, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6701, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 449\n","\n","Epoch [449/1000], Step [0/43], Loss: 1.4874\n","Epoch [449/1000], Step [20/43], Loss: 1.4873\n","Epoch [449/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6158, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6699, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 450\n","\n","Epoch [450/1000], Step [0/43], Loss: 1.4874\n","Epoch [450/1000], Step [20/43], Loss: 1.4873\n","Epoch [450/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6156, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6698, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 451\n","\n","Epoch [451/1000], Step [0/43], Loss: 1.4874\n","Epoch [451/1000], Step [20/43], Loss: 1.4873\n","Epoch [451/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6154, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6696, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 452\n","\n","Epoch [452/1000], Step [0/43], Loss: 1.4874\n","Epoch [452/1000], Step [20/43], Loss: 1.4873\n","Epoch [452/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6152, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6694, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 453\n","\n","Epoch [453/1000], Step [0/43], Loss: 1.4874\n","Epoch [453/1000], Step [20/43], Loss: 1.4873\n","Epoch [453/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6150, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6693, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 454\n","\n","Epoch [454/1000], Step [0/43], Loss: 1.4874\n","Epoch [454/1000], Step [20/43], Loss: 1.4873\n","Epoch [454/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6149, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6691, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 455\n","\n","Epoch [455/1000], Step [0/43], Loss: 1.4874\n","Epoch [455/1000], Step [20/43], Loss: 1.4873\n","Epoch [455/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6147, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6689, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 456\n","\n","Epoch [456/1000], Step [0/43], Loss: 1.4874\n","Epoch [456/1000], Step [20/43], Loss: 1.4873\n","Epoch [456/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6145, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6688, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 457\n","\n","Epoch [457/1000], Step [0/43], Loss: 1.4874\n","Epoch [457/1000], Step [20/43], Loss: 1.4873\n","Epoch [457/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6143, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6686, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 458\n","\n","Epoch [458/1000], Step [0/43], Loss: 1.4874\n","Epoch [458/1000], Step [20/43], Loss: 1.4873\n","Epoch [458/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.6141, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6685, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 459\n","\n","Epoch [459/1000], Step [0/43], Loss: 1.4874\n","Epoch [459/1000], Step [20/43], Loss: 1.4873\n","Epoch [459/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6139, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6683, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 460\n","\n","Epoch [460/1000], Step [0/43], Loss: 1.4874\n","Epoch [460/1000], Step [20/43], Loss: 1.4873\n","Epoch [460/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.6138, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6681, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 461\n","\n","Epoch [461/1000], Step [0/43], Loss: 1.4874\n","Epoch [461/1000], Step [20/43], Loss: 1.4873\n","Epoch [461/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.6136, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6680, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 462\n","\n","Epoch [462/1000], Step [0/43], Loss: 1.4874\n","Epoch [462/1000], Step [20/43], Loss: 1.4873\n","Epoch [462/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.6134, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6678, validation acc1: 93.3105, validation acc2: 93.1389\n","\n","Epoch 463\n","\n","Epoch [463/1000], Step [0/43], Loss: 1.4874\n","Epoch [463/1000], Step [20/43], Loss: 1.4873\n","Epoch [463/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6132, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6677, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 464\n","\n","Epoch [464/1000], Step [0/43], Loss: 1.4874\n","Epoch [464/1000], Step [20/43], Loss: 1.4873\n","Epoch [464/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6130, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6675, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 465\n","\n","Epoch [465/1000], Step [0/43], Loss: 1.4874\n","Epoch [465/1000], Step [20/43], Loss: 1.4873\n","Epoch [465/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6129, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6674, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 466\n","\n","Epoch [466/1000], Step [0/43], Loss: 1.4874\n","Epoch [466/1000], Step [20/43], Loss: 1.4873\n","Epoch [466/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6127, train acc1: 98.0854, train acc2: 96.2445\n","validation loss: 1.6672, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 467\n","\n","Epoch [467/1000], Step [0/43], Loss: 1.4873\n","Epoch [467/1000], Step [20/43], Loss: 1.4876\n","Epoch [467/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.6125, train acc1: 98.0854, train acc2: 96.3181\n","validation loss: 1.6671, validation acc1: 93.3105, validation acc2: 93.1389\n","\n","Epoch 468\n","\n","Epoch [468/1000], Step [0/43], Loss: 1.5000\n","Epoch [468/1000], Step [20/43], Loss: 1.4873\n","Epoch [468/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6123, train acc1: 97.9381, train acc2: 96.4654\n","validation loss: 1.6670, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 469\n","\n","Epoch [469/1000], Step [0/43], Loss: 1.4874\n","Epoch [469/1000], Step [20/43], Loss: 1.4907\n","Epoch [469/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6121, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6668, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 470\n","\n","Epoch [470/1000], Step [0/43], Loss: 1.4874\n","Epoch [470/1000], Step [20/43], Loss: 1.4900\n","Epoch [470/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6120, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6667, validation acc1: 93.4820, validation acc2: 93.6535\n","\n","Epoch 471\n","\n","Epoch [471/1000], Step [0/43], Loss: 1.4875\n","Epoch [471/1000], Step [20/43], Loss: 1.4874\n","Epoch [471/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6118, train acc1: 98.0854, train acc2: 96.3918\n","validation loss: 1.6665, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 472\n","\n","Epoch [472/1000], Step [0/43], Loss: 1.4875\n","Epoch [472/1000], Step [20/43], Loss: 1.4873\n","Epoch [472/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.6116, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6664, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 473\n","\n","Epoch [473/1000], Step [0/43], Loss: 1.4874\n","Epoch [473/1000], Step [20/43], Loss: 1.4873\n","Epoch [473/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6115, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6662, validation acc1: 93.4820, validation acc2: 93.6535\n","\n","Epoch 474\n","\n","Epoch [474/1000], Step [0/43], Loss: 1.4874\n","Epoch [474/1000], Step [20/43], Loss: 1.4873\n","Epoch [474/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6113, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6661, validation acc1: 93.6535, validation acc2: 93.6535\n","\n","Epoch 475\n","\n","Epoch [475/1000], Step [0/43], Loss: 1.4874\n","Epoch [475/1000], Step [20/43], Loss: 1.4873\n","Epoch [475/1000], Step [40/43], Loss: 1.6446\n","\n","train loss: 1.6111, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6659, validation acc1: 93.4820, validation acc2: 93.6535\n","\n","Epoch 476\n","\n","Epoch [476/1000], Step [0/43], Loss: 1.4874\n","Epoch [476/1000], Step [20/43], Loss: 1.4873\n","Epoch [476/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6109, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6658, validation acc1: 93.4820, validation acc2: 93.6535\n","\n","Epoch 477\n","\n","Epoch [477/1000], Step [0/43], Loss: 1.4874\n","Epoch [477/1000], Step [20/43], Loss: 1.4873\n","Epoch [477/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6108, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6657, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 478\n","\n","Epoch [478/1000], Step [0/43], Loss: 1.4874\n","Epoch [478/1000], Step [20/43], Loss: 1.4874\n","Epoch [478/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6106, train acc1: 98.0854, train acc2: 96.4654\n","validation loss: 1.6655, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 479\n","\n","Epoch [479/1000], Step [0/43], Loss: 1.4874\n","Epoch [479/1000], Step [20/43], Loss: 1.4874\n","Epoch [479/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6104, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6654, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 480\n","\n","Epoch [480/1000], Step [0/43], Loss: 1.4874\n","Epoch [480/1000], Step [20/43], Loss: 1.4874\n","Epoch [480/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.6103, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6652, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 481\n","\n","Epoch [481/1000], Step [0/43], Loss: 1.4874\n","Epoch [481/1000], Step [20/43], Loss: 1.4874\n","Epoch [481/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.6101, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6651, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 482\n","\n","Epoch [482/1000], Step [0/43], Loss: 1.4874\n","Epoch [482/1000], Step [20/43], Loss: 1.4873\n","Epoch [482/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6099, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6650, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 483\n","\n","Epoch [483/1000], Step [0/43], Loss: 1.4874\n","Epoch [483/1000], Step [20/43], Loss: 1.4874\n","Epoch [483/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6097, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6648, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 484\n","\n","Epoch [484/1000], Step [0/43], Loss: 1.4874\n","Epoch [484/1000], Step [20/43], Loss: 1.4874\n","Epoch [484/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.6096, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6647, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 485\n","\n","Epoch [485/1000], Step [0/43], Loss: 1.4874\n","Epoch [485/1000], Step [20/43], Loss: 1.4874\n","Epoch [485/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6094, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6645, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 486\n","\n","Epoch [486/1000], Step [0/43], Loss: 1.4874\n","Epoch [486/1000], Step [20/43], Loss: 1.4873\n","Epoch [486/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6092, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6644, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 487\n","\n","Epoch [487/1000], Step [0/43], Loss: 1.4874\n","Epoch [487/1000], Step [20/43], Loss: 1.4874\n","Epoch [487/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6091, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6643, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 488\n","\n","Epoch [488/1000], Step [0/43], Loss: 1.4874\n","Epoch [488/1000], Step [20/43], Loss: 1.4873\n","Epoch [488/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.6089, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6641, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 489\n","\n","Epoch [489/1000], Step [0/43], Loss: 1.4874\n","Epoch [489/1000], Step [20/43], Loss: 1.4873\n","Epoch [489/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6088, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6640, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 490\n","\n","Epoch [490/1000], Step [0/43], Loss: 1.4874\n","Epoch [490/1000], Step [20/43], Loss: 1.4873\n","Epoch [490/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6086, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6639, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 491\n","\n","Epoch [491/1000], Step [0/43], Loss: 1.4874\n","Epoch [491/1000], Step [20/43], Loss: 1.4873\n","Epoch [491/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.6084, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6637, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 492\n","\n","Epoch [492/1000], Step [0/43], Loss: 1.4874\n","Epoch [492/1000], Step [20/43], Loss: 1.4873\n","Epoch [492/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6083, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6636, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 493\n","\n","Epoch [493/1000], Step [0/43], Loss: 1.4874\n","Epoch [493/1000], Step [20/43], Loss: 1.4873\n","Epoch [493/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.6081, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6635, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 494\n","\n","Epoch [494/1000], Step [0/43], Loss: 1.4874\n","Epoch [494/1000], Step [20/43], Loss: 1.4873\n","Epoch [494/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.6079, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6633, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 495\n","\n","Epoch [495/1000], Step [0/43], Loss: 1.4874\n","Epoch [495/1000], Step [20/43], Loss: 1.4873\n","Epoch [495/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6078, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6632, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 496\n","\n","Epoch [496/1000], Step [0/43], Loss: 1.4874\n","Epoch [496/1000], Step [20/43], Loss: 1.4873\n","Epoch [496/1000], Step [40/43], Loss: 1.6446\n","\n","train loss: 1.6076, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6631, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 497\n","\n","Epoch [497/1000], Step [0/43], Loss: 1.4874\n","Epoch [497/1000], Step [20/43], Loss: 1.4873\n","Epoch [497/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6075, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6629, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 498\n","\n","Epoch [498/1000], Step [0/43], Loss: 1.4874\n","Epoch [498/1000], Step [20/43], Loss: 1.4873\n","Epoch [498/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6073, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6628, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 499\n","\n","Epoch [499/1000], Step [0/43], Loss: 1.4874\n","Epoch [499/1000], Step [20/43], Loss: 1.4873\n","Epoch [499/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.6072, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6627, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 500\n","\n","Epoch [500/1000], Step [0/43], Loss: 1.4874\n","Epoch [500/1000], Step [20/43], Loss: 1.4873\n","Epoch [500/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.6070, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6625, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 501\n","\n","Epoch [501/1000], Step [0/43], Loss: 1.4874\n","Epoch [501/1000], Step [20/43], Loss: 1.4873\n","Epoch [501/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6068, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6624, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 502\n","\n","Epoch [502/1000], Step [0/43], Loss: 1.4874\n","Epoch [502/1000], Step [20/43], Loss: 1.4873\n","Epoch [502/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.6067, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6623, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 503\n","\n","Epoch [503/1000], Step [0/43], Loss: 1.4874\n","Epoch [503/1000], Step [20/43], Loss: 1.4873\n","Epoch [503/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6065, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6621, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 504\n","\n","Epoch [504/1000], Step [0/43], Loss: 1.4874\n","Epoch [504/1000], Step [20/43], Loss: 1.4873\n","Epoch [504/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.6064, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6620, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 505\n","\n","Epoch [505/1000], Step [0/43], Loss: 1.4874\n","Epoch [505/1000], Step [20/43], Loss: 1.4873\n","Epoch [505/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.6062, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6619, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 506\n","\n","Epoch [506/1000], Step [0/43], Loss: 1.4874\n","Epoch [506/1000], Step [20/43], Loss: 1.4873\n","Epoch [506/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.6061, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6618, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 507\n","\n","Epoch [507/1000], Step [0/43], Loss: 1.4874\n","Epoch [507/1000], Step [20/43], Loss: 1.4873\n","Epoch [507/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6059, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6616, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 508\n","\n","Epoch [508/1000], Step [0/43], Loss: 1.4874\n","Epoch [508/1000], Step [20/43], Loss: 1.4873\n","Epoch [508/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6058, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6615, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 509\n","\n","Epoch [509/1000], Step [0/43], Loss: 1.4874\n","Epoch [509/1000], Step [20/43], Loss: 1.4873\n","Epoch [509/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6056, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6614, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 510\n","\n","Epoch [510/1000], Step [0/43], Loss: 1.4874\n","Epoch [510/1000], Step [20/43], Loss: 1.4873\n","Epoch [510/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.6055, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6613, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 511\n","\n","Epoch [511/1000], Step [0/43], Loss: 1.4874\n","Epoch [511/1000], Step [20/43], Loss: 1.4873\n","Epoch [511/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.6053, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6611, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 512\n","\n","Epoch [512/1000], Step [0/43], Loss: 1.4874\n","Epoch [512/1000], Step [20/43], Loss: 1.4873\n","Epoch [512/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6052, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6610, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 513\n","\n","Epoch [513/1000], Step [0/43], Loss: 1.4874\n","Epoch [513/1000], Step [20/43], Loss: 1.4873\n","Epoch [513/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6050, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6609, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 514\n","\n","Epoch [514/1000], Step [0/43], Loss: 1.4874\n","Epoch [514/1000], Step [20/43], Loss: 1.4873\n","Epoch [514/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6049, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6607, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 515\n","\n","Epoch [515/1000], Step [0/43], Loss: 1.4874\n","Epoch [515/1000], Step [20/43], Loss: 1.4873\n","Epoch [515/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.6047, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6606, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 516\n","\n","Epoch [516/1000], Step [0/43], Loss: 1.4874\n","Epoch [516/1000], Step [20/43], Loss: 1.4873\n","Epoch [516/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6046, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6605, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 517\n","\n","Epoch [517/1000], Step [0/43], Loss: 1.4874\n","Epoch [517/1000], Step [20/43], Loss: 1.4873\n","Epoch [517/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.6044, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6604, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 518\n","\n","Epoch [518/1000], Step [0/43], Loss: 1.4874\n","Epoch [518/1000], Step [20/43], Loss: 1.4873\n","Epoch [518/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.6043, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6603, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 519\n","\n","Epoch [519/1000], Step [0/43], Loss: 1.4874\n","Epoch [519/1000], Step [20/43], Loss: 1.4873\n","Epoch [519/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6041, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6601, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 520\n","\n","Epoch [520/1000], Step [0/43], Loss: 1.4874\n","Epoch [520/1000], Step [20/43], Loss: 1.4873\n","Epoch [520/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.6040, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6600, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 521\n","\n","Epoch [521/1000], Step [0/43], Loss: 1.4874\n","Epoch [521/1000], Step [20/43], Loss: 1.4873\n","Epoch [521/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.6039, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6599, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 522\n","\n","Epoch [522/1000], Step [0/43], Loss: 1.4874\n","Epoch [522/1000], Step [20/43], Loss: 1.4873\n","Epoch [522/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6037, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6598, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 523\n","\n","Epoch [523/1000], Step [0/43], Loss: 1.4874\n","Epoch [523/1000], Step [20/43], Loss: 1.4873\n","Epoch [523/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6036, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6596, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 524\n","\n","Epoch [524/1000], Step [0/43], Loss: 1.4874\n","Epoch [524/1000], Step [20/43], Loss: 1.4873\n","Epoch [524/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.6034, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6595, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 525\n","\n","Epoch [525/1000], Step [0/43], Loss: 1.4874\n","Epoch [525/1000], Step [20/43], Loss: 1.4873\n","Epoch [525/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6033, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6594, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 526\n","\n","Epoch [526/1000], Step [0/43], Loss: 1.4874\n","Epoch [526/1000], Step [20/43], Loss: 1.4873\n","Epoch [526/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.6031, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6593, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 527\n","\n","Epoch [527/1000], Step [0/43], Loss: 1.4874\n","Epoch [527/1000], Step [20/43], Loss: 1.4873\n","Epoch [527/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6030, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6592, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 528\n","\n","Epoch [528/1000], Step [0/43], Loss: 1.4874\n","Epoch [528/1000], Step [20/43], Loss: 1.4873\n","Epoch [528/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6029, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6591, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 529\n","\n","Epoch [529/1000], Step [0/43], Loss: 1.4874\n","Epoch [529/1000], Step [20/43], Loss: 1.4873\n","Epoch [529/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.6027, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6589, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 530\n","\n","Epoch [530/1000], Step [0/43], Loss: 1.4874\n","Epoch [530/1000], Step [20/43], Loss: 1.4873\n","Epoch [530/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.6026, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6588, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 531\n","\n","Epoch [531/1000], Step [0/43], Loss: 1.4874\n","Epoch [531/1000], Step [20/43], Loss: 1.4873\n","Epoch [531/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.6024, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6587, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 532\n","\n","Epoch [532/1000], Step [0/43], Loss: 1.4874\n","Epoch [532/1000], Step [20/43], Loss: 1.4873\n","Epoch [532/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.6023, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6586, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 533\n","\n","Epoch [533/1000], Step [0/43], Loss: 1.4874\n","Epoch [533/1000], Step [20/43], Loss: 1.4873\n","Epoch [533/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.6022, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6585, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 534\n","\n","Epoch [534/1000], Step [0/43], Loss: 1.4874\n","Epoch [534/1000], Step [20/43], Loss: 1.4873\n","Epoch [534/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.6020, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6584, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 535\n","\n","Epoch [535/1000], Step [0/43], Loss: 1.4874\n","Epoch [535/1000], Step [20/43], Loss: 1.4873\n","Epoch [535/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6019, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6582, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 536\n","\n","Epoch [536/1000], Step [0/43], Loss: 1.4874\n","Epoch [536/1000], Step [20/43], Loss: 1.4873\n","Epoch [536/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.6018, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6581, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 537\n","\n","Epoch [537/1000], Step [0/43], Loss: 1.4874\n","Epoch [537/1000], Step [20/43], Loss: 1.4873\n","Epoch [537/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6016, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6580, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 538\n","\n","Epoch [538/1000], Step [0/43], Loss: 1.4874\n","Epoch [538/1000], Step [20/43], Loss: 1.4873\n","Epoch [538/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.6015, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6579, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 539\n","\n","Epoch [539/1000], Step [0/43], Loss: 1.4874\n","Epoch [539/1000], Step [20/43], Loss: 1.4873\n","Epoch [539/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.6013, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6578, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 540\n","\n","Epoch [540/1000], Step [0/43], Loss: 1.4874\n","Epoch [540/1000], Step [20/43], Loss: 1.4873\n","Epoch [540/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6012, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6577, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 541\n","\n","Epoch [541/1000], Step [0/43], Loss: 1.4874\n","Epoch [541/1000], Step [20/43], Loss: 1.4873\n","Epoch [541/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.6011, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6576, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 542\n","\n","Epoch [542/1000], Step [0/43], Loss: 1.4874\n","Epoch [542/1000], Step [20/43], Loss: 1.4873\n","Epoch [542/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6009, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6575, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 543\n","\n","Epoch [543/1000], Step [0/43], Loss: 1.4874\n","Epoch [543/1000], Step [20/43], Loss: 1.4873\n","Epoch [543/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6008, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6573, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 544\n","\n","Epoch [544/1000], Step [0/43], Loss: 1.4874\n","Epoch [544/1000], Step [20/43], Loss: 1.4873\n","Epoch [544/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.6007, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6572, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 545\n","\n","Epoch [545/1000], Step [0/43], Loss: 1.4874\n","Epoch [545/1000], Step [20/43], Loss: 1.4873\n","Epoch [545/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.6005, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6571, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 546\n","\n","Epoch [546/1000], Step [0/43], Loss: 1.4874\n","Epoch [546/1000], Step [20/43], Loss: 1.4873\n","Epoch [546/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6004, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6570, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 547\n","\n","Epoch [547/1000], Step [0/43], Loss: 1.4874\n","Epoch [547/1000], Step [20/43], Loss: 1.4873\n","Epoch [547/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6003, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6569, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 548\n","\n","Epoch [548/1000], Step [0/43], Loss: 1.4874\n","Epoch [548/1000], Step [20/43], Loss: 1.4873\n","Epoch [548/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.6002, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6568, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 549\n","\n","Epoch [549/1000], Step [0/43], Loss: 1.4874\n","Epoch [549/1000], Step [20/43], Loss: 1.4873\n","Epoch [549/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.6000, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6567, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 550\n","\n","Epoch [550/1000], Step [0/43], Loss: 1.4874\n","Epoch [550/1000], Step [20/43], Loss: 1.4873\n","Epoch [550/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5999, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6566, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 551\n","\n","Epoch [551/1000], Step [0/43], Loss: 1.4874\n","Epoch [551/1000], Step [20/43], Loss: 1.4873\n","Epoch [551/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5998, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6565, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 552\n","\n","Epoch [552/1000], Step [0/43], Loss: 1.4874\n","Epoch [552/1000], Step [20/43], Loss: 1.4873\n","Epoch [552/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5996, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6564, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 553\n","\n","Epoch [553/1000], Step [0/43], Loss: 1.4874\n","Epoch [553/1000], Step [20/43], Loss: 1.4873\n","Epoch [553/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5995, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6562, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 554\n","\n","Epoch [554/1000], Step [0/43], Loss: 1.4874\n","Epoch [554/1000], Step [20/43], Loss: 1.4873\n","Epoch [554/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5994, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6561, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 555\n","\n","Epoch [555/1000], Step [0/43], Loss: 1.4874\n","Epoch [555/1000], Step [20/43], Loss: 1.4873\n","Epoch [555/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5993, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6560, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 556\n","\n","Epoch [556/1000], Step [0/43], Loss: 1.4874\n","Epoch [556/1000], Step [20/43], Loss: 1.4873\n","Epoch [556/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5991, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6559, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 557\n","\n","Epoch [557/1000], Step [0/43], Loss: 1.4874\n","Epoch [557/1000], Step [20/43], Loss: 1.4873\n","Epoch [557/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5990, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6558, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 558\n","\n","Epoch [558/1000], Step [0/43], Loss: 1.4874\n","Epoch [558/1000], Step [20/43], Loss: 1.4873\n","Epoch [558/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5989, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6557, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 559\n","\n","Epoch [559/1000], Step [0/43], Loss: 1.4874\n","Epoch [559/1000], Step [20/43], Loss: 1.4873\n","Epoch [559/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5988, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6556, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 560\n","\n","Epoch [560/1000], Step [0/43], Loss: 1.4874\n","Epoch [560/1000], Step [20/43], Loss: 1.4873\n","Epoch [560/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5986, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6555, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 561\n","\n","Epoch [561/1000], Step [0/43], Loss: 1.4874\n","Epoch [561/1000], Step [20/43], Loss: 1.4873\n","Epoch [561/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5985, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6554, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 562\n","\n","Epoch [562/1000], Step [0/43], Loss: 1.4874\n","Epoch [562/1000], Step [20/43], Loss: 1.4873\n","Epoch [562/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5984, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6553, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 563\n","\n","Epoch [563/1000], Step [0/43], Loss: 1.4874\n","Epoch [563/1000], Step [20/43], Loss: 1.4873\n","Epoch [563/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5983, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6552, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 564\n","\n","Epoch [564/1000], Step [0/43], Loss: 1.4874\n","Epoch [564/1000], Step [20/43], Loss: 1.4873\n","Epoch [564/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5981, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6551, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 565\n","\n","Epoch [565/1000], Step [0/43], Loss: 1.4874\n","Epoch [565/1000], Step [20/43], Loss: 1.4873\n","Epoch [565/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5980, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6550, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 566\n","\n","Epoch [566/1000], Step [0/43], Loss: 1.4874\n","Epoch [566/1000], Step [20/43], Loss: 1.4873\n","Epoch [566/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5979, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6549, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 567\n","\n","Epoch [567/1000], Step [0/43], Loss: 1.4874\n","Epoch [567/1000], Step [20/43], Loss: 1.4873\n","Epoch [567/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5978, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6548, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 568\n","\n","Epoch [568/1000], Step [0/43], Loss: 1.4874\n","Epoch [568/1000], Step [20/43], Loss: 1.4873\n","Epoch [568/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5976, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6547, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 569\n","\n","Epoch [569/1000], Step [0/43], Loss: 1.4874\n","Epoch [569/1000], Step [20/43], Loss: 1.4873\n","Epoch [569/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5975, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6546, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 570\n","\n","Epoch [570/1000], Step [0/43], Loss: 1.4874\n","Epoch [570/1000], Step [20/43], Loss: 1.4873\n","Epoch [570/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5974, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6545, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 571\n","\n","Epoch [571/1000], Step [0/43], Loss: 1.4874\n","Epoch [571/1000], Step [20/43], Loss: 1.4873\n","Epoch [571/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5973, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6544, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 572\n","\n","Epoch [572/1000], Step [0/43], Loss: 1.4874\n","Epoch [572/1000], Step [20/43], Loss: 1.4873\n","Epoch [572/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5972, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6543, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 573\n","\n","Epoch [573/1000], Step [0/43], Loss: 1.4874\n","Epoch [573/1000], Step [20/43], Loss: 1.4873\n","Epoch [573/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5970, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6542, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 574\n","\n","Epoch [574/1000], Step [0/43], Loss: 1.4874\n","Epoch [574/1000], Step [20/43], Loss: 1.4873\n","Epoch [574/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5969, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6541, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 575\n","\n","Epoch [575/1000], Step [0/43], Loss: 1.4874\n","Epoch [575/1000], Step [20/43], Loss: 1.4873\n","Epoch [575/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5968, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6540, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 576\n","\n","Epoch [576/1000], Step [0/43], Loss: 1.4874\n","Epoch [576/1000], Step [20/43], Loss: 1.4873\n","Epoch [576/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5967, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6539, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 577\n","\n","Epoch [577/1000], Step [0/43], Loss: 1.4874\n","Epoch [577/1000], Step [20/43], Loss: 1.4873\n","Epoch [577/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5966, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6538, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 578\n","\n","Epoch [578/1000], Step [0/43], Loss: 1.4874\n","Epoch [578/1000], Step [20/43], Loss: 1.4873\n","Epoch [578/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5965, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6537, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 579\n","\n","Epoch [579/1000], Step [0/43], Loss: 1.4874\n","Epoch [579/1000], Step [20/43], Loss: 1.4873\n","Epoch [579/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5963, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6536, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 580\n","\n","Epoch [580/1000], Step [0/43], Loss: 1.4874\n","Epoch [580/1000], Step [20/43], Loss: 1.4873\n","Epoch [580/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5962, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6535, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 581\n","\n","Epoch [581/1000], Step [0/43], Loss: 1.4874\n","Epoch [581/1000], Step [20/43], Loss: 1.4873\n","Epoch [581/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5961, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6534, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 582\n","\n","Epoch [582/1000], Step [0/43], Loss: 1.4874\n","Epoch [582/1000], Step [20/43], Loss: 1.4873\n","Epoch [582/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5960, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6533, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 583\n","\n","Epoch [583/1000], Step [0/43], Loss: 1.4874\n","Epoch [583/1000], Step [20/43], Loss: 1.4873\n","Epoch [583/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5959, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6532, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 584\n","\n","Epoch [584/1000], Step [0/43], Loss: 1.4874\n","Epoch [584/1000], Step [20/43], Loss: 1.4873\n","Epoch [584/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5958, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6531, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 585\n","\n","Epoch [585/1000], Step [0/43], Loss: 1.4874\n","Epoch [585/1000], Step [20/43], Loss: 1.4873\n","Epoch [585/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5956, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6530, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 586\n","\n","Epoch [586/1000], Step [0/43], Loss: 1.4874\n","Epoch [586/1000], Step [20/43], Loss: 1.4873\n","Epoch [586/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5955, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6529, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 587\n","\n","Epoch [587/1000], Step [0/43], Loss: 1.4874\n","Epoch [587/1000], Step [20/43], Loss: 1.4873\n","Epoch [587/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5954, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6528, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 588\n","\n","Epoch [588/1000], Step [0/43], Loss: 1.4874\n","Epoch [588/1000], Step [20/43], Loss: 1.4873\n","Epoch [588/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5953, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6527, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 589\n","\n","Epoch [589/1000], Step [0/43], Loss: 1.4874\n","Epoch [589/1000], Step [20/43], Loss: 1.4873\n","Epoch [589/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5952, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6526, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 590\n","\n","Epoch [590/1000], Step [0/43], Loss: 1.4874\n","Epoch [590/1000], Step [20/43], Loss: 1.4873\n","Epoch [590/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5951, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6525, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 591\n","\n","Epoch [591/1000], Step [0/43], Loss: 1.4874\n","Epoch [591/1000], Step [20/43], Loss: 1.4873\n","Epoch [591/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5950, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6524, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 592\n","\n","Epoch [592/1000], Step [0/43], Loss: 1.4874\n","Epoch [592/1000], Step [20/43], Loss: 1.4873\n","Epoch [592/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5949, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6523, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 593\n","\n","Epoch [593/1000], Step [0/43], Loss: 1.4874\n","Epoch [593/1000], Step [20/43], Loss: 1.4873\n","Epoch [593/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5947, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6522, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 594\n","\n","Epoch [594/1000], Step [0/43], Loss: 1.4874\n","Epoch [594/1000], Step [20/43], Loss: 1.4873\n","Epoch [594/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5946, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6521, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 595\n","\n","Epoch [595/1000], Step [0/43], Loss: 1.4874\n","Epoch [595/1000], Step [20/43], Loss: 1.4873\n","Epoch [595/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5945, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6520, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 596\n","\n","Epoch [596/1000], Step [0/43], Loss: 1.4874\n","Epoch [596/1000], Step [20/43], Loss: 1.4873\n","Epoch [596/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5944, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6519, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 597\n","\n","Epoch [597/1000], Step [0/43], Loss: 1.4874\n","Epoch [597/1000], Step [20/43], Loss: 1.4873\n","Epoch [597/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5943, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6518, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 598\n","\n","Epoch [598/1000], Step [0/43], Loss: 1.4874\n","Epoch [598/1000], Step [20/43], Loss: 1.4873\n","Epoch [598/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5942, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6517, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 599\n","\n","Epoch [599/1000], Step [0/43], Loss: 1.4874\n","Epoch [599/1000], Step [20/43], Loss: 1.4873\n","Epoch [599/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5941, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6516, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 600\n","\n","Epoch [600/1000], Step [0/43], Loss: 1.4874\n","Epoch [600/1000], Step [20/43], Loss: 1.4873\n","Epoch [600/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5940, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6515, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 601\n","\n","Epoch [601/1000], Step [0/43], Loss: 1.4874\n","Epoch [601/1000], Step [20/43], Loss: 1.4873\n","Epoch [601/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5939, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6515, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 602\n","\n","Epoch [602/1000], Step [0/43], Loss: 1.4874\n","Epoch [602/1000], Step [20/43], Loss: 1.4873\n","Epoch [602/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5938, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6514, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 603\n","\n","Epoch [603/1000], Step [0/43], Loss: 1.4874\n","Epoch [603/1000], Step [20/43], Loss: 1.4873\n","Epoch [603/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5937, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6513, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 604\n","\n","Epoch [604/1000], Step [0/43], Loss: 1.4874\n","Epoch [604/1000], Step [20/43], Loss: 1.4873\n","Epoch [604/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5935, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6512, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 605\n","\n","Epoch [605/1000], Step [0/43], Loss: 1.4874\n","Epoch [605/1000], Step [20/43], Loss: 1.4873\n","Epoch [605/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5934, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6511, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 606\n","\n","Epoch [606/1000], Step [0/43], Loss: 1.4874\n","Epoch [606/1000], Step [20/43], Loss: 1.4873\n","Epoch [606/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5933, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6510, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 607\n","\n","Epoch [607/1000], Step [0/43], Loss: 1.4874\n","Epoch [607/1000], Step [20/43], Loss: 1.4873\n","Epoch [607/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5932, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6509, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 608\n","\n","Epoch [608/1000], Step [0/43], Loss: 1.4874\n","Epoch [608/1000], Step [20/43], Loss: 1.4873\n","Epoch [608/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5931, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6508, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 609\n","\n","Epoch [609/1000], Step [0/43], Loss: 1.4874\n","Epoch [609/1000], Step [20/43], Loss: 1.4873\n","Epoch [609/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5930, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6507, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 610\n","\n","Epoch [610/1000], Step [0/43], Loss: 1.4874\n","Epoch [610/1000], Step [20/43], Loss: 1.4873\n","Epoch [610/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5929, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6506, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 611\n","\n","Epoch [611/1000], Step [0/43], Loss: 1.4874\n","Epoch [611/1000], Step [20/43], Loss: 1.4873\n","Epoch [611/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5928, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6506, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 612\n","\n","Epoch [612/1000], Step [0/43], Loss: 1.4874\n","Epoch [612/1000], Step [20/43], Loss: 1.4873\n","Epoch [612/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5927, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6505, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 613\n","\n","Epoch [613/1000], Step [0/43], Loss: 1.4874\n","Epoch [613/1000], Step [20/43], Loss: 1.4873\n","Epoch [613/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5926, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6504, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 614\n","\n","Epoch [614/1000], Step [0/43], Loss: 1.4874\n","Epoch [614/1000], Step [20/43], Loss: 1.4873\n","Epoch [614/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5925, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6503, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 615\n","\n","Epoch [615/1000], Step [0/43], Loss: 1.4874\n","Epoch [615/1000], Step [20/43], Loss: 1.4873\n","Epoch [615/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5924, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6502, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 616\n","\n","Epoch [616/1000], Step [0/43], Loss: 1.4874\n","Epoch [616/1000], Step [20/43], Loss: 1.4873\n","Epoch [616/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5923, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6501, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 617\n","\n","Epoch [617/1000], Step [0/43], Loss: 1.4874\n","Epoch [617/1000], Step [20/43], Loss: 1.4873\n","Epoch [617/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5922, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6500, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 618\n","\n","Epoch [618/1000], Step [0/43], Loss: 1.4874\n","Epoch [618/1000], Step [20/43], Loss: 1.4873\n","Epoch [618/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5921, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6499, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 619\n","\n","Epoch [619/1000], Step [0/43], Loss: 1.4874\n","Epoch [619/1000], Step [20/43], Loss: 1.4873\n","Epoch [619/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5920, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6498, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 620\n","\n","Epoch [620/1000], Step [0/43], Loss: 1.4874\n","Epoch [620/1000], Step [20/43], Loss: 1.4873\n","Epoch [620/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5919, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6498, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 621\n","\n","Epoch [621/1000], Step [0/43], Loss: 1.4874\n","Epoch [621/1000], Step [20/43], Loss: 1.4873\n","Epoch [621/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5918, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6497, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 622\n","\n","Epoch [622/1000], Step [0/43], Loss: 1.4874\n","Epoch [622/1000], Step [20/43], Loss: 1.4873\n","Epoch [622/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5917, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6496, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 623\n","\n","Epoch [623/1000], Step [0/43], Loss: 1.4874\n","Epoch [623/1000], Step [20/43], Loss: 1.4873\n","Epoch [623/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5916, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6495, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 624\n","\n","Epoch [624/1000], Step [0/43], Loss: 1.4874\n","Epoch [624/1000], Step [20/43], Loss: 1.4873\n","Epoch [624/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5915, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6494, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 625\n","\n","Epoch [625/1000], Step [0/43], Loss: 1.4874\n","Epoch [625/1000], Step [20/43], Loss: 1.4873\n","Epoch [625/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5914, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6493, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 626\n","\n","Epoch [626/1000], Step [0/43], Loss: 1.4874\n","Epoch [626/1000], Step [20/43], Loss: 1.4873\n","Epoch [626/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5913, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6492, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 627\n","\n","Epoch [627/1000], Step [0/43], Loss: 1.4874\n","Epoch [627/1000], Step [20/43], Loss: 1.4873\n","Epoch [627/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5912, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6492, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 628\n","\n","Epoch [628/1000], Step [0/43], Loss: 1.4874\n","Epoch [628/1000], Step [20/43], Loss: 1.4873\n","Epoch [628/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5911, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6491, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 629\n","\n","Epoch [629/1000], Step [0/43], Loss: 1.4874\n","Epoch [629/1000], Step [20/43], Loss: 1.4873\n","Epoch [629/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5910, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6490, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 630\n","\n","Epoch [630/1000], Step [0/43], Loss: 1.4874\n","Epoch [630/1000], Step [20/43], Loss: 1.4873\n","Epoch [630/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5909, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6489, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 631\n","\n","Epoch [631/1000], Step [0/43], Loss: 1.4874\n","Epoch [631/1000], Step [20/43], Loss: 1.4873\n","Epoch [631/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5908, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6488, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 632\n","\n","Epoch [632/1000], Step [0/43], Loss: 1.4874\n","Epoch [632/1000], Step [20/43], Loss: 1.4873\n","Epoch [632/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5907, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6487, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 633\n","\n","Epoch [633/1000], Step [0/43], Loss: 1.4874\n","Epoch [633/1000], Step [20/43], Loss: 1.4873\n","Epoch [633/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5906, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6486, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 634\n","\n","Epoch [634/1000], Step [0/43], Loss: 1.4874\n","Epoch [634/1000], Step [20/43], Loss: 1.4873\n","Epoch [634/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5905, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6486, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 635\n","\n","Epoch [635/1000], Step [0/43], Loss: 1.4874\n","Epoch [635/1000], Step [20/43], Loss: 1.4873\n","Epoch [635/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5904, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6485, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 636\n","\n","Epoch [636/1000], Step [0/43], Loss: 1.4874\n","Epoch [636/1000], Step [20/43], Loss: 1.4873\n","Epoch [636/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5903, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6484, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 637\n","\n","Epoch [637/1000], Step [0/43], Loss: 1.4874\n","Epoch [637/1000], Step [20/43], Loss: 1.4873\n","Epoch [637/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5902, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6483, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 638\n","\n","Epoch [638/1000], Step [0/43], Loss: 1.4874\n","Epoch [638/1000], Step [20/43], Loss: 1.4873\n","Epoch [638/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5901, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6482, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 639\n","\n","Epoch [639/1000], Step [0/43], Loss: 1.4874\n","Epoch [639/1000], Step [20/43], Loss: 1.4873\n","Epoch [639/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5900, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6481, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 640\n","\n","Epoch [640/1000], Step [0/43], Loss: 1.4874\n","Epoch [640/1000], Step [20/43], Loss: 1.4873\n","Epoch [640/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5899, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6481, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 641\n","\n","Epoch [641/1000], Step [0/43], Loss: 1.4874\n","Epoch [641/1000], Step [20/43], Loss: 1.4873\n","Epoch [641/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5898, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6480, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 642\n","\n","Epoch [642/1000], Step [0/43], Loss: 1.4874\n","Epoch [642/1000], Step [20/43], Loss: 1.4873\n","Epoch [642/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5897, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6479, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 643\n","\n","Epoch [643/1000], Step [0/43], Loss: 1.4874\n","Epoch [643/1000], Step [20/43], Loss: 1.4873\n","Epoch [643/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5896, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6478, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 644\n","\n","Epoch [644/1000], Step [0/43], Loss: 1.4874\n","Epoch [644/1000], Step [20/43], Loss: 1.4873\n","Epoch [644/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5895, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6477, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 645\n","\n","Epoch [645/1000], Step [0/43], Loss: 1.4874\n","Epoch [645/1000], Step [20/43], Loss: 1.4873\n","Epoch [645/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5894, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6477, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 646\n","\n","Epoch [646/1000], Step [0/43], Loss: 1.4874\n","Epoch [646/1000], Step [20/43], Loss: 1.4873\n","Epoch [646/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5893, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6476, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 647\n","\n","Epoch [647/1000], Step [0/43], Loss: 1.4874\n","Epoch [647/1000], Step [20/43], Loss: 1.4873\n","Epoch [647/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5892, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6475, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 648\n","\n","Epoch [648/1000], Step [0/43], Loss: 1.4874\n","Epoch [648/1000], Step [20/43], Loss: 1.4873\n","Epoch [648/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5892, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6474, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 649\n","\n","Epoch [649/1000], Step [0/43], Loss: 1.4874\n","Epoch [649/1000], Step [20/43], Loss: 1.4873\n","Epoch [649/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5891, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6473, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 650\n","\n","Epoch [650/1000], Step [0/43], Loss: 1.4874\n","Epoch [650/1000], Step [20/43], Loss: 1.4873\n","Epoch [650/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5890, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6472, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 651\n","\n","Epoch [651/1000], Step [0/43], Loss: 1.4874\n","Epoch [651/1000], Step [20/43], Loss: 1.4873\n","Epoch [651/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5889, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6472, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 652\n","\n","Epoch [652/1000], Step [0/43], Loss: 1.4874\n","Epoch [652/1000], Step [20/43], Loss: 1.4873\n","Epoch [652/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5888, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6471, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 653\n","\n","Epoch [653/1000], Step [0/43], Loss: 1.4874\n","Epoch [653/1000], Step [20/43], Loss: 1.4873\n","Epoch [653/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5887, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6470, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 654\n","\n","Epoch [654/1000], Step [0/43], Loss: 1.4874\n","Epoch [654/1000], Step [20/43], Loss: 1.4873\n","Epoch [654/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5886, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6469, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 655\n","\n","Epoch [655/1000], Step [0/43], Loss: 1.4874\n","Epoch [655/1000], Step [20/43], Loss: 1.4873\n","Epoch [655/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5885, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6469, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 656\n","\n","Epoch [656/1000], Step [0/43], Loss: 1.4874\n","Epoch [656/1000], Step [20/43], Loss: 1.4873\n","Epoch [656/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5884, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6468, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 657\n","\n","Epoch [657/1000], Step [0/43], Loss: 1.4874\n","Epoch [657/1000], Step [20/43], Loss: 1.4873\n","Epoch [657/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5883, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6467, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 658\n","\n","Epoch [658/1000], Step [0/43], Loss: 1.4874\n","Epoch [658/1000], Step [20/43], Loss: 1.4873\n","Epoch [658/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5882, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6466, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 659\n","\n","Epoch [659/1000], Step [0/43], Loss: 1.4874\n","Epoch [659/1000], Step [20/43], Loss: 1.4873\n","Epoch [659/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5882, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6465, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 660\n","\n","Epoch [660/1000], Step [0/43], Loss: 1.4874\n","Epoch [660/1000], Step [20/43], Loss: 1.4873\n","Epoch [660/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5881, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6465, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 661\n","\n","Epoch [661/1000], Step [0/43], Loss: 1.4874\n","Epoch [661/1000], Step [20/43], Loss: 1.4873\n","Epoch [661/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5880, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6464, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 662\n","\n","Epoch [662/1000], Step [0/43], Loss: 1.4874\n","Epoch [662/1000], Step [20/43], Loss: 1.4873\n","Epoch [662/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5879, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6463, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 663\n","\n","Epoch [663/1000], Step [0/43], Loss: 1.4874\n","Epoch [663/1000], Step [20/43], Loss: 1.4873\n","Epoch [663/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5878, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6462, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 664\n","\n","Epoch [664/1000], Step [0/43], Loss: 1.4874\n","Epoch [664/1000], Step [20/43], Loss: 1.4873\n","Epoch [664/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5877, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6462, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 665\n","\n","Epoch [665/1000], Step [0/43], Loss: 1.4874\n","Epoch [665/1000], Step [20/43], Loss: 1.4873\n","Epoch [665/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5876, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6461, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 666\n","\n","Epoch [666/1000], Step [0/43], Loss: 1.4874\n","Epoch [666/1000], Step [20/43], Loss: 1.4873\n","Epoch [666/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5875, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6460, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 667\n","\n","Epoch [667/1000], Step [0/43], Loss: 1.4874\n","Epoch [667/1000], Step [20/43], Loss: 1.4873\n","Epoch [667/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5874, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6459, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 668\n","\n","Epoch [668/1000], Step [0/43], Loss: 1.4874\n","Epoch [668/1000], Step [20/43], Loss: 1.4873\n","Epoch [668/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5874, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6459, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 669\n","\n","Epoch [669/1000], Step [0/43], Loss: 1.4874\n","Epoch [669/1000], Step [20/43], Loss: 1.4873\n","Epoch [669/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5873, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6458, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 670\n","\n","Epoch [670/1000], Step [0/43], Loss: 1.4874\n","Epoch [670/1000], Step [20/43], Loss: 1.4873\n","Epoch [670/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5872, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6457, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 671\n","\n","Epoch [671/1000], Step [0/43], Loss: 1.4874\n","Epoch [671/1000], Step [20/43], Loss: 1.4873\n","Epoch [671/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5871, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6456, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 672\n","\n","Epoch [672/1000], Step [0/43], Loss: 1.4874\n","Epoch [672/1000], Step [20/43], Loss: 1.4873\n","Epoch [672/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5870, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6456, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 673\n","\n","Epoch [673/1000], Step [0/43], Loss: 1.4874\n","Epoch [673/1000], Step [20/43], Loss: 1.4873\n","Epoch [673/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5869, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6455, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 674\n","\n","Epoch [674/1000], Step [0/43], Loss: 1.4874\n","Epoch [674/1000], Step [20/43], Loss: 1.4873\n","Epoch [674/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5868, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6454, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 675\n","\n","Epoch [675/1000], Step [0/43], Loss: 1.4874\n","Epoch [675/1000], Step [20/43], Loss: 1.4873\n","Epoch [675/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5867, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6453, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 676\n","\n","Epoch [676/1000], Step [0/43], Loss: 1.4874\n","Epoch [676/1000], Step [20/43], Loss: 1.4873\n","Epoch [676/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5867, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6453, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 677\n","\n","Epoch [677/1000], Step [0/43], Loss: 1.4874\n","Epoch [677/1000], Step [20/43], Loss: 1.4873\n","Epoch [677/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5866, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6452, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 678\n","\n","Epoch [678/1000], Step [0/43], Loss: 1.4874\n","Epoch [678/1000], Step [20/43], Loss: 1.4873\n","Epoch [678/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5865, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6451, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 679\n","\n","Epoch [679/1000], Step [0/43], Loss: 1.4874\n","Epoch [679/1000], Step [20/43], Loss: 1.4873\n","Epoch [679/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5864, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6450, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 680\n","\n","Epoch [680/1000], Step [0/43], Loss: 1.4874\n","Epoch [680/1000], Step [20/43], Loss: 1.4873\n","Epoch [680/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5863, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6450, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 681\n","\n","Epoch [681/1000], Step [0/43], Loss: 1.4874\n","Epoch [681/1000], Step [20/43], Loss: 1.4873\n","Epoch [681/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5862, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6449, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 682\n","\n","Epoch [682/1000], Step [0/43], Loss: 1.4874\n","Epoch [682/1000], Step [20/43], Loss: 1.4873\n","Epoch [682/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5862, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6448, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 683\n","\n","Epoch [683/1000], Step [0/43], Loss: 1.4874\n","Epoch [683/1000], Step [20/43], Loss: 1.4873\n","Epoch [683/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5861, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6447, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 684\n","\n","Epoch [684/1000], Step [0/43], Loss: 1.4874\n","Epoch [684/1000], Step [20/43], Loss: 1.4873\n","Epoch [684/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5860, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6447, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 685\n","\n","Epoch [685/1000], Step [0/43], Loss: 1.4874\n","Epoch [685/1000], Step [20/43], Loss: 1.4873\n","Epoch [685/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5859, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6446, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 686\n","\n","Epoch [686/1000], Step [0/43], Loss: 1.4874\n","Epoch [686/1000], Step [20/43], Loss: 1.4873\n","Epoch [686/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5858, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6445, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 687\n","\n","Epoch [687/1000], Step [0/43], Loss: 1.4874\n","Epoch [687/1000], Step [20/43], Loss: 1.4873\n","Epoch [687/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5857, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6445, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 688\n","\n","Epoch [688/1000], Step [0/43], Loss: 1.4874\n","Epoch [688/1000], Step [20/43], Loss: 1.4873\n","Epoch [688/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5857, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6444, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 689\n","\n","Epoch [689/1000], Step [0/43], Loss: 1.4874\n","Epoch [689/1000], Step [20/43], Loss: 1.4873\n","Epoch [689/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5856, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6443, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 690\n","\n","Epoch [690/1000], Step [0/43], Loss: 1.4874\n","Epoch [690/1000], Step [20/43], Loss: 1.4873\n","Epoch [690/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5855, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6442, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 691\n","\n","Epoch [691/1000], Step [0/43], Loss: 1.4874\n","Epoch [691/1000], Step [20/43], Loss: 1.4873\n","Epoch [691/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5854, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6442, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 692\n","\n","Epoch [692/1000], Step [0/43], Loss: 1.4874\n","Epoch [692/1000], Step [20/43], Loss: 1.4873\n","Epoch [692/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5853, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6441, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 693\n","\n","Epoch [693/1000], Step [0/43], Loss: 1.4874\n","Epoch [693/1000], Step [20/43], Loss: 1.4873\n","Epoch [693/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5852, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6440, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 694\n","\n","Epoch [694/1000], Step [0/43], Loss: 1.4874\n","Epoch [694/1000], Step [20/43], Loss: 1.4873\n","Epoch [694/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5852, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6440, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 695\n","\n","Epoch [695/1000], Step [0/43], Loss: 1.4874\n","Epoch [695/1000], Step [20/43], Loss: 1.4873\n","Epoch [695/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5851, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6439, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 696\n","\n","Epoch [696/1000], Step [0/43], Loss: 1.4874\n","Epoch [696/1000], Step [20/43], Loss: 1.4873\n","Epoch [696/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5850, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6438, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 697\n","\n","Epoch [697/1000], Step [0/43], Loss: 1.4874\n","Epoch [697/1000], Step [20/43], Loss: 1.4873\n","Epoch [697/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5849, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6438, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 698\n","\n","Epoch [698/1000], Step [0/43], Loss: 1.4874\n","Epoch [698/1000], Step [20/43], Loss: 1.4873\n","Epoch [698/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5848, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6437, validation acc1: 93.3105, validation acc2: 93.1389\n","\n","Epoch 699\n","\n","Epoch [699/1000], Step [0/43], Loss: 1.4874\n","Epoch [699/1000], Step [20/43], Loss: 1.4873\n","Epoch [699/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5848, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6436, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 700\n","\n","Epoch [700/1000], Step [0/43], Loss: 1.4874\n","Epoch [700/1000], Step [20/43], Loss: 1.4873\n","Epoch [700/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5847, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6435, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 701\n","\n","Epoch [701/1000], Step [0/43], Loss: 1.4874\n","Epoch [701/1000], Step [20/43], Loss: 1.4873\n","Epoch [701/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5846, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6435, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 702\n","\n","Epoch [702/1000], Step [0/43], Loss: 1.4874\n","Epoch [702/1000], Step [20/43], Loss: 1.4873\n","Epoch [702/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5845, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6434, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 703\n","\n","Epoch [703/1000], Step [0/43], Loss: 1.4874\n","Epoch [703/1000], Step [20/43], Loss: 1.4873\n","Epoch [703/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5844, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6433, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 704\n","\n","Epoch [704/1000], Step [0/43], Loss: 1.4874\n","Epoch [704/1000], Step [20/43], Loss: 1.4873\n","Epoch [704/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5844, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6433, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 705\n","\n","Epoch [705/1000], Step [0/43], Loss: 1.4874\n","Epoch [705/1000], Step [20/43], Loss: 1.4873\n","Epoch [705/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5843, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6432, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 706\n","\n","Epoch [706/1000], Step [0/43], Loss: 1.4874\n","Epoch [706/1000], Step [20/43], Loss: 1.4873\n","Epoch [706/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5842, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6431, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 707\n","\n","Epoch [707/1000], Step [0/43], Loss: 1.4874\n","Epoch [707/1000], Step [20/43], Loss: 1.4873\n","Epoch [707/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5841, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6431, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 708\n","\n","Epoch [708/1000], Step [0/43], Loss: 1.4874\n","Epoch [708/1000], Step [20/43], Loss: 1.4873\n","Epoch [708/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5840, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6430, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 709\n","\n","Epoch [709/1000], Step [0/43], Loss: 1.4874\n","Epoch [709/1000], Step [20/43], Loss: 1.4873\n","Epoch [709/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5840, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6429, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 710\n","\n","Epoch [710/1000], Step [0/43], Loss: 1.4874\n","Epoch [710/1000], Step [20/43], Loss: 1.4873\n","Epoch [710/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5839, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6429, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 711\n","\n","Epoch [711/1000], Step [0/43], Loss: 1.4874\n","Epoch [711/1000], Step [20/43], Loss: 1.4873\n","Epoch [711/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5838, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6428, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 712\n","\n","Epoch [712/1000], Step [0/43], Loss: 1.4874\n","Epoch [712/1000], Step [20/43], Loss: 1.4873\n","Epoch [712/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5837, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6427, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 713\n","\n","Epoch [713/1000], Step [0/43], Loss: 1.4874\n","Epoch [713/1000], Step [20/43], Loss: 1.4873\n","Epoch [713/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5837, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6427, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 714\n","\n","Epoch [714/1000], Step [0/43], Loss: 1.4874\n","Epoch [714/1000], Step [20/43], Loss: 1.4873\n","Epoch [714/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5836, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6426, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 715\n","\n","Epoch [715/1000], Step [0/43], Loss: 1.4874\n","Epoch [715/1000], Step [20/43], Loss: 1.4873\n","Epoch [715/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5835, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6425, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 716\n","\n","Epoch [716/1000], Step [0/43], Loss: 1.4874\n","Epoch [716/1000], Step [20/43], Loss: 1.4873\n","Epoch [716/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5834, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6425, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 717\n","\n","Epoch [717/1000], Step [0/43], Loss: 1.4874\n","Epoch [717/1000], Step [20/43], Loss: 1.4873\n","Epoch [717/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5834, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6424, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 718\n","\n","Epoch [718/1000], Step [0/43], Loss: 1.4874\n","Epoch [718/1000], Step [20/43], Loss: 1.4873\n","Epoch [718/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5833, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6423, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 719\n","\n","Epoch [719/1000], Step [0/43], Loss: 1.4874\n","Epoch [719/1000], Step [20/43], Loss: 1.4873\n","Epoch [719/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5832, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6423, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 720\n","\n","Epoch [720/1000], Step [0/43], Loss: 1.4874\n","Epoch [720/1000], Step [20/43], Loss: 1.4873\n","Epoch [720/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5831, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6422, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 721\n","\n","Epoch [721/1000], Step [0/43], Loss: 1.4874\n","Epoch [721/1000], Step [20/43], Loss: 1.4873\n","Epoch [721/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5831, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6421, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 722\n","\n","Epoch [722/1000], Step [0/43], Loss: 1.4874\n","Epoch [722/1000], Step [20/43], Loss: 1.4873\n","Epoch [722/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5830, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6421, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 723\n","\n","Epoch [723/1000], Step [0/43], Loss: 1.4874\n","Epoch [723/1000], Step [20/43], Loss: 1.4873\n","Epoch [723/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5829, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6420, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 724\n","\n","Epoch [724/1000], Step [0/43], Loss: 1.4874\n","Epoch [724/1000], Step [20/43], Loss: 1.4873\n","Epoch [724/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5828, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6419, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 725\n","\n","Epoch [725/1000], Step [0/43], Loss: 1.4874\n","Epoch [725/1000], Step [20/43], Loss: 1.4873\n","Epoch [725/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5828, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6419, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 726\n","\n","Epoch [726/1000], Step [0/43], Loss: 1.4874\n","Epoch [726/1000], Step [20/43], Loss: 1.4873\n","Epoch [726/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5827, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6418, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 727\n","\n","Epoch [727/1000], Step [0/43], Loss: 1.4874\n","Epoch [727/1000], Step [20/43], Loss: 1.4873\n","Epoch [727/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5826, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6417, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 728\n","\n","Epoch [728/1000], Step [0/43], Loss: 1.4874\n","Epoch [728/1000], Step [20/43], Loss: 1.4873\n","Epoch [728/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5825, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6417, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 729\n","\n","Epoch [729/1000], Step [0/43], Loss: 1.4874\n","Epoch [729/1000], Step [20/43], Loss: 1.4873\n","Epoch [729/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5825, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6416, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 730\n","\n","Epoch [730/1000], Step [0/43], Loss: 1.4874\n","Epoch [730/1000], Step [20/43], Loss: 1.4873\n","Epoch [730/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5824, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6415, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 731\n","\n","Epoch [731/1000], Step [0/43], Loss: 1.4874\n","Epoch [731/1000], Step [20/43], Loss: 1.4873\n","Epoch [731/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5823, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6415, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 732\n","\n","Epoch [732/1000], Step [0/43], Loss: 1.4874\n","Epoch [732/1000], Step [20/43], Loss: 1.4873\n","Epoch [732/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5822, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6414, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 733\n","\n","Epoch [733/1000], Step [0/43], Loss: 1.4874\n","Epoch [733/1000], Step [20/43], Loss: 1.4873\n","Epoch [733/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5822, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6414, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 734\n","\n","Epoch [734/1000], Step [0/43], Loss: 1.4874\n","Epoch [734/1000], Step [20/43], Loss: 1.4873\n","Epoch [734/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5821, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6413, validation acc1: 93.4820, validation acc2: 93.4820\n","\n","Epoch 735\n","\n","Epoch [735/1000], Step [0/43], Loss: 1.4874\n","Epoch [735/1000], Step [20/43], Loss: 1.4873\n","Epoch [735/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5820, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6412, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 736\n","\n","Epoch [736/1000], Step [0/43], Loss: 1.4874\n","Epoch [736/1000], Step [20/43], Loss: 1.4873\n","Epoch [736/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5819, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6412, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 737\n","\n","Epoch [737/1000], Step [0/43], Loss: 1.4874\n","Epoch [737/1000], Step [20/43], Loss: 1.4873\n","Epoch [737/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5819, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6411, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 738\n","\n","Epoch [738/1000], Step [0/43], Loss: 1.4874\n","Epoch [738/1000], Step [20/43], Loss: 1.4873\n","Epoch [738/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5818, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6410, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 739\n","\n","Epoch [739/1000], Step [0/43], Loss: 1.4874\n","Epoch [739/1000], Step [20/43], Loss: 1.4873\n","Epoch [739/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5817, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6410, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 740\n","\n","Epoch [740/1000], Step [0/43], Loss: 1.4874\n","Epoch [740/1000], Step [20/43], Loss: 1.4873\n","Epoch [740/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5817, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6409, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 741\n","\n","Epoch [741/1000], Step [0/43], Loss: 1.4874\n","Epoch [741/1000], Step [20/43], Loss: 1.4873\n","Epoch [741/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5816, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6408, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 742\n","\n","Epoch [742/1000], Step [0/43], Loss: 1.4874\n","Epoch [742/1000], Step [20/43], Loss: 1.4873\n","Epoch [742/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5815, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6408, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 743\n","\n","Epoch [743/1000], Step [0/43], Loss: 1.4874\n","Epoch [743/1000], Step [20/43], Loss: 1.4873\n","Epoch [743/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5814, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6407, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 744\n","\n","Epoch [744/1000], Step [0/43], Loss: 1.4874\n","Epoch [744/1000], Step [20/43], Loss: 1.4873\n","Epoch [744/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5814, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6407, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 745\n","\n","Epoch [745/1000], Step [0/43], Loss: 1.4874\n","Epoch [745/1000], Step [20/43], Loss: 1.4873\n","Epoch [745/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5813, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6406, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 746\n","\n","Epoch [746/1000], Step [0/43], Loss: 1.4874\n","Epoch [746/1000], Step [20/43], Loss: 1.4873\n","Epoch [746/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5812, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6405, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 747\n","\n","Epoch [747/1000], Step [0/43], Loss: 1.4874\n","Epoch [747/1000], Step [20/43], Loss: 1.4873\n","Epoch [747/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5812, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6405, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 748\n","\n","Epoch [748/1000], Step [0/43], Loss: 1.4874\n","Epoch [748/1000], Step [20/43], Loss: 1.4873\n","Epoch [748/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5811, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6404, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 749\n","\n","Epoch [749/1000], Step [0/43], Loss: 1.4874\n","Epoch [749/1000], Step [20/43], Loss: 1.4873\n","Epoch [749/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5810, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6404, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 750\n","\n","Epoch [750/1000], Step [0/43], Loss: 1.4874\n","Epoch [750/1000], Step [20/43], Loss: 1.4873\n","Epoch [750/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5810, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6403, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 751\n","\n","Epoch [751/1000], Step [0/43], Loss: 1.4874\n","Epoch [751/1000], Step [20/43], Loss: 1.4873\n","Epoch [751/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5809, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6402, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 752\n","\n","Epoch [752/1000], Step [0/43], Loss: 1.4874\n","Epoch [752/1000], Step [20/43], Loss: 1.4873\n","Epoch [752/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5808, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6402, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 753\n","\n","Epoch [753/1000], Step [0/43], Loss: 1.4874\n","Epoch [753/1000], Step [20/43], Loss: 1.4873\n","Epoch [753/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5807, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6401, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 754\n","\n","Epoch [754/1000], Step [0/43], Loss: 1.4874\n","Epoch [754/1000], Step [20/43], Loss: 1.4873\n","Epoch [754/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5807, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6401, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 755\n","\n","Epoch [755/1000], Step [0/43], Loss: 1.4874\n","Epoch [755/1000], Step [20/43], Loss: 1.4873\n","Epoch [755/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5806, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6400, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 756\n","\n","Epoch [756/1000], Step [0/43], Loss: 1.4874\n","Epoch [756/1000], Step [20/43], Loss: 1.4873\n","Epoch [756/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5805, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6399, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 757\n","\n","Epoch [757/1000], Step [0/43], Loss: 1.4874\n","Epoch [757/1000], Step [20/43], Loss: 1.4873\n","Epoch [757/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5805, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6399, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 758\n","\n","Epoch [758/1000], Step [0/43], Loss: 1.4874\n","Epoch [758/1000], Step [20/43], Loss: 1.4873\n","Epoch [758/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5804, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6398, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 759\n","\n","Epoch [759/1000], Step [0/43], Loss: 1.4874\n","Epoch [759/1000], Step [20/43], Loss: 1.4873\n","Epoch [759/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5803, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6398, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 760\n","\n","Epoch [760/1000], Step [0/43], Loss: 1.4874\n","Epoch [760/1000], Step [20/43], Loss: 1.4873\n","Epoch [760/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5803, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6397, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 761\n","\n","Epoch [761/1000], Step [0/43], Loss: 1.4874\n","Epoch [761/1000], Step [20/43], Loss: 1.4873\n","Epoch [761/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5802, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6396, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 762\n","\n","Epoch [762/1000], Step [0/43], Loss: 1.4874\n","Epoch [762/1000], Step [20/43], Loss: 1.4873\n","Epoch [762/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5801, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6396, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 763\n","\n","Epoch [763/1000], Step [0/43], Loss: 1.4874\n","Epoch [763/1000], Step [20/43], Loss: 1.4873\n","Epoch [763/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5801, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6395, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 764\n","\n","Epoch [764/1000], Step [0/43], Loss: 1.4874\n","Epoch [764/1000], Step [20/43], Loss: 1.4873\n","Epoch [764/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5800, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6395, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 765\n","\n","Epoch [765/1000], Step [0/43], Loss: 1.4874\n","Epoch [765/1000], Step [20/43], Loss: 1.4873\n","Epoch [765/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5799, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6394, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 766\n","\n","Epoch [766/1000], Step [0/43], Loss: 1.4874\n","Epoch [766/1000], Step [20/43], Loss: 1.4873\n","Epoch [766/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5799, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6393, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 767\n","\n","Epoch [767/1000], Step [0/43], Loss: 1.4874\n","Epoch [767/1000], Step [20/43], Loss: 1.4873\n","Epoch [767/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5798, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6393, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 768\n","\n","Epoch [768/1000], Step [0/43], Loss: 1.4874\n","Epoch [768/1000], Step [20/43], Loss: 1.4873\n","Epoch [768/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5797, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6392, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 769\n","\n","Epoch [769/1000], Step [0/43], Loss: 1.4874\n","Epoch [769/1000], Step [20/43], Loss: 1.4873\n","Epoch [769/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5797, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6392, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 770\n","\n","Epoch [770/1000], Step [0/43], Loss: 1.4874\n","Epoch [770/1000], Step [20/43], Loss: 1.4873\n","Epoch [770/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5796, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6391, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 771\n","\n","Epoch [771/1000], Step [0/43], Loss: 1.4874\n","Epoch [771/1000], Step [20/43], Loss: 1.4873\n","Epoch [771/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5795, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6390, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 772\n","\n","Epoch [772/1000], Step [0/43], Loss: 1.4874\n","Epoch [772/1000], Step [20/43], Loss: 1.4873\n","Epoch [772/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5795, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6390, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 773\n","\n","Epoch [773/1000], Step [0/43], Loss: 1.4874\n","Epoch [773/1000], Step [20/43], Loss: 1.4873\n","Epoch [773/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5794, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6389, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 774\n","\n","Epoch [774/1000], Step [0/43], Loss: 1.4874\n","Epoch [774/1000], Step [20/43], Loss: 1.4873\n","Epoch [774/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5793, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6389, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 775\n","\n","Epoch [775/1000], Step [0/43], Loss: 1.4874\n","Epoch [775/1000], Step [20/43], Loss: 1.4873\n","Epoch [775/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5793, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6388, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 776\n","\n","Epoch [776/1000], Step [0/43], Loss: 1.4874\n","Epoch [776/1000], Step [20/43], Loss: 1.4873\n","Epoch [776/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5792, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6388, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 777\n","\n","Epoch [777/1000], Step [0/43], Loss: 1.4874\n","Epoch [777/1000], Step [20/43], Loss: 1.4876\n","Epoch [777/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5791, train acc1: 98.0854, train acc2: 96.5390\n","validation loss: 1.6387, validation acc1: 93.9966, validation acc2: 93.6535\n","\n","Detected network improvement, saving current model\n","Epoch 778\n","\n","Epoch [778/1000], Step [0/43], Loss: 1.4874\n","Epoch [778/1000], Step [20/43], Loss: 1.4874\n","Epoch [778/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5791, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6386, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 779\n","\n","Epoch [779/1000], Step [0/43], Loss: 1.4874\n","Epoch [779/1000], Step [20/43], Loss: 1.4874\n","Epoch [779/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5790, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6386, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 780\n","\n","Epoch [780/1000], Step [0/43], Loss: 1.4874\n","Epoch [780/1000], Step [20/43], Loss: 1.4874\n","Epoch [780/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5790, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6385, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 781\n","\n","Epoch [781/1000], Step [0/43], Loss: 1.4874\n","Epoch [781/1000], Step [20/43], Loss: 1.4874\n","Epoch [781/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5789, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6385, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 782\n","\n","Epoch [782/1000], Step [0/43], Loss: 1.4874\n","Epoch [782/1000], Step [20/43], Loss: 1.4874\n","Epoch [782/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5788, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6384, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 783\n","\n","Epoch [783/1000], Step [0/43], Loss: 1.4874\n","Epoch [783/1000], Step [20/43], Loss: 1.4874\n","Epoch [783/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5788, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6384, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 784\n","\n","Epoch [784/1000], Step [0/43], Loss: 1.4874\n","Epoch [784/1000], Step [20/43], Loss: 1.4874\n","Epoch [784/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5787, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6383, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 785\n","\n","Epoch [785/1000], Step [0/43], Loss: 1.4874\n","Epoch [785/1000], Step [20/43], Loss: 1.4874\n","Epoch [785/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5786, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6382, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 786\n","\n","Epoch [786/1000], Step [0/43], Loss: 1.4874\n","Epoch [786/1000], Step [20/43], Loss: 1.4874\n","Epoch [786/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5786, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6382, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 787\n","\n","Epoch [787/1000], Step [0/43], Loss: 1.4874\n","Epoch [787/1000], Step [20/43], Loss: 1.4874\n","Epoch [787/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5785, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6381, validation acc1: 93.8250, validation acc2: 93.6535\n","\n","Epoch 788\n","\n","Epoch [788/1000], Step [0/43], Loss: 1.4874\n","Epoch [788/1000], Step [20/43], Loss: 1.4874\n","Epoch [788/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5784, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6381, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 789\n","\n","Epoch [789/1000], Step [0/43], Loss: 1.4874\n","Epoch [789/1000], Step [20/43], Loss: 1.4874\n","Epoch [789/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5784, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6380, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 790\n","\n","Epoch [790/1000], Step [0/43], Loss: 1.4874\n","Epoch [790/1000], Step [20/43], Loss: 1.4874\n","Epoch [790/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5783, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6380, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 791\n","\n","Epoch [791/1000], Step [0/43], Loss: 1.4874\n","Epoch [791/1000], Step [20/43], Loss: 1.4874\n","Epoch [791/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5782, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6379, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 792\n","\n","Epoch [792/1000], Step [0/43], Loss: 1.4874\n","Epoch [792/1000], Step [20/43], Loss: 1.4874\n","Epoch [792/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5782, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6378, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 793\n","\n","Epoch [793/1000], Step [0/43], Loss: 1.4874\n","Epoch [793/1000], Step [20/43], Loss: 1.4874\n","Epoch [793/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5781, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6378, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 794\n","\n","Epoch [794/1000], Step [0/43], Loss: 1.4874\n","Epoch [794/1000], Step [20/43], Loss: 1.4874\n","Epoch [794/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5781, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6377, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 795\n","\n","Epoch [795/1000], Step [0/43], Loss: 1.4874\n","Epoch [795/1000], Step [20/43], Loss: 1.4874\n","Epoch [795/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5780, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6377, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 796\n","\n","Epoch [796/1000], Step [0/43], Loss: 1.4874\n","Epoch [796/1000], Step [20/43], Loss: 1.4874\n","Epoch [796/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5779, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6376, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 797\n","\n","Epoch [797/1000], Step [0/43], Loss: 1.4874\n","Epoch [797/1000], Step [20/43], Loss: 1.4874\n","Epoch [797/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5779, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6376, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 798\n","\n","Epoch [798/1000], Step [0/43], Loss: 1.4874\n","Epoch [798/1000], Step [20/43], Loss: 1.4874\n","Epoch [798/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5778, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6375, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 799\n","\n","Epoch [799/1000], Step [0/43], Loss: 1.4874\n","Epoch [799/1000], Step [20/43], Loss: 1.4873\n","Epoch [799/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5777, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6375, validation acc1: 93.8250, validation acc2: 93.6535\n","\n","Epoch 800\n","\n","Epoch [800/1000], Step [0/43], Loss: 1.4874\n","Epoch [800/1000], Step [20/43], Loss: 1.4874\n","Epoch [800/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5777, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6374, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 801\n","\n","Epoch [801/1000], Step [0/43], Loss: 1.4874\n","Epoch [801/1000], Step [20/43], Loss: 1.4874\n","Epoch [801/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5776, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6374, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 802\n","\n","Epoch [802/1000], Step [0/43], Loss: 1.4874\n","Epoch [802/1000], Step [20/43], Loss: 1.4874\n","Epoch [802/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5776, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6373, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 803\n","\n","Epoch [803/1000], Step [0/43], Loss: 1.4874\n","Epoch [803/1000], Step [20/43], Loss: 1.4874\n","Epoch [803/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5775, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6372, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 804\n","\n","Epoch [804/1000], Step [0/43], Loss: 1.4874\n","Epoch [804/1000], Step [20/43], Loss: 1.4874\n","Epoch [804/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5774, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6372, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 805\n","\n","Epoch [805/1000], Step [0/43], Loss: 1.4874\n","Epoch [805/1000], Step [20/43], Loss: 1.4874\n","Epoch [805/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5774, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6371, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 806\n","\n","Epoch [806/1000], Step [0/43], Loss: 1.4874\n","Epoch [806/1000], Step [20/43], Loss: 1.4874\n","Epoch [806/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5773, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6371, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 807\n","\n","Epoch [807/1000], Step [0/43], Loss: 1.4874\n","Epoch [807/1000], Step [20/43], Loss: 1.4874\n","Epoch [807/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5772, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6370, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 808\n","\n","Epoch [808/1000], Step [0/43], Loss: 1.4874\n","Epoch [808/1000], Step [20/43], Loss: 1.4874\n","Epoch [808/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5772, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6370, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 809\n","\n","Epoch [809/1000], Step [0/43], Loss: 1.4874\n","Epoch [809/1000], Step [20/43], Loss: 1.4874\n","Epoch [809/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5771, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6369, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 810\n","\n","Epoch [810/1000], Step [0/43], Loss: 1.4874\n","Epoch [810/1000], Step [20/43], Loss: 1.4874\n","Epoch [810/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5771, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6369, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 811\n","\n","Epoch [811/1000], Step [0/43], Loss: 1.4874\n","Epoch [811/1000], Step [20/43], Loss: 1.4874\n","Epoch [811/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5770, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6368, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 812\n","\n","Epoch [812/1000], Step [0/43], Loss: 1.4874\n","Epoch [812/1000], Step [20/43], Loss: 1.4874\n","Epoch [812/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5769, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6368, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 813\n","\n","Epoch [813/1000], Step [0/43], Loss: 1.4874\n","Epoch [813/1000], Step [20/43], Loss: 1.4874\n","Epoch [813/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5769, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6367, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 814\n","\n","Epoch [814/1000], Step [0/43], Loss: 1.4874\n","Epoch [814/1000], Step [20/43], Loss: 1.4874\n","Epoch [814/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5768, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6367, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 815\n","\n","Epoch [815/1000], Step [0/43], Loss: 1.4874\n","Epoch [815/1000], Step [20/43], Loss: 1.4874\n","Epoch [815/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5768, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6366, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 816\n","\n","Epoch [816/1000], Step [0/43], Loss: 1.4874\n","Epoch [816/1000], Step [20/43], Loss: 1.4874\n","Epoch [816/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5767, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6366, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 817\n","\n","Epoch [817/1000], Step [0/43], Loss: 1.4874\n","Epoch [817/1000], Step [20/43], Loss: 1.4873\n","Epoch [817/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5766, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6365, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 818\n","\n","Epoch [818/1000], Step [0/43], Loss: 1.4874\n","Epoch [818/1000], Step [20/43], Loss: 1.4873\n","Epoch [818/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5766, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6365, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 819\n","\n","Epoch [819/1000], Step [0/43], Loss: 1.4874\n","Epoch [819/1000], Step [20/43], Loss: 1.4874\n","Epoch [819/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5765, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6364, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 820\n","\n","Epoch [820/1000], Step [0/43], Loss: 1.4874\n","Epoch [820/1000], Step [20/43], Loss: 1.4874\n","Epoch [820/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5765, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6364, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 821\n","\n","Epoch [821/1000], Step [0/43], Loss: 1.4874\n","Epoch [821/1000], Step [20/43], Loss: 1.4874\n","Epoch [821/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5764, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6363, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 822\n","\n","Epoch [822/1000], Step [0/43], Loss: 1.4874\n","Epoch [822/1000], Step [20/43], Loss: 1.4874\n","Epoch [822/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5764, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6362, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 823\n","\n","Epoch [823/1000], Step [0/43], Loss: 1.4874\n","Epoch [823/1000], Step [20/43], Loss: 1.4873\n","Epoch [823/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5763, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6362, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 824\n","\n","Epoch [824/1000], Step [0/43], Loss: 1.4874\n","Epoch [824/1000], Step [20/43], Loss: 1.4873\n","Epoch [824/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5762, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6361, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 825\n","\n","Epoch [825/1000], Step [0/43], Loss: 1.4874\n","Epoch [825/1000], Step [20/43], Loss: 1.4873\n","Epoch [825/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5762, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6361, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 826\n","\n","Epoch [826/1000], Step [0/43], Loss: 1.4874\n","Epoch [826/1000], Step [20/43], Loss: 1.4874\n","Epoch [826/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5761, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6360, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 827\n","\n","Epoch [827/1000], Step [0/43], Loss: 1.4874\n","Epoch [827/1000], Step [20/43], Loss: 1.4874\n","Epoch [827/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5761, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6360, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 828\n","\n","Epoch [828/1000], Step [0/43], Loss: 1.4874\n","Epoch [828/1000], Step [20/43], Loss: 1.4874\n","Epoch [828/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5760, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6359, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 829\n","\n","Epoch [829/1000], Step [0/43], Loss: 1.4874\n","Epoch [829/1000], Step [20/43], Loss: 1.4873\n","Epoch [829/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5759, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6359, validation acc1: 93.8250, validation acc2: 93.6535\n","\n","Epoch 830\n","\n","Epoch [830/1000], Step [0/43], Loss: 1.4874\n","Epoch [830/1000], Step [20/43], Loss: 1.4874\n","Epoch [830/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5759, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6358, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 831\n","\n","Epoch [831/1000], Step [0/43], Loss: 1.4874\n","Epoch [831/1000], Step [20/43], Loss: 1.4873\n","Epoch [831/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5758, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6358, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 832\n","\n","Epoch [832/1000], Step [0/43], Loss: 1.4874\n","Epoch [832/1000], Step [20/43], Loss: 1.4874\n","Epoch [832/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5758, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6357, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 833\n","\n","Epoch [833/1000], Step [0/43], Loss: 1.4874\n","Epoch [833/1000], Step [20/43], Loss: 1.4874\n","Epoch [833/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5757, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6357, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 834\n","\n","Epoch [834/1000], Step [0/43], Loss: 1.4874\n","Epoch [834/1000], Step [20/43], Loss: 1.4873\n","Epoch [834/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5757, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6356, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 835\n","\n","Epoch [835/1000], Step [0/43], Loss: 1.4874\n","Epoch [835/1000], Step [20/43], Loss: 1.4873\n","Epoch [835/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5756, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6356, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 836\n","\n","Epoch [836/1000], Step [0/43], Loss: 1.4874\n","Epoch [836/1000], Step [20/43], Loss: 1.4874\n","Epoch [836/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5755, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6355, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 837\n","\n","Epoch [837/1000], Step [0/43], Loss: 1.4874\n","Epoch [837/1000], Step [20/43], Loss: 1.4873\n","Epoch [837/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5755, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6355, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 838\n","\n","Epoch [838/1000], Step [0/43], Loss: 1.4874\n","Epoch [838/1000], Step [20/43], Loss: 1.4874\n","Epoch [838/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5754, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6354, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 839\n","\n","Epoch [839/1000], Step [0/43], Loss: 1.4874\n","Epoch [839/1000], Step [20/43], Loss: 1.4874\n","Epoch [839/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5754, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6354, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 840\n","\n","Epoch [840/1000], Step [0/43], Loss: 1.4874\n","Epoch [840/1000], Step [20/43], Loss: 1.4873\n","Epoch [840/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5753, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6353, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 841\n","\n","Epoch [841/1000], Step [0/43], Loss: 1.4874\n","Epoch [841/1000], Step [20/43], Loss: 1.4873\n","Epoch [841/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5753, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6353, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 842\n","\n","Epoch [842/1000], Step [0/43], Loss: 1.4874\n","Epoch [842/1000], Step [20/43], Loss: 1.4874\n","Epoch [842/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5752, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6352, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 843\n","\n","Epoch [843/1000], Step [0/43], Loss: 1.4874\n","Epoch [843/1000], Step [20/43], Loss: 1.4873\n","Epoch [843/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5751, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6352, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 844\n","\n","Epoch [844/1000], Step [0/43], Loss: 1.4874\n","Epoch [844/1000], Step [20/43], Loss: 1.4874\n","Epoch [844/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5751, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6352, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 845\n","\n","Epoch [845/1000], Step [0/43], Loss: 1.4874\n","Epoch [845/1000], Step [20/43], Loss: 1.4874\n","Epoch [845/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5750, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6351, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 846\n","\n","Epoch [846/1000], Step [0/43], Loss: 1.4874\n","Epoch [846/1000], Step [20/43], Loss: 1.4874\n","Epoch [846/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5750, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6351, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 847\n","\n","Epoch [847/1000], Step [0/43], Loss: 1.4874\n","Epoch [847/1000], Step [20/43], Loss: 1.4873\n","Epoch [847/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5749, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6350, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 848\n","\n","Epoch [848/1000], Step [0/43], Loss: 1.4874\n","Epoch [848/1000], Step [20/43], Loss: 1.4873\n","Epoch [848/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5749, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6350, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 849\n","\n","Epoch [849/1000], Step [0/43], Loss: 1.4874\n","Epoch [849/1000], Step [20/43], Loss: 1.4873\n","Epoch [849/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5748, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6349, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 850\n","\n","Epoch [850/1000], Step [0/43], Loss: 1.4874\n","Epoch [850/1000], Step [20/43], Loss: 1.4874\n","Epoch [850/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5748, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6349, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 851\n","\n","Epoch [851/1000], Step [0/43], Loss: 1.4874\n","Epoch [851/1000], Step [20/43], Loss: 1.4873\n","Epoch [851/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5747, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6348, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 852\n","\n","Epoch [852/1000], Step [0/43], Loss: 1.4874\n","Epoch [852/1000], Step [20/43], Loss: 1.4874\n","Epoch [852/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5747, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6348, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 853\n","\n","Epoch [853/1000], Step [0/43], Loss: 1.4874\n","Epoch [853/1000], Step [20/43], Loss: 1.4873\n","Epoch [853/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5746, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6347, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 854\n","\n","Epoch [854/1000], Step [0/43], Loss: 1.4874\n","Epoch [854/1000], Step [20/43], Loss: 1.4874\n","Epoch [854/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5745, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6347, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 855\n","\n","Epoch [855/1000], Step [0/43], Loss: 1.4874\n","Epoch [855/1000], Step [20/43], Loss: 1.4873\n","Epoch [855/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5745, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6346, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 856\n","\n","Epoch [856/1000], Step [0/43], Loss: 1.4874\n","Epoch [856/1000], Step [20/43], Loss: 1.4874\n","Epoch [856/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5744, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6346, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 857\n","\n","Epoch [857/1000], Step [0/43], Loss: 1.4874\n","Epoch [857/1000], Step [20/43], Loss: 1.4873\n","Epoch [857/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5744, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6345, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 858\n","\n","Epoch [858/1000], Step [0/43], Loss: 1.4874\n","Epoch [858/1000], Step [20/43], Loss: 1.4873\n","Epoch [858/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5743, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6345, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 859\n","\n","Epoch [859/1000], Step [0/43], Loss: 1.4874\n","Epoch [859/1000], Step [20/43], Loss: 1.4873\n","Epoch [859/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5743, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6344, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 860\n","\n","Epoch [860/1000], Step [0/43], Loss: 1.4874\n","Epoch [860/1000], Step [20/43], Loss: 1.4873\n","Epoch [860/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5742, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6344, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 861\n","\n","Epoch [861/1000], Step [0/43], Loss: 1.4874\n","Epoch [861/1000], Step [20/43], Loss: 1.4874\n","Epoch [861/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5742, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6343, validation acc1: 93.4820, validation acc2: 92.9674\n","\n","Epoch 862\n","\n","Epoch [862/1000], Step [0/43], Loss: 1.4874\n","Epoch [862/1000], Step [20/43], Loss: 1.4874\n","Epoch [862/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5741, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6343, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 863\n","\n","Epoch [863/1000], Step [0/43], Loss: 1.4874\n","Epoch [863/1000], Step [20/43], Loss: 1.4873\n","Epoch [863/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5741, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6343, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 864\n","\n","Epoch [864/1000], Step [0/43], Loss: 1.4874\n","Epoch [864/1000], Step [20/43], Loss: 1.4873\n","Epoch [864/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5740, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6342, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 865\n","\n","Epoch [865/1000], Step [0/43], Loss: 1.4874\n","Epoch [865/1000], Step [20/43], Loss: 1.4873\n","Epoch [865/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5740, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6342, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 866\n","\n","Epoch [866/1000], Step [0/43], Loss: 1.4874\n","Epoch [866/1000], Step [20/43], Loss: 1.4873\n","Epoch [866/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5739, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6341, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 867\n","\n","Epoch [867/1000], Step [0/43], Loss: 1.4874\n","Epoch [867/1000], Step [20/43], Loss: 1.4873\n","Epoch [867/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5738, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6341, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 868\n","\n","Epoch [868/1000], Step [0/43], Loss: 1.4874\n","Epoch [868/1000], Step [20/43], Loss: 1.4873\n","Epoch [868/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5738, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6340, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 869\n","\n","Epoch [869/1000], Step [0/43], Loss: 1.4874\n","Epoch [869/1000], Step [20/43], Loss: 1.4874\n","Epoch [869/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5737, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6340, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 870\n","\n","Epoch [870/1000], Step [0/43], Loss: 1.4874\n","Epoch [870/1000], Step [20/43], Loss: 1.4874\n","Epoch [870/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5737, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6339, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 871\n","\n","Epoch [871/1000], Step [0/43], Loss: 1.4874\n","Epoch [871/1000], Step [20/43], Loss: 1.4873\n","Epoch [871/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5736, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6339, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 872\n","\n","Epoch [872/1000], Step [0/43], Loss: 1.4874\n","Epoch [872/1000], Step [20/43], Loss: 1.4873\n","Epoch [872/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5736, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6338, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 873\n","\n","Epoch [873/1000], Step [0/43], Loss: 1.4874\n","Epoch [873/1000], Step [20/43], Loss: 1.4873\n","Epoch [873/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5735, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6338, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 874\n","\n","Epoch [874/1000], Step [0/43], Loss: 1.4874\n","Epoch [874/1000], Step [20/43], Loss: 1.4874\n","Epoch [874/1000], Step [40/43], Loss: 1.6441\n","\n","train loss: 1.5735, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6338, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 875\n","\n","Epoch [875/1000], Step [0/43], Loss: 1.4874\n","Epoch [875/1000], Step [20/43], Loss: 1.4873\n","Epoch [875/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5734, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6337, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 876\n","\n","Epoch [876/1000], Step [0/43], Loss: 1.4874\n","Epoch [876/1000], Step [20/43], Loss: 1.4874\n","Epoch [876/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5734, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6337, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 877\n","\n","Epoch [877/1000], Step [0/43], Loss: 1.4874\n","Epoch [877/1000], Step [20/43], Loss: 1.4873\n","Epoch [877/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5733, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6336, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 878\n","\n","Epoch [878/1000], Step [0/43], Loss: 1.4874\n","Epoch [878/1000], Step [20/43], Loss: 1.4874\n","Epoch [878/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5733, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6336, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 879\n","\n","Epoch [879/1000], Step [0/43], Loss: 1.4874\n","Epoch [879/1000], Step [20/43], Loss: 1.4873\n","Epoch [879/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5732, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6335, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 880\n","\n","Epoch [880/1000], Step [0/43], Loss: 1.4874\n","Epoch [880/1000], Step [20/43], Loss: 1.4874\n","Epoch [880/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5732, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6335, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 881\n","\n","Epoch [881/1000], Step [0/43], Loss: 1.4874\n","Epoch [881/1000], Step [20/43], Loss: 1.4874\n","Epoch [881/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5731, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6334, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 882\n","\n","Epoch [882/1000], Step [0/43], Loss: 1.4874\n","Epoch [882/1000], Step [20/43], Loss: 1.4874\n","Epoch [882/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5731, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6334, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 883\n","\n","Epoch [883/1000], Step [0/43], Loss: 1.4874\n","Epoch [883/1000], Step [20/43], Loss: 1.4873\n","Epoch [883/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5730, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6333, validation acc1: 93.8250, validation acc2: 93.6535\n","\n","Epoch 884\n","\n","Epoch [884/1000], Step [0/43], Loss: 1.4874\n","Epoch [884/1000], Step [20/43], Loss: 1.4874\n","Epoch [884/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5730, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6333, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 885\n","\n","Epoch [885/1000], Step [0/43], Loss: 1.4874\n","Epoch [885/1000], Step [20/43], Loss: 1.4873\n","Epoch [885/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5729, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6333, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 886\n","\n","Epoch [886/1000], Step [0/43], Loss: 1.4874\n","Epoch [886/1000], Step [20/43], Loss: 1.4874\n","Epoch [886/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5729, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6332, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 887\n","\n","Epoch [887/1000], Step [0/43], Loss: 1.4874\n","Epoch [887/1000], Step [20/43], Loss: 1.4873\n","Epoch [887/1000], Step [40/43], Loss: 1.6442\n","\n","train loss: 1.5728, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6332, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 888\n","\n","Epoch [888/1000], Step [0/43], Loss: 1.4874\n","Epoch [888/1000], Step [20/43], Loss: 1.4873\n","Epoch [888/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5728, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6331, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 889\n","\n","Epoch [889/1000], Step [0/43], Loss: 1.4874\n","Epoch [889/1000], Step [20/43], Loss: 1.4873\n","Epoch [889/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5727, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6331, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 890\n","\n","Epoch [890/1000], Step [0/43], Loss: 1.4874\n","Epoch [890/1000], Step [20/43], Loss: 1.4873\n","Epoch [890/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5727, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6330, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 891\n","\n","Epoch [891/1000], Step [0/43], Loss: 1.4874\n","Epoch [891/1000], Step [20/43], Loss: 1.4873\n","Epoch [891/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5726, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6330, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 892\n","\n","Epoch [892/1000], Step [0/43], Loss: 1.4874\n","Epoch [892/1000], Step [20/43], Loss: 1.4874\n","Epoch [892/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5726, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6329, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 893\n","\n","Epoch [893/1000], Step [0/43], Loss: 1.4874\n","Epoch [893/1000], Step [20/43], Loss: 1.4874\n","Epoch [893/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5725, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6329, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 894\n","\n","Epoch [894/1000], Step [0/43], Loss: 1.4874\n","Epoch [894/1000], Step [20/43], Loss: 1.4873\n","Epoch [894/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5725, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6329, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 895\n","\n","Epoch [895/1000], Step [0/43], Loss: 1.4874\n","Epoch [895/1000], Step [20/43], Loss: 1.4873\n","Epoch [895/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5724, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6328, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 896\n","\n","Epoch [896/1000], Step [0/43], Loss: 1.4874\n","Epoch [896/1000], Step [20/43], Loss: 1.4873\n","Epoch [896/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5724, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6328, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 897\n","\n","Epoch [897/1000], Step [0/43], Loss: 1.4874\n","Epoch [897/1000], Step [20/43], Loss: 1.4873\n","Epoch [897/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5723, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6327, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 898\n","\n","Epoch [898/1000], Step [0/43], Loss: 1.4874\n","Epoch [898/1000], Step [20/43], Loss: 1.4874\n","Epoch [898/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5723, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6327, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 899\n","\n","Epoch [899/1000], Step [0/43], Loss: 1.4874\n","Epoch [899/1000], Step [20/43], Loss: 1.4873\n","Epoch [899/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5722, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6326, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 900\n","\n","Epoch [900/1000], Step [0/43], Loss: 1.4874\n","Epoch [900/1000], Step [20/43], Loss: 1.4873\n","Epoch [900/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5722, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6326, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 901\n","\n","Epoch [901/1000], Step [0/43], Loss: 1.4874\n","Epoch [901/1000], Step [20/43], Loss: 1.4873\n","Epoch [901/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5721, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6326, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 902\n","\n","Epoch [902/1000], Step [0/43], Loss: 1.4874\n","Epoch [902/1000], Step [20/43], Loss: 1.4873\n","Epoch [902/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5721, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6325, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 903\n","\n","Epoch [903/1000], Step [0/43], Loss: 1.4874\n","Epoch [903/1000], Step [20/43], Loss: 1.4873\n","Epoch [903/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5720, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6325, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 904\n","\n","Epoch [904/1000], Step [0/43], Loss: 1.4874\n","Epoch [904/1000], Step [20/43], Loss: 1.4873\n","Epoch [904/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5720, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6324, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 905\n","\n","Epoch [905/1000], Step [0/43], Loss: 1.4874\n","Epoch [905/1000], Step [20/43], Loss: 1.4873\n","Epoch [905/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5719, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6324, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 906\n","\n","Epoch [906/1000], Step [0/43], Loss: 1.4874\n","Epoch [906/1000], Step [20/43], Loss: 1.4873\n","Epoch [906/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5719, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6323, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 907\n","\n","Epoch [907/1000], Step [0/43], Loss: 1.4874\n","Epoch [907/1000], Step [20/43], Loss: 1.4873\n","Epoch [907/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5718, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6323, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 908\n","\n","Epoch [908/1000], Step [0/43], Loss: 1.4874\n","Epoch [908/1000], Step [20/43], Loss: 1.4873\n","Epoch [908/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5718, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6323, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 909\n","\n","Epoch [909/1000], Step [0/43], Loss: 1.4874\n","Epoch [909/1000], Step [20/43], Loss: 1.4873\n","Epoch [909/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5717, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6322, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 910\n","\n","Epoch [910/1000], Step [0/43], Loss: 1.4874\n","Epoch [910/1000], Step [20/43], Loss: 1.4873\n","Epoch [910/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5717, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6322, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 911\n","\n","Epoch [911/1000], Step [0/43], Loss: 1.4874\n","Epoch [911/1000], Step [20/43], Loss: 1.4873\n","Epoch [911/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5716, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6321, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 912\n","\n","Epoch [912/1000], Step [0/43], Loss: 1.4874\n","Epoch [912/1000], Step [20/43], Loss: 1.4873\n","Epoch [912/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5716, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6321, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 913\n","\n","Epoch [913/1000], Step [0/43], Loss: 1.4874\n","Epoch [913/1000], Step [20/43], Loss: 1.4873\n","Epoch [913/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5715, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6321, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 914\n","\n","Epoch [914/1000], Step [0/43], Loss: 1.4874\n","Epoch [914/1000], Step [20/43], Loss: 1.4873\n","Epoch [914/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5715, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6320, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 915\n","\n","Epoch [915/1000], Step [0/43], Loss: 1.4874\n","Epoch [915/1000], Step [20/43], Loss: 1.4873\n","Epoch [915/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5714, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6320, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 916\n","\n","Epoch [916/1000], Step [0/43], Loss: 1.4874\n","Epoch [916/1000], Step [20/43], Loss: 1.4874\n","Epoch [916/1000], Step [40/43], Loss: 1.6437\n","\n","train loss: 1.5714, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6319, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 917\n","\n","Epoch [917/1000], Step [0/43], Loss: 1.4874\n","Epoch [917/1000], Step [20/43], Loss: 1.4873\n","Epoch [917/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5714, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6319, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 918\n","\n","Epoch [918/1000], Step [0/43], Loss: 1.4874\n","Epoch [918/1000], Step [20/43], Loss: 1.4873\n","Epoch [918/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5713, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6318, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 919\n","\n","Epoch [919/1000], Step [0/43], Loss: 1.4874\n","Epoch [919/1000], Step [20/43], Loss: 1.4873\n","Epoch [919/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5713, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6318, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 920\n","\n","Epoch [920/1000], Step [0/43], Loss: 1.4874\n","Epoch [920/1000], Step [20/43], Loss: 1.4873\n","Epoch [920/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5712, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6318, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 921\n","\n","Epoch [921/1000], Step [0/43], Loss: 1.4874\n","Epoch [921/1000], Step [20/43], Loss: 1.4873\n","Epoch [921/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5712, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6317, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 922\n","\n","Epoch [922/1000], Step [0/43], Loss: 1.4874\n","Epoch [922/1000], Step [20/43], Loss: 1.4873\n","Epoch [922/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5711, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6317, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 923\n","\n","Epoch [923/1000], Step [0/43], Loss: 1.4874\n","Epoch [923/1000], Step [20/43], Loss: 1.4873\n","Epoch [923/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5711, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6316, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 924\n","\n","Epoch [924/1000], Step [0/43], Loss: 1.4874\n","Epoch [924/1000], Step [20/43], Loss: 1.4873\n","Epoch [924/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5710, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6316, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 925\n","\n","Epoch [925/1000], Step [0/43], Loss: 1.4874\n","Epoch [925/1000], Step [20/43], Loss: 1.4873\n","Epoch [925/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5710, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6316, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 926\n","\n","Epoch [926/1000], Step [0/43], Loss: 1.4874\n","Epoch [926/1000], Step [20/43], Loss: 1.4873\n","Epoch [926/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5709, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6315, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 927\n","\n","Epoch [927/1000], Step [0/43], Loss: 1.4874\n","Epoch [927/1000], Step [20/43], Loss: 1.4873\n","Epoch [927/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5709, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6315, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 928\n","\n","Epoch [928/1000], Step [0/43], Loss: 1.4874\n","Epoch [928/1000], Step [20/43], Loss: 1.4873\n","Epoch [928/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5708, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6314, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 929\n","\n","Epoch [929/1000], Step [0/43], Loss: 1.4874\n","Epoch [929/1000], Step [20/43], Loss: 1.4873\n","Epoch [929/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5708, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6314, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 930\n","\n","Epoch [930/1000], Step [0/43], Loss: 1.4874\n","Epoch [930/1000], Step [20/43], Loss: 1.4873\n","Epoch [930/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5707, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6314, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 931\n","\n","Epoch [931/1000], Step [0/43], Loss: 1.4874\n","Epoch [931/1000], Step [20/43], Loss: 1.4873\n","Epoch [931/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5707, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6313, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 932\n","\n","Epoch [932/1000], Step [0/43], Loss: 1.4874\n","Epoch [932/1000], Step [20/43], Loss: 1.4873\n","Epoch [932/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5707, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6313, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 933\n","\n","Epoch [933/1000], Step [0/43], Loss: 1.4874\n","Epoch [933/1000], Step [20/43], Loss: 1.4873\n","Epoch [933/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5706, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6312, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 934\n","\n","Epoch [934/1000], Step [0/43], Loss: 1.4874\n","Epoch [934/1000], Step [20/43], Loss: 1.4873\n","Epoch [934/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5706, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6312, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 935\n","\n","Epoch [935/1000], Step [0/43], Loss: 1.4874\n","Epoch [935/1000], Step [20/43], Loss: 1.4873\n","Epoch [935/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5705, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6312, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 936\n","\n","Epoch [936/1000], Step [0/43], Loss: 1.4874\n","Epoch [936/1000], Step [20/43], Loss: 1.4873\n","Epoch [936/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5705, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6311, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 937\n","\n","Epoch [937/1000], Step [0/43], Loss: 1.4874\n","Epoch [937/1000], Step [20/43], Loss: 1.4873\n","Epoch [937/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5704, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6311, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 938\n","\n","Epoch [938/1000], Step [0/43], Loss: 1.4874\n","Epoch [938/1000], Step [20/43], Loss: 1.4873\n","Epoch [938/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5704, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6310, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 939\n","\n","Epoch [939/1000], Step [0/43], Loss: 1.4874\n","Epoch [939/1000], Step [20/43], Loss: 1.4873\n","Epoch [939/1000], Step [40/43], Loss: 1.6440\n","\n","train loss: 1.5703, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6310, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 940\n","\n","Epoch [940/1000], Step [0/43], Loss: 1.4874\n","Epoch [940/1000], Step [20/43], Loss: 1.4874\n","Epoch [940/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5703, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6310, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 941\n","\n","Epoch [941/1000], Step [0/43], Loss: 1.4874\n","Epoch [941/1000], Step [20/43], Loss: 1.4873\n","Epoch [941/1000], Step [40/43], Loss: 1.6445\n","\n","train loss: 1.5702, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6309, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 942\n","\n","Epoch [942/1000], Step [0/43], Loss: 1.4874\n","Epoch [942/1000], Step [20/43], Loss: 1.4873\n","Epoch [942/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5702, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6309, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 943\n","\n","Epoch [943/1000], Step [0/43], Loss: 1.4874\n","Epoch [943/1000], Step [20/43], Loss: 1.4873\n","Epoch [943/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5702, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6308, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 944\n","\n","Epoch [944/1000], Step [0/43], Loss: 1.4874\n","Epoch [944/1000], Step [20/43], Loss: 1.4873\n","Epoch [944/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5701, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6308, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 945\n","\n","Epoch [945/1000], Step [0/43], Loss: 1.4874\n","Epoch [945/1000], Step [20/43], Loss: 1.4873\n","Epoch [945/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5701, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6308, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 946\n","\n","Epoch [946/1000], Step [0/43], Loss: 1.4874\n","Epoch [946/1000], Step [20/43], Loss: 1.4873\n","Epoch [946/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5700, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6307, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 947\n","\n","Epoch [947/1000], Step [0/43], Loss: 1.4874\n","Epoch [947/1000], Step [20/43], Loss: 1.4873\n","Epoch [947/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5700, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6307, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 948\n","\n","Epoch [948/1000], Step [0/43], Loss: 1.4874\n","Epoch [948/1000], Step [20/43], Loss: 1.4873\n","Epoch [948/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5699, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6307, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 949\n","\n","Epoch [949/1000], Step [0/43], Loss: 1.4874\n","Epoch [949/1000], Step [20/43], Loss: 1.4873\n","Epoch [949/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5699, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6306, validation acc1: 93.8250, validation acc2: 93.4820\n","\n","Epoch 950\n","\n","Epoch [950/1000], Step [0/43], Loss: 1.4874\n","Epoch [950/1000], Step [20/43], Loss: 1.4873\n","Epoch [950/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5698, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6306, validation acc1: 93.4820, validation acc2: 93.3105\n","\n","Epoch 951\n","\n","Epoch [951/1000], Step [0/43], Loss: 1.4874\n","Epoch [951/1000], Step [20/43], Loss: 1.4873\n","Epoch [951/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5698, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6305, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 952\n","\n","Epoch [952/1000], Step [0/43], Loss: 1.4874\n","Epoch [952/1000], Step [20/43], Loss: 1.4873\n","Epoch [952/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5698, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6305, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 953\n","\n","Epoch [953/1000], Step [0/43], Loss: 1.4874\n","Epoch [953/1000], Step [20/43], Loss: 1.4873\n","Epoch [953/1000], Step [40/43], Loss: 1.6439\n","\n","train loss: 1.5697, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6305, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 954\n","\n","Epoch [954/1000], Step [0/43], Loss: 1.4874\n","Epoch [954/1000], Step [20/43], Loss: 1.4873\n","Epoch [954/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5697, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6304, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 955\n","\n","Epoch [955/1000], Step [0/43], Loss: 1.4874\n","Epoch [955/1000], Step [20/43], Loss: 1.4873\n","Epoch [955/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5696, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6304, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 956\n","\n","Epoch [956/1000], Step [0/43], Loss: 1.4874\n","Epoch [956/1000], Step [20/43], Loss: 1.4873\n","Epoch [956/1000], Step [40/43], Loss: 1.6443\n","\n","train loss: 1.5696, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6303, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 957\n","\n","Epoch [957/1000], Step [0/43], Loss: 1.4874\n","Epoch [957/1000], Step [20/43], Loss: 1.4873\n","Epoch [957/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5695, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6303, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 958\n","\n","Epoch [958/1000], Step [0/43], Loss: 1.4874\n","Epoch [958/1000], Step [20/43], Loss: 1.4873\n","Epoch [958/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5695, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6303, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 959\n","\n","Epoch [959/1000], Step [0/43], Loss: 1.4874\n","Epoch [959/1000], Step [20/43], Loss: 1.4873\n","Epoch [959/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5695, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6302, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 960\n","\n","Epoch [960/1000], Step [0/43], Loss: 1.4874\n","Epoch [960/1000], Step [20/43], Loss: 1.4873\n","Epoch [960/1000], Step [40/43], Loss: 1.6444\n","\n","train loss: 1.5694, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6302, validation acc1: 93.4820, validation acc2: 93.1389\n","\n","Epoch 961\n","\n","Epoch [961/1000], Step [0/43], Loss: 1.4874\n","Epoch [961/1000], Step [20/43], Loss: 1.4873\n","Epoch [961/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5694, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6302, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 962\n","\n","Epoch [962/1000], Step [0/43], Loss: 1.4874\n","Epoch [962/1000], Step [20/43], Loss: 1.4873\n","Epoch [962/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5693, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6301, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 963\n","\n","Epoch [963/1000], Step [0/43], Loss: 1.4874\n","Epoch [963/1000], Step [20/43], Loss: 1.4873\n","Epoch [963/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5693, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6301, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 964\n","\n","Epoch [964/1000], Step [0/43], Loss: 1.4874\n","Epoch [964/1000], Step [20/43], Loss: 1.4873\n","Epoch [964/1000], Step [40/43], Loss: 1.6438\n","\n","train loss: 1.5692, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6300, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 965\n","\n","Epoch [965/1000], Step [0/43], Loss: 1.4874\n","Epoch [965/1000], Step [20/43], Loss: 1.4873\n","Epoch [965/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5692, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6300, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 966\n","\n","Epoch [966/1000], Step [0/43], Loss: 1.4874\n","Epoch [966/1000], Step [20/43], Loss: 1.4873\n","Epoch [966/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5692, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6300, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 967\n","\n","Epoch [967/1000], Step [0/43], Loss: 1.4874\n","Epoch [967/1000], Step [20/43], Loss: 1.4873\n","Epoch [967/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5691, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6299, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 968\n","\n","Epoch [968/1000], Step [0/43], Loss: 1.4874\n","Epoch [968/1000], Step [20/43], Loss: 1.4873\n","Epoch [968/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5691, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6299, validation acc1: 93.8250, validation acc2: 93.1389\n","\n","Epoch 969\n","\n","Epoch [969/1000], Step [0/43], Loss: 1.4874\n","Epoch [969/1000], Step [20/43], Loss: 1.4873\n","Epoch [969/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5690, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6299, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 970\n","\n","Epoch [970/1000], Step [0/43], Loss: 1.4874\n","Epoch [970/1000], Step [20/43], Loss: 1.4873\n","Epoch [970/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5690, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6298, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 971\n","\n","Epoch [971/1000], Step [0/43], Loss: 1.4874\n","Epoch [971/1000], Step [20/43], Loss: 1.4873\n","Epoch [971/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5689, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6298, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 972\n","\n","Epoch [972/1000], Step [0/43], Loss: 1.4874\n","Epoch [972/1000], Step [20/43], Loss: 1.4873\n","Epoch [972/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5689, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6298, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 973\n","\n","Epoch [973/1000], Step [0/43], Loss: 1.4874\n","Epoch [973/1000], Step [20/43], Loss: 1.4873\n","Epoch [973/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5689, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6297, validation acc1: 93.8250, validation acc2: 93.3105\n","\n","Epoch 974\n","\n","Epoch [974/1000], Step [0/43], Loss: 1.4874\n","Epoch [974/1000], Step [20/43], Loss: 1.4873\n","Epoch [974/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5688, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6297, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 975\n","\n","Epoch [975/1000], Step [0/43], Loss: 1.4874\n","Epoch [975/1000], Step [20/43], Loss: 1.4873\n","Epoch [975/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5688, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6296, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 976\n","\n","Epoch [976/1000], Step [0/43], Loss: 1.4874\n","Epoch [976/1000], Step [20/43], Loss: 1.4873\n","Epoch [976/1000], Step [40/43], Loss: 1.6433\n","\n","train loss: 1.5687, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6296, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 977\n","\n","Epoch [977/1000], Step [0/43], Loss: 1.4874\n","Epoch [977/1000], Step [20/43], Loss: 1.4873\n","Epoch [977/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5687, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6296, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 978\n","\n","Epoch [978/1000], Step [0/43], Loss: 1.4874\n","Epoch [978/1000], Step [20/43], Loss: 1.4873\n","Epoch [978/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5686, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6295, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 979\n","\n","Epoch [979/1000], Step [0/43], Loss: 1.4874\n","Epoch [979/1000], Step [20/43], Loss: 1.4873\n","Epoch [979/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5686, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6295, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 980\n","\n","Epoch [980/1000], Step [0/43], Loss: 1.4874\n","Epoch [980/1000], Step [20/43], Loss: 1.4873\n","Epoch [980/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5686, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6295, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 981\n","\n","Epoch [981/1000], Step [0/43], Loss: 1.4874\n","Epoch [981/1000], Step [20/43], Loss: 1.4873\n","Epoch [981/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5685, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6294, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 982\n","\n","Epoch [982/1000], Step [0/43], Loss: 1.4874\n","Epoch [982/1000], Step [20/43], Loss: 1.4873\n","Epoch [982/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5685, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6294, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 983\n","\n","Epoch [983/1000], Step [0/43], Loss: 1.4874\n","Epoch [983/1000], Step [20/43], Loss: 1.4873\n","Epoch [983/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5684, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6294, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 984\n","\n","Epoch [984/1000], Step [0/43], Loss: 1.4874\n","Epoch [984/1000], Step [20/43], Loss: 1.4873\n","Epoch [984/1000], Step [40/43], Loss: 1.6432\n","\n","train loss: 1.5684, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6293, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 985\n","\n","Epoch [985/1000], Step [0/43], Loss: 1.4874\n","Epoch [985/1000], Step [20/43], Loss: 1.4873\n","Epoch [985/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5684, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6293, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 986\n","\n","Epoch [986/1000], Step [0/43], Loss: 1.4874\n","Epoch [986/1000], Step [20/43], Loss: 1.4873\n","Epoch [986/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5683, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6293, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 987\n","\n","Epoch [987/1000], Step [0/43], Loss: 1.4874\n","Epoch [987/1000], Step [20/43], Loss: 1.4873\n","Epoch [987/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5683, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6292, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 988\n","\n","Epoch [988/1000], Step [0/43], Loss: 1.4874\n","Epoch [988/1000], Step [20/43], Loss: 1.4873\n","Epoch [988/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5682, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6292, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 989\n","\n","Epoch [989/1000], Step [0/43], Loss: 1.4874\n","Epoch [989/1000], Step [20/43], Loss: 1.4873\n","Epoch [989/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5682, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6291, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 990\n","\n","Epoch [990/1000], Step [0/43], Loss: 1.4874\n","Epoch [990/1000], Step [20/43], Loss: 1.4873\n","Epoch [990/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5682, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6291, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 991\n","\n","Epoch [991/1000], Step [0/43], Loss: 1.4874\n","Epoch [991/1000], Step [20/43], Loss: 1.4873\n","Epoch [991/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5681, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6291, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 992\n","\n","Epoch [992/1000], Step [0/43], Loss: 1.4874\n","Epoch [992/1000], Step [20/43], Loss: 1.4873\n","Epoch [992/1000], Step [40/43], Loss: 1.6436\n","\n","train loss: 1.5681, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6290, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 993\n","\n","Epoch [993/1000], Step [0/43], Loss: 1.4874\n","Epoch [993/1000], Step [20/43], Loss: 1.4873\n","Epoch [993/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5680, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6290, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 994\n","\n","Epoch [994/1000], Step [0/43], Loss: 1.4874\n","Epoch [994/1000], Step [20/43], Loss: 1.4873\n","Epoch [994/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5680, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6290, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 995\n","\n","Epoch [995/1000], Step [0/43], Loss: 1.4874\n","Epoch [995/1000], Step [20/43], Loss: 1.4873\n","Epoch [995/1000], Step [40/43], Loss: 1.6435\n","\n","train loss: 1.5680, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6289, validation acc1: 93.6535, validation acc2: 93.4820\n","\n","Epoch 996\n","\n","Epoch [996/1000], Step [0/43], Loss: 1.4874\n","Epoch [996/1000], Step [20/43], Loss: 1.4873\n","Epoch [996/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5679, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6289, validation acc1: 93.6535, validation acc2: 93.1389\n","\n","Epoch 997\n","\n","Epoch [997/1000], Step [0/43], Loss: 1.4874\n","Epoch [997/1000], Step [20/43], Loss: 1.4873\n","Epoch [997/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5679, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6289, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 998\n","\n","Epoch [998/1000], Step [0/43], Loss: 1.4874\n","Epoch [998/1000], Step [20/43], Loss: 1.4873\n","Epoch [998/1000], Step [40/43], Loss: 1.6434\n","\n","train loss: 1.5678, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6288, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 999\n","\n","Epoch [999/1000], Step [0/43], Loss: 1.4874\n","Epoch [999/1000], Step [20/43], Loss: 1.4873\n","Epoch [999/1000], Step [40/43], Loss: 1.6430\n","\n","train loss: 1.5678, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6288, validation acc1: 93.6535, validation acc2: 93.3105\n","\n","Epoch 1000\n","\n","Epoch [1000/1000], Step [0/43], Loss: 1.4874\n","Epoch [1000/1000], Step [20/43], Loss: 1.4873\n","Epoch [1000/1000], Step [40/43], Loss: 1.6431\n","\n","train loss: 1.5678, train acc1: 98.0854, train acc2: 96.6127\n","validation loss: 1.6288, validation acc1: 93.6535, validation acc2: 93.1389\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fdize3iB84bi"},"source":["# Testing\n"]},{"cell_type":"code","metadata":{"id":"vlgztFTt8pzS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606063264708,"user_tz":300,"elapsed":3020,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"8bdb3699-83ee-443c-8bc2-a5a6900fb824"},"source":["test_acc1 = []\n","test_acc2 = []\n","\n","total_t1=0\n","total_t2=0\n","\n","correct_t1=0\n","correct_t2=0\n","\n","allpreds1 = torch.rand(0)\n","allpreds2 = torch.rand(0)\n","val_allpreds1 = torch.rand(0)\n","val_allpreds2 = torch.rand(0)\n","\n","allpreds1 = allpreds1.to(device)\n","allpreds2 = allpreds2.to(device)\n","val_allpreds1 = val_allpreds1.to(device)\n","val_allpreds2 = val_allpreds2.to(device)\n","\n","alltgs1 = torch.rand(0)\n","alltgs2 = torch.rand(0)\n","alltgs1 = alltgs1.to(device)\n","alltgs2 = alltgs2.to(device)\n","\n","\n","# ## load model state\n","checkpoint = torch.load(\"model_classification_tutorial22.pt\") #model_classification_tutorial23.pt\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","epoch = checkpoint['epoch']\n","valid_loss_min = checkpoint['loss']\n","\n","\n","\n","with torch.no_grad():\n","    model.eval()\n","    for data_t, target_t in (test_loader):\n","        data_t, target_t = data_t.to(device), target_t.to(device)# on GPU\n","        outputs_t1, outputs_t2 = model(data_t)\n","\n","        val_pred_t1,pred_t1 = torch.max(outputs_t1, dim=1)\n","        val_pred_t2,pred_t2 = torch.max(outputs_t2, dim=1)\n","\n","        alltgs1 = torch.cat((alltgs1,target_t[:,0]),dim=0)\n","        alltgs2 = torch.cat((alltgs2,target_t[:,1]),dim=0)\n","        \n","        allpreds1 = torch.cat((allpreds1,pred_t1),dim=0)\n","        allpreds2 = torch.cat((allpreds2,pred_t2),dim=0)        \n","        val_allpreds1 = torch.cat((val_allpreds1,val_pred_t1),dim=0)\n","        val_allpreds2 = torch.cat((val_allpreds2,val_pred_t2),dim=0)\n","\n","\n","        correct_t1 += torch.sum(pred_t1==target_t[:,0]).item()\n","        correct_t2 += torch.sum(pred_t2==target_t[:,1]).item()\n","        total_t1 += target_t[:,0].size(0)\n","        total_t2 += target_t[:,1].size(0)\n","    test_acc1.append(100 * correct_t1 / total_t1)\n","    test_acc2.append(100 * correct_t2 / total_t2)\n","\n","    print(f'test acc1: {(100 * correct_t1 / total_t1):.4f}, test acc2: {(100 * correct_t2 / total_t2):.4f}\\n') #, test acc3: {(100 * correct_t3 / total_t3):.4f}\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test acc1: 80.0983, test acc2: 84.7666\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueQ104-cgL1z","executionInfo":{"status":"ok","timestamp":1606058233306,"user_tz":300,"elapsed":450,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"1e136120-7075-48a6-c379-c6ace7378ef1"},"source":["### confusion matrix endeffecttor location\n","y_act = alltgs1.cpu().detach().numpy() \n","y_pred = allpreds1.cpu().detach().numpy()\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_act, y_pred, labels=[0, 1, 2, 3]))\n","# Printing the precision and recall, among other metrics\n","print(metrics.classification_report(y_act, y_pred, labels=[0, 1, 2, 3]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[126   0   3   1]\n"," [  0  50   0   1]\n"," [  8   0  52   0]\n"," [ 59   1   8  98]]\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.97      0.78       130\n","           1       0.98      0.98      0.98        51\n","           2       0.83      0.87      0.85        60\n","           3       0.98      0.59      0.74       166\n","\n","    accuracy                           0.80       407\n","   macro avg       0.86      0.85      0.84       407\n","weighted avg       0.85      0.80      0.80       407\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RcSXgqepc8T","executionInfo":{"status":"ok","timestamp":1606060213330,"user_tz":300,"elapsed":357,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"97af5159-31d9-48a5-c7e4-30ae194b7406"},"source":["### confusion matrix onion location\n","y_act = alltgs2.cpu().detach().numpy() \n","y_pred = allpreds2.cpu().detach().numpy()\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_act, y_pred, labels=[0, 1, 2, 3]))\n","# Printing the precision and recall, among other metrics\n","print(metrics.classification_report(y_act, y_pred, labels=[0, 1, 2, 3]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[170   0   9   5]\n"," [  0  51   0   0]\n"," [  8   0  51   1]\n"," [ 30   6   3  73]]\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.92      0.87       184\n","           1       0.89      1.00      0.94        51\n","           2       0.81      0.85      0.83        60\n","           3       0.92      0.65      0.76       112\n","\n","    accuracy                           0.85       407\n","   macro avg       0.86      0.86      0.85       407\n","weighted avg       0.86      0.85      0.84       407\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6v08mPKWiPJC"},"source":["# For saving the model\n","# state = {'epoch': 500, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'loss': valid_loss_min,  }\n","# torch.save(state, 'model_classification_tutorial22.pt') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mW-jTytnNThj"},"source":["# Saving video"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qDIyG0ijeLX","executionInfo":{"status":"ok","timestamp":1606065108036,"user_tz":300,"elapsed":4658,"user":{"displayName":"Farah Saeed","photoUrl":"","userId":"07441351312168922724"}},"outputId":"48fd3627-94e4-473f-b4ab-445aed708d88"},"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","\n","font                   = cv2.FONT_HERSHEY_SIMPLEX\n","bottomLeftCornerOfText = (50,100)\n","fontScale              = 0.5\n","fontColor              = (0,0,255)\n","lineType               = 2\n","\n","\n","def img_display(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    npimg = np.transpose(npimg, (1, 2, 0))\n","    return npimg\n","ee_loc_vals = {0.0: 'conveyor', 1.0: 'eye', 2.0:'bin', 3.0:'hover'}\n","o_loc_vals = {0.0: 'conveyor', 1.0: 'eye', 2.0:'bin', 3.0:'hover'}\n","\n","eepreds = allpreds1.cpu().detach().numpy()\n","opreds = allpreds2.cpu().detach().numpy()\n","conf_eepreds = val_allpreds1.cpu().detach().numpy()\n","conf_opreds = val_allpreds2.cpu().detach().numpy()\n","\n","eevals = alltgs1.cpu().detach().numpy()\n","ovals = alltgs2.cpu().detach().numpy()\n","\n","out = cv2.VideoWriter('project52.avi',cv2.VideoWriter_fourcc(*'DIVX'), 3, (640,480))\n","\n","\n","k = 0\n","for data_t, target_t in (test_loader):\n","    for i in range(len(data_t)):\n","        print('k',k)\n","        # if k == 0:\n","        img = img_display(data_t[i,:,:,:])[:, :, ::-1]*255\n","        img = img.astype(np.uint8)\n","        is_ee = (eepreds[k] == eevals[k]) \n","        is_o = (opreds[k] == ovals[k]) \n","        tag = 'Frame: '+str(k)+' EE loc: '+ee_loc_vals[eepreds[k]]+' '+str(conf_eepreds[k])+' Onion loc: '+o_loc_vals[opreds[k]]+' '+str(conf_opreds[k])+'\\n True EE loc: '+ee_loc_vals[eevals[k]]+' True O loc: '+o_loc_vals[ovals[k]]\n","        tag1 = 'True EE loc: '+ee_loc_vals[eevals[k]]+' True O loc: '+o_loc_vals[ovals[k]]\n","        print(tag)\n","        print('ee:', is_ee, ' o:',is_o)\n","        img1 = img.copy()\n","        cv2.putText(img1, tag, \n","            bottomLeftCornerOfText, \n","            font, \n","            fontScale,\n","            fontColor,\n","            lineType)\n","        cv2.putText(img1, tag1, \n","            (50,150), \n","            font, \n","            fontScale,\n","            fontColor,\n","            lineType)\n","        out.write(img1)\n","        k+=1\n","out.release()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["k 0\n","Frame: 0 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 1\n","Frame: 1 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 2\n","Frame: 2 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 3\n","Frame: 3 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 4\n","Frame: 4 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 5\n","Frame: 5 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 6\n","Frame: 6 EE loc: conveyor 0.93349725 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 7\n","Frame: 7 EE loc: conveyor 0.99182194 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 8\n","Frame: 8 EE loc: conveyor 0.91696835 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 9\n","Frame: 9 EE loc: conveyor 0.7494214 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 10\n","Frame: 10 EE loc: hover 0.91197085 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: False  o: True\n","k 11\n","Frame: 11 EE loc: hover 1.0 Onion loc: conveyor 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 12\n","Frame: 12 EE loc: hover 1.0 Onion loc: hover 0.98443013\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 13\n","Frame: 13 EE loc: hover 1.0 Onion loc: conveyor 0.9217575\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 14\n","Frame: 14 EE loc: hover 1.0 Onion loc: conveyor 0.98342633\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 15\n","Frame: 15 EE loc: hover 1.0 Onion loc: hover 0.86267346\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 16\n","Frame: 16 EE loc: hover 0.9999676 Onion loc: eye 0.67779696\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 17\n","Frame: 17 EE loc: hover 0.9956981 Onion loc: eye 0.99651504\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 18\n","Frame: 18 EE loc: hover 0.99029857 Onion loc: eye 0.99817014\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 19\n","Frame: 19 EE loc: hover 0.999329 Onion loc: eye 0.92711204\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 20\n","Frame: 20 EE loc: hover 0.978454 Onion loc: eye 0.98681855\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 21\n","Frame: 21 EE loc: hover 0.7446651 Onion loc: eye 0.99149466\n"," True EE loc: eye True O loc: eye\n","ee: False  o: True\n","k 22\n","Frame: 22 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 23\n","Frame: 23 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 24\n","Frame: 24 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 25\n","Frame: 25 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 26\n","Frame: 26 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 27\n","Frame: 27 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 28\n","Frame: 28 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 29\n","Frame: 29 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 30\n","Frame: 30 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 31\n","Frame: 31 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 32\n","Frame: 32 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 33\n","Frame: 33 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 34\n","Frame: 34 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 35\n","Frame: 35 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 36\n","Frame: 36 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 37\n","Frame: 37 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 38\n","Frame: 38 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 39\n","Frame: 39 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 40\n","Frame: 40 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 41\n","Frame: 41 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 42\n","Frame: 42 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 43\n","Frame: 43 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 44\n","Frame: 44 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 45\n","Frame: 45 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 46\n","Frame: 46 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 47\n","Frame: 47 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 48\n","Frame: 48 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 49\n","Frame: 49 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 50\n","Frame: 50 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 51\n","Frame: 51 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 52\n","Frame: 52 EE loc: hover 0.77283835 Onion loc: conveyor 0.9999831\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 53\n","Frame: 53 EE loc: hover 0.9999174 Onion loc: hover 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 54\n","Frame: 54 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 55\n","Frame: 55 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 56\n","Frame: 56 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 57\n","Frame: 57 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 58\n","Frame: 58 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 59\n","Frame: 59 EE loc: bin 0.9993032 Onion loc: bin 0.97258025\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 60\n","Frame: 60 EE loc: hover 0.9999994 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 61\n","Frame: 61 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 62\n","Frame: 62 EE loc: hover 0.9943218 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 63\n","Frame: 63 EE loc: hover 1.0 Onion loc: conveyor 0.99630606\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 64\n","Frame: 64 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 65\n","Frame: 65 EE loc: hover 0.7671901 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 66\n","Frame: 66 EE loc: hover 0.9993747 Onion loc: conveyor 0.99999774\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 67\n","Frame: 67 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 68\n","Frame: 68 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 69\n","Frame: 69 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 70\n","Frame: 70 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 71\n","Frame: 71 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 72\n","Frame: 72 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 73\n","Frame: 73 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 74\n","Frame: 74 EE loc: conveyor 0.99999607 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 75\n","Frame: 75 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 76\n","Frame: 76 EE loc: hover 1.0 Onion loc: hover 0.9999995\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 77\n","Frame: 77 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 78\n","Frame: 78 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 79\n","Frame: 79 EE loc: hover 1.0 Onion loc: hover 0.9999974\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 80\n","Frame: 80 EE loc: hover 0.99945134 Onion loc: hover 0.9981693\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 81\n","Frame: 81 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 82\n","Frame: 82 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 83\n","Frame: 83 EE loc: eye 0.99999666 Onion loc: eye 0.9999969\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 84\n","Frame: 84 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 85\n","Frame: 85 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 86\n","Frame: 86 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 87\n","Frame: 87 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 88\n","Frame: 88 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 89\n","Frame: 89 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 90\n","Frame: 90 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 91\n","Frame: 91 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 92\n","Frame: 92 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 93\n","Frame: 93 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 94\n","Frame: 94 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 95\n","Frame: 95 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 96\n","Frame: 96 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 97\n","Frame: 97 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 98\n","Frame: 98 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 99\n","Frame: 99 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 100\n","Frame: 100 EE loc: eye 1.0 Onion loc: eye 1.0\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 101\n","Frame: 101 EE loc: eye 0.9994313 Onion loc: eye 0.9973279\n"," True EE loc: eye True O loc: eye\n","ee: True  o: True\n","k 102\n","Frame: 102 EE loc: hover 0.9180977 Onion loc: hover 0.97686464\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 103\n","Frame: 103 EE loc: hover 0.9995547 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 104\n","Frame: 104 EE loc: hover 0.9999999 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 105\n","Frame: 105 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 106\n","Frame: 106 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 107\n","Frame: 107 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 108\n","Frame: 108 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 109\n","Frame: 109 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 110\n","Frame: 110 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 111\n","Frame: 111 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 112\n","Frame: 112 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 113\n","Frame: 113 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 114\n","Frame: 114 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 115\n","Frame: 115 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 116\n","Frame: 116 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 117\n","Frame: 117 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 118\n","Frame: 118 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 119\n","Frame: 119 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 120\n","Frame: 120 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 121\n","Frame: 121 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 122\n","Frame: 122 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 123\n","Frame: 123 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 124\n","Frame: 124 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 125\n","Frame: 125 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 126\n","Frame: 126 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 127\n","Frame: 127 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 128\n","Frame: 128 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 129\n","Frame: 129 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 130\n","Frame: 130 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 131\n","Frame: 131 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 132\n","Frame: 132 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 133\n","Frame: 133 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 134\n","Frame: 134 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 135\n","Frame: 135 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 136\n","Frame: 136 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 137\n","Frame: 137 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 138\n","Frame: 138 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 139\n","Frame: 139 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 140\n","Frame: 140 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 141\n","Frame: 141 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 142\n","Frame: 142 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 143\n","Frame: 143 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 144\n","Frame: 144 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 145\n","Frame: 145 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 146\n","Frame: 146 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 147\n","Frame: 147 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 148\n","Frame: 148 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 149\n","Frame: 149 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 150\n","Frame: 150 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 151\n","Frame: 151 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 152\n","Frame: 152 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 153\n","Frame: 153 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 154\n","Frame: 154 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 155\n","Frame: 155 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 156\n","Frame: 156 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 157\n","Frame: 157 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 158\n","Frame: 158 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 159\n","Frame: 159 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 160\n","Frame: 160 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 161\n","Frame: 161 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 162\n","Frame: 162 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 163\n","Frame: 163 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 164\n","Frame: 164 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 165\n","Frame: 165 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 166\n","Frame: 166 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 167\n","Frame: 167 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 168\n","Frame: 168 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 169\n","Frame: 169 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 170\n","Frame: 170 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 171\n","Frame: 171 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 172\n","Frame: 172 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 173\n","Frame: 173 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 174\n","Frame: 174 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 175\n","Frame: 175 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 176\n","Frame: 176 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 177\n","Frame: 177 EE loc: conveyor 0.99991834 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 178\n","Frame: 178 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 179\n","Frame: 179 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 180\n","Frame: 180 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 181\n","Frame: 181 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 182\n","Frame: 182 EE loc: hover 0.9999895 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 183\n","Frame: 183 EE loc: hover 0.99976486 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 184\n","Frame: 184 EE loc: hover 0.99989665 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 185\n","Frame: 185 EE loc: conveyor 0.98649615 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 186\n","Frame: 186 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 187\n","Frame: 187 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 188\n","Frame: 188 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 189\n","Frame: 189 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 190\n","Frame: 190 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 191\n","Frame: 191 EE loc: hover 0.98359424 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 192\n","Frame: 192 EE loc: conveyor 0.99999785 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 193\n","Frame: 193 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 194\n","Frame: 194 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 195\n","Frame: 195 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 196\n","Frame: 196 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 197\n","Frame: 197 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 198\n","Frame: 198 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 199\n","Frame: 199 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 200\n","Frame: 200 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 201\n","Frame: 201 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 202\n","Frame: 202 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 203\n","Frame: 203 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 204\n","Frame: 204 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 205\n","Frame: 205 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 206\n","Frame: 206 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 207\n","Frame: 207 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 208\n","Frame: 208 EE loc: hover 0.52487224 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 209\n","Frame: 209 EE loc: hover 1.0 Onion loc: conveyor 0.9999745\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 210\n","Frame: 210 EE loc: hover 0.9993662 Onion loc: conveyor 0.99999154\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 211\n","Frame: 211 EE loc: conveyor 0.9964904 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 212\n","Frame: 212 EE loc: hover 0.9999925 Onion loc: conveyor 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 213\n","Frame: 213 EE loc: hover 0.9468036 Onion loc: conveyor 0.9999982\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 214\n","Frame: 214 EE loc: conveyor 0.6769512 Onion loc: conveyor 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 215\n","Frame: 215 EE loc: conveyor 0.8618266 Onion loc: conveyor 0.9999927\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 216\n","Frame: 216 EE loc: hover 0.955726 Onion loc: conveyor 0.99952495\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 217\n","Frame: 217 EE loc: hover 0.99994683 Onion loc: conveyor 0.97672254\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 218\n","Frame: 218 EE loc: hover 0.9149921 Onion loc: conveyor 0.9996183\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 219\n","Frame: 219 EE loc: hover 0.99414045 Onion loc: conveyor 0.9998117\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 220\n","Frame: 220 EE loc: hover 0.55016404 Onion loc: conveyor 0.9999888\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 221\n","Frame: 221 EE loc: conveyor 0.99703014 Onion loc: conveyor 0.9999205\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 222\n","Frame: 222 EE loc: conveyor 0.99999905 Onion loc: conveyor 0.9993247\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 223\n","Frame: 223 EE loc: conveyor 0.9999267 Onion loc: hover 0.71974003\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 224\n","Frame: 224 EE loc: conveyor 1.0 Onion loc: conveyor 0.9999689\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 225\n","Frame: 225 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 226\n","Frame: 226 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 227\n","Frame: 227 EE loc: conveyor 1.0 Onion loc: conveyor 0.99889153\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 228\n","Frame: 228 EE loc: conveyor 0.99832183 Onion loc: conveyor 0.8291486\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 229\n","Frame: 229 EE loc: hover 0.9996025 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 230\n","Frame: 230 EE loc: conveyor 0.9996892 Onion loc: conveyor 0.9993844\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 231\n","Frame: 231 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 232\n","Frame: 232 EE loc: conveyor 0.99946505 Onion loc: hover 0.9966396\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 233\n","Frame: 233 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 234\n","Frame: 234 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 235\n","Frame: 235 EE loc: conveyor 0.9998543 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 236\n","Frame: 236 EE loc: bin 1.0 Onion loc: bin 0.99999905\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 237\n","Frame: 237 EE loc: bin 0.9984504 Onion loc: conveyor 0.8364784\n"," True EE loc: bin True O loc: bin\n","ee: True  o: False\n","k 238\n","Frame: 238 EE loc: conveyor 0.9479523 Onion loc: conveyor 0.99999833\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 239\n","Frame: 239 EE loc: conveyor 0.9176338 Onion loc: conveyor 0.99899274\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 240\n","Frame: 240 EE loc: conveyor 0.9863055 Onion loc: conveyor 0.99802953\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 241\n","Frame: 241 EE loc: conveyor 0.99762064 Onion loc: conveyor 0.92472965\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 242\n","Frame: 242 EE loc: conveyor 0.9981529 Onion loc: bin 0.65583813\n"," True EE loc: bin True O loc: bin\n","ee: False  o: True\n","k 243\n","Frame: 243 EE loc: conveyor 0.999861 Onion loc: bin 0.6617427\n"," True EE loc: bin True O loc: bin\n","ee: False  o: True\n","k 244\n","Frame: 244 EE loc: conveyor 0.9997371 Onion loc: conveyor 0.67583644\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 245\n","Frame: 245 EE loc: conveyor 0.9994814 Onion loc: conveyor 0.89821225\n"," True EE loc: bin True O loc: bin\n","ee: False  o: False\n","k 246\n","Frame: 246 EE loc: bin 0.9661086 Onion loc: conveyor 0.84305155\n"," True EE loc: bin True O loc: bin\n","ee: True  o: False\n","k 247\n","Frame: 247 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 248\n","Frame: 248 EE loc: hover 0.997509 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 249\n","Frame: 249 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 250\n","Frame: 250 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 251\n","Frame: 251 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 252\n","Frame: 252 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 253\n","Frame: 253 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 254\n","Frame: 254 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 255\n","Frame: 255 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 256\n","Frame: 256 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 257\n","Frame: 257 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 258\n","Frame: 258 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 259\n","Frame: 259 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 260\n","Frame: 260 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 261\n","Frame: 261 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 262\n","Frame: 262 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 263\n","Frame: 263 EE loc: conveyor 1.0 Onion loc: conveyor 0.99673235\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 264\n","Frame: 264 EE loc: hover 0.9999995 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 265\n","Frame: 265 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 266\n","Frame: 266 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 267\n","Frame: 267 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 268\n","Frame: 268 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 269\n","Frame: 269 EE loc: conveyor 0.99983275 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 270\n","Frame: 270 EE loc: conveyor 0.99076885 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 271\n","Frame: 271 EE loc: hover 0.9999974 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 272\n","Frame: 272 EE loc: hover 0.8804745 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 273\n","Frame: 273 EE loc: hover 0.9997694 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 274\n","Frame: 274 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 275\n","Frame: 275 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 276\n","Frame: 276 EE loc: hover 0.9996655 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 277\n","Frame: 277 EE loc: conveyor 0.8355546 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 278\n","Frame: 278 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 279\n","Frame: 279 EE loc: hover 0.9976987 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 280\n","Frame: 280 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 281\n","Frame: 281 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 282\n","Frame: 282 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 283\n","Frame: 283 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 284\n","Frame: 284 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 285\n","Frame: 285 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 286\n","Frame: 286 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 287\n","Frame: 287 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 288\n","Frame: 288 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 289\n","Frame: 289 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 290\n","Frame: 290 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 291\n","Frame: 291 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 292\n","Frame: 292 EE loc: bin 0.99993443 Onion loc: bin 0.99999905\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: False\n","k 293\n","Frame: 293 EE loc: conveyor 1.0 Onion loc: conveyor 0.99999964\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 294\n","Frame: 294 EE loc: hover 1.0 Onion loc: conveyor 0.9999994\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 295\n","Frame: 295 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 296\n","Frame: 296 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 297\n","Frame: 297 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 298\n","Frame: 298 EE loc: conveyor 0.9997749 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 299\n","Frame: 299 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 300\n","Frame: 300 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 301\n","Frame: 301 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 302\n","Frame: 302 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 303\n","Frame: 303 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 304\n","Frame: 304 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 305\n","Frame: 305 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 306\n","Frame: 306 EE loc: hover 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 307\n","Frame: 307 EE loc: hover 1.0 Onion loc: hover 0.98709434\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 308\n","Frame: 308 EE loc: hover 0.9999912 Onion loc: hover 0.9999964\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 309\n","Frame: 309 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 310\n","Frame: 310 EE loc: hover 0.92190385 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 311\n","Frame: 311 EE loc: hover 0.9057625 Onion loc: hover 0.9999913\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 312\n","Frame: 312 EE loc: hover 1.0 Onion loc: hover 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 313\n","Frame: 313 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 314\n","Frame: 314 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 315\n","Frame: 315 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 316\n","Frame: 316 EE loc: hover 0.9999987 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 317\n","Frame: 317 EE loc: hover 0.99994576 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 318\n","Frame: 318 EE loc: bin 0.9994917 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 319\n","Frame: 319 EE loc: hover 0.9985555 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 320\n","Frame: 320 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 321\n","Frame: 321 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 322\n","Frame: 322 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 323\n","Frame: 323 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 324\n","Frame: 324 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 325\n","Frame: 325 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 326\n","Frame: 326 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 327\n","Frame: 327 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 328\n","Frame: 328 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 329\n","Frame: 329 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 330\n","Frame: 330 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 331\n","Frame: 331 EE loc: bin 0.99999213 Onion loc: bin 0.97998995\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: False\n","k 332\n","Frame: 332 EE loc: conveyor 0.99578565 Onion loc: hover 0.86110586\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: False\n","k 333\n","Frame: 333 EE loc: hover 0.9999927 Onion loc: hover 0.71532303\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: False\n","k 334\n","Frame: 334 EE loc: hover 1.0 Onion loc: conveyor 0.98667705\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: True\n","k 335\n","Frame: 335 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: False  o: True\n","k 336\n","Frame: 336 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 337\n","Frame: 337 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 338\n","Frame: 338 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 339\n","Frame: 339 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 340\n","Frame: 340 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 341\n","Frame: 341 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 342\n","Frame: 342 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 343\n","Frame: 343 EE loc: conveyor 1.0 Onion loc: conveyor 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 344\n","Frame: 344 EE loc: hover 1.0 Onion loc: conveyor 0.99999976\n"," True EE loc: hover True O loc: hover\n","ee: True  o: False\n","k 345\n","Frame: 345 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 346\n","Frame: 346 EE loc: hover 1.0 Onion loc: hover 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 347\n","Frame: 347 EE loc: hover 0.98577535 Onion loc: hover 0.99999964\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 348\n","Frame: 348 EE loc: hover 0.99999905 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 349\n","Frame: 349 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 350\n","Frame: 350 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 351\n","Frame: 351 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 352\n","Frame: 352 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 353\n","Frame: 353 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 354\n","Frame: 354 EE loc: conveyor 0.7851015 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 355\n","Frame: 355 EE loc: conveyor 0.99983513 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 356\n","Frame: 356 EE loc: conveyor 0.9999993 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 357\n","Frame: 357 EE loc: bin 0.9688139 Onion loc: hover 0.99985385\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 358\n","Frame: 358 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 359\n","Frame: 359 EE loc: conveyor 0.99999404 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 360\n","Frame: 360 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 361\n","Frame: 361 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 362\n","Frame: 362 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 363\n","Frame: 363 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 364\n","Frame: 364 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 365\n","Frame: 365 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 366\n","Frame: 366 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 367\n","Frame: 367 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 368\n","Frame: 368 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 369\n","Frame: 369 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 370\n","Frame: 370 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: conveyor True O loc: conveyor\n","ee: False  o: False\n","k 371\n","Frame: 371 EE loc: bin 1.0 Onion loc: bin 0.9987502\n"," True EE loc: conveyor True O loc: conveyor\n","ee: False  o: False\n","k 372\n","Frame: 372 EE loc: bin 0.97969157 Onion loc: bin 0.8958041\n"," True EE loc: conveyor True O loc: conveyor\n","ee: False  o: False\n","k 373\n","Frame: 373 EE loc: conveyor 0.99872017 Onion loc: bin 0.7222777\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: False\n","k 374\n","Frame: 374 EE loc: conveyor 0.9999509 Onion loc: bin 0.9618352\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: False\n","k 375\n","Frame: 375 EE loc: conveyor 0.99994254 Onion loc: bin 0.84783274\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: False\n","k 376\n","Frame: 376 EE loc: conveyor 0.9999999 Onion loc: conveyor 0.93044776\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: True\n","k 377\n","Frame: 377 EE loc: conveyor 0.87115675 Onion loc: bin 0.9994535\n"," True EE loc: conveyor True O loc: conveyor\n","ee: True  o: False\n","k 378\n","Frame: 378 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 379\n","Frame: 379 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: False\n","k 380\n","Frame: 380 EE loc: hover 1.0 Onion loc: hover 0.9999999\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 381\n","Frame: 381 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 382\n","Frame: 382 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 383\n","Frame: 383 EE loc: hover 0.9996247 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: True  o: True\n","k 384\n","Frame: 384 EE loc: conveyor 0.9999112 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 385\n","Frame: 385 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 386\n","Frame: 386 EE loc: conveyor 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 387\n","Frame: 387 EE loc: conveyor 1.0 Onion loc: hover 0.99999607\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 388\n","Frame: 388 EE loc: conveyor 1.0 Onion loc: hover 0.9999964\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 389\n","Frame: 389 EE loc: conveyor 0.9999995 Onion loc: hover 0.99999917\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 390\n","Frame: 390 EE loc: conveyor 0.9986981 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 391\n","Frame: 391 EE loc: bin 0.98432934 Onion loc: hover 0.9595013\n"," True EE loc: hover True O loc: hover\n","ee: False  o: True\n","k 392\n","Frame: 392 EE loc: bin 0.9993824 Onion loc: hover 0.99999964\n"," True EE loc: bin True O loc: bin\n","ee: True  o: False\n","k 393\n","Frame: 393 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 394\n","Frame: 394 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 395\n","Frame: 395 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 396\n","Frame: 396 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 397\n","Frame: 397 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 398\n","Frame: 398 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 399\n","Frame: 399 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 400\n","Frame: 400 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 401\n","Frame: 401 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 402\n","Frame: 402 EE loc: bin 1.0 Onion loc: bin 1.0\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 403\n","Frame: 403 EE loc: bin 0.99999917 Onion loc: bin 0.99444455\n"," True EE loc: bin True O loc: bin\n","ee: True  o: True\n","k 404\n","Frame: 404 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: False\n","k 405\n","Frame: 405 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: False\n","k 406\n","Frame: 406 EE loc: hover 1.0 Onion loc: hover 1.0\n"," True EE loc: hover True O loc: conveyor\n","ee: True  o: False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a_lioMQ9vqc2"},"source":["# Saving predictions in text file\n","\n","import pickle\n","with open(\"eepreds.txt\", \"wb\") as fp:   #Pickling\n","    pickle.dump(eepreds, fp)\n","\n","with open(\"opreds.txt\", \"wb\") as fp:   #Pickling\n","    pickle.dump(opreds, fp)\n"],"execution_count":null,"outputs":[]}]}